I0623 22:37:00.120826 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/init.pt
I0623 22:37:00.446448 139741818646912 train.py:268] Epoch 0 TRAIN info lr 2e-07
I0623 22:37:00.448593 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:37:00.448725 139741818646912 executor.py:46] total epoch is 0.
I0623 22:37:09.483626 139741818646912 executor.py:115] TRAIN Batch 0/0 loss 154.547302 loss_att 53.018124 loss_ctc 391.448669 lr 0.00000020 rank 0
I0623 22:37:21.035996 139741818646912 executor.py:115] TRAIN Batch 0/100 loss 333.553375 loss_att 117.518784 loss_ctc 837.634094 lr 0.00000140 rank 0
I0623 22:37:32.990025 139741818646912 executor.py:115] TRAIN Batch 0/200 loss 238.051254 loss_att 85.700562 loss_ctc 593.536194 lr 0.00000260 rank 0
I0623 22:37:44.591145 139741818646912 executor.py:115] TRAIN Batch 0/300 loss 398.277466 loss_att 133.585648 loss_ctc 1015.891724 lr 0.00000380 rank 0
I0623 22:37:56.120135 139741818646912 executor.py:115] TRAIN Batch 0/400 loss 244.962662 loss_att 97.915848 loss_ctc 588.071899 lr 0.00000520 rank 0
I0623 22:38:08.702509 139741818646912 executor.py:115] TRAIN Batch 0/500 loss 111.784142 loss_att 51.594055 loss_ctc 252.227661 lr 0.00000640 rank 0
I0623 22:38:20.383955 139741818646912 executor.py:115] TRAIN Batch 0/600 loss 215.777817 loss_att 124.707687 loss_ctc 428.274780 lr 0.00000760 rank 0
I0623 22:38:32.422553 139741818646912 executor.py:115] TRAIN Batch 0/700 loss 124.518288 loss_att 88.514259 loss_ctc 208.527679 lr 0.00000880 rank 0
I0623 22:38:43.981855 139741818646912 executor.py:152] CV Batch 0/0 loss 67.987526 loss_att 53.014641 loss_ctc 102.924255 history loss 63.988260 rank 0
I0623 22:38:49.191828 139741818646912 train.py:275] Epoch 0 CV info cv_loss 138.56313054044182
I0623 22:38:49.191952 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/0.pt
I0623 22:38:49.499418 139741818646912 train.py:268] Epoch 1 TRAIN info lr 9.4e-06
I0623 22:38:49.501871 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:38:49.502012 139741818646912 executor.py:46] total epoch is 1.
I0623 22:38:57.426700 139741818646912 executor.py:115] TRAIN Batch 1/0 loss 69.550972 loss_att 53.824230 loss_ctc 106.246696 lr 0.00000960 rank 0
I0623 22:39:10.040229 139741818646912 executor.py:115] TRAIN Batch 1/100 loss 135.375443 loss_att 114.922455 loss_ctc 183.099060 lr 0.00001080 rank 0
I0623 22:39:21.560918 139741818646912 executor.py:115] TRAIN Batch 1/200 loss 91.170547 loss_att 82.762909 loss_ctc 110.788376 lr 0.00001200 rank 0
I0623 22:39:32.542678 139741818646912 executor.py:115] TRAIN Batch 1/300 loss 148.750214 loss_att 136.699478 loss_ctc 176.868561 lr 0.00001320 rank 0
I0623 22:39:44.060845 139741818646912 executor.py:115] TRAIN Batch 1/400 loss 108.271545 loss_att 101.477066 loss_ctc 124.125328 lr 0.00001460 rank 0
I0623 22:39:56.201596 139741818646912 executor.py:115] TRAIN Batch 1/500 loss 57.262894 loss_att 54.877861 loss_ctc 62.827969 lr 0.00001580 rank 0
I0623 22:40:07.337884 139741818646912 executor.py:115] TRAIN Batch 1/600 loss 128.898224 loss_att 123.007225 loss_ctc 142.643890 lr 0.00001700 rank 0
I0623 22:40:19.271004 139741818646912 executor.py:115] TRAIN Batch 1/700 loss 82.484756 loss_att 79.259460 loss_ctc 90.010437 lr 0.00001820 rank 0
I0623 22:40:30.773461 139741818646912 executor.py:152] CV Batch 1/0 loss 53.838562 loss_att 52.261612 loss_ctc 57.518124 history loss 50.671588 rank 0
I0623 22:40:36.284925 139741818646912 train.py:275] Epoch 1 CV info cv_loss 111.54763361630702
I0623 22:40:36.285227 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/1.pt
I0623 22:40:36.585106 139741818646912 train.py:268] Epoch 2 TRAIN info lr 1.88e-05
I0623 22:40:36.587556 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:40:36.587692 139741818646912 executor.py:46] total epoch is 2.
I0623 22:40:44.551716 139741818646912 executor.py:115] TRAIN Batch 2/0 loss 59.837997 loss_att 57.745987 loss_ctc 64.719345 lr 0.00001900 rank 0
I0623 22:40:57.151860 139741818646912 executor.py:115] TRAIN Batch 2/100 loss 114.408768 loss_att 109.503502 loss_ctc 125.854370 lr 0.00002020 rank 0
I0623 22:41:09.408488 139741818646912 executor.py:115] TRAIN Batch 2/200 loss 82.748322 loss_att 80.155273 loss_ctc 88.798767 lr 0.00002140 rank 0
I0623 22:41:21.156769 139741818646912 executor.py:115] TRAIN Batch 2/300 loss 145.477722 loss_att 139.643463 loss_ctc 159.091003 lr 0.00002260 rank 0
I0623 22:41:33.339351 139741818646912 executor.py:115] TRAIN Batch 2/400 loss 96.892014 loss_att 93.863693 loss_ctc 103.958092 lr 0.00002400 rank 0
I0623 22:41:46.442782 139741818646912 executor.py:115] TRAIN Batch 2/500 loss 60.160835 loss_att 59.120277 loss_ctc 62.588802 lr 0.00002520 rank 0
I0623 22:41:58.219808 139741818646912 executor.py:115] TRAIN Batch 2/600 loss 116.606476 loss_att 113.246979 loss_ctc 124.445297 lr 0.00002640 rank 0
I0623 22:42:10.470520 139741818646912 executor.py:115] TRAIN Batch 2/700 loss 83.193069 loss_att 81.134064 loss_ctc 87.997406 lr 0.00002760 rank 0
I0623 22:42:22.861412 139741818646912 executor.py:152] CV Batch 2/0 loss 50.401279 loss_att 49.720016 loss_ctc 51.990891 history loss 47.436498 rank 0
I0623 22:42:28.598217 139741818646912 train.py:275] Epoch 2 CV info cv_loss 105.67557297232096
I0623 22:42:28.598381 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/2.pt
I0623 22:42:28.894088 139741818646912 train.py:268] Epoch 3 TRAIN info lr 2.8199999999999998e-05
I0623 22:42:28.896442 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:42:28.896729 139741818646912 executor.py:46] total epoch is 3.
I0623 22:42:37.472794 139741818646912 executor.py:115] TRAIN Batch 3/0 loss 50.197449 loss_att 49.313774 loss_ctc 52.259350 lr 0.00002840 rank 0
I0623 22:42:50.676004 139741818646912 executor.py:115] TRAIN Batch 3/100 loss 112.671143 loss_att 109.668327 loss_ctc 119.677719 lr 0.00002960 rank 0
I0623 22:43:02.921620 139741818646912 executor.py:115] TRAIN Batch 3/200 loss 82.258591 loss_att 80.025352 loss_ctc 87.469490 lr 0.00003080 rank 0
I0623 22:43:14.481330 139741818646912 executor.py:115] TRAIN Batch 3/300 loss 128.504944 loss_att 124.629425 loss_ctc 137.547806 lr 0.00003200 rank 0
I0623 22:43:26.257508 139741818646912 executor.py:115] TRAIN Batch 3/400 loss 88.315636 loss_att 85.314133 loss_ctc 95.319138 lr 0.00003340 rank 0
I0623 22:43:38.382571 139741818646912 executor.py:115] TRAIN Batch 3/500 loss 48.751251 loss_att 47.237968 loss_ctc 52.282249 lr 0.00003460 rank 0
I0623 22:43:49.862432 139741818646912 executor.py:115] TRAIN Batch 3/600 loss 101.925934 loss_att 98.237312 loss_ctc 110.532692 lr 0.00003580 rank 0
I0623 22:44:01.484327 139741818646912 executor.py:115] TRAIN Batch 3/700 loss 72.143829 loss_att 69.207916 loss_ctc 78.994278 lr 0.00003700 rank 0
I0623 22:44:13.626764 139741818646912 executor.py:152] CV Batch 3/0 loss 44.504551 loss_att 43.021713 loss_ctc 47.964504 history loss 41.886636 rank 0
I0623 22:44:19.160772 139741818646912 train.py:275] Epoch 3 CV info cv_loss 95.5952947900084
I0623 22:44:19.161007 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/3.pt
I0623 22:44:19.467105 139741818646912 train.py:268] Epoch 4 TRAIN info lr 3.76e-05
I0623 22:44:19.469443 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:44:19.469708 139741818646912 executor.py:46] total epoch is 4.
I0623 22:44:27.729693 139741818646912 executor.py:115] TRAIN Batch 4/0 loss 48.199715 loss_att 46.386833 loss_ctc 52.429775 lr 0.00003780 rank 0
I0623 22:44:41.077441 139741818646912 executor.py:115] TRAIN Batch 4/100 loss 102.639359 loss_att 98.552345 loss_ctc 112.175705 lr 0.00003900 rank 0
I0623 22:44:53.560389 139741818646912 executor.py:115] TRAIN Batch 4/200 loss 69.754158 loss_att 66.820969 loss_ctc 76.598259 lr 0.00004020 rank 0
I0623 22:45:05.567683 139741818646912 executor.py:115] TRAIN Batch 4/300 loss 115.038811 loss_att 110.243683 loss_ctc 126.227432 lr 0.00004140 rank 0
I0623 22:45:17.885942 139741818646912 executor.py:115] TRAIN Batch 4/400 loss 82.911613 loss_att 79.423790 loss_ctc 91.049858 lr 0.00004280 rank 0
I0623 22:45:30.533218 139741818646912 executor.py:115] TRAIN Batch 4/500 loss 41.943935 loss_att 40.295212 loss_ctc 45.790958 lr 0.00004400 rank 0
I0623 22:45:42.515144 139741818646912 executor.py:115] TRAIN Batch 4/600 loss 91.331444 loss_att 87.396629 loss_ctc 100.512680 lr 0.00004520 rank 0
I0623 22:45:54.685398 139741818646912 executor.py:115] TRAIN Batch 4/700 loss 67.310684 loss_att 64.307777 loss_ctc 74.317474 lr 0.00004640 rank 0
I0623 22:46:06.907819 139741818646912 executor.py:152] CV Batch 4/0 loss 42.417759 loss_att 40.785923 loss_ctc 46.225372 history loss 39.922597 rank 0
I0623 22:46:12.560158 139741818646912 train.py:275] Epoch 4 CV info cv_loss 91.23899676262376
I0623 22:46:12.560303 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/4.pt
I0623 22:46:12.857728 139741818646912 train.py:268] Epoch 5 TRAIN info lr 4.7e-05
I0623 22:46:12.860054 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:46:12.860350 139741818646912 executor.py:46] total epoch is 5.
I0623 22:46:21.513055 139741818646912 executor.py:115] TRAIN Batch 5/0 loss 44.099144 loss_att 42.286163 loss_ctc 48.329441 lr 0.00004720 rank 0
I0623 22:46:34.503092 139741818646912 executor.py:115] TRAIN Batch 5/100 loss 105.645760 loss_att 100.953102 loss_ctc 116.595306 lr 0.00004840 rank 0
I0623 22:46:47.196201 139741818646912 executor.py:115] TRAIN Batch 5/200 loss 67.499435 loss_att 64.614380 loss_ctc 74.231239 lr 0.00004960 rank 0
I0623 22:46:59.413604 139741818646912 executor.py:115] TRAIN Batch 5/300 loss 121.150238 loss_att 115.683395 loss_ctc 133.906204 lr 0.00005080 rank 0
I0623 22:47:12.028462 139741818646912 executor.py:115] TRAIN Batch 5/400 loss 76.199425 loss_att 72.966599 loss_ctc 83.742676 lr 0.00005220 rank 0
I0623 22:47:24.676211 139741818646912 executor.py:115] TRAIN Batch 5/500 loss 45.846748 loss_att 43.973492 loss_ctc 50.217682 lr 0.00005340 rank 0
I0623 22:47:36.630294 139741818646912 executor.py:115] TRAIN Batch 5/600 loss 100.066681 loss_att 95.436958 loss_ctc 110.869354 lr 0.00005460 rank 0
I0623 22:47:48.888028 139741818646912 executor.py:115] TRAIN Batch 5/700 loss 68.070541 loss_att 65.115311 loss_ctc 74.966080 lr 0.00005580 rank 0
I0623 22:48:01.117240 139741818646912 executor.py:152] CV Batch 5/0 loss 42.033390 loss_att 40.374294 loss_ctc 45.904617 history loss 39.560838 rank 0
I0623 22:48:06.800043 139741818646912 train.py:275] Epoch 5 CV info cv_loss 90.34037070530574
I0623 22:48:06.800246 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/5.pt
I0623 22:48:07.110014 139741818646912 train.py:268] Epoch 6 TRAIN info lr 5.6399999999999995e-05
I0623 22:48:07.112471 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:48:07.112646 139741818646912 executor.py:46] total epoch is 6.
I0623 22:48:15.595898 139741818646912 executor.py:115] TRAIN Batch 6/0 loss 37.460854 loss_att 35.844650 loss_ctc 41.231995 lr 0.00005660 rank 0
I0623 22:48:28.596314 139741818646912 executor.py:115] TRAIN Batch 6/100 loss 89.653854 loss_att 85.537918 loss_ctc 99.257721 lr 0.00005780 rank 0
I0623 22:48:41.158692 139741818646912 executor.py:115] TRAIN Batch 6/200 loss 66.507553 loss_att 63.545650 loss_ctc 73.418663 lr 0.00005900 rank 0
I0623 22:48:53.446822 139741818646912 executor.py:115] TRAIN Batch 6/300 loss 114.219238 loss_att 108.960098 loss_ctc 126.490570 lr 0.00006020 rank 0
I0623 22:49:06.006728 139741818646912 executor.py:115] TRAIN Batch 6/400 loss 81.442001 loss_att 77.679558 loss_ctc 90.221046 lr 0.00006160 rank 0
I0623 22:49:18.560889 139741818646912 executor.py:115] TRAIN Batch 6/500 loss 45.880077 loss_att 44.024803 loss_ctc 50.209045 lr 0.00006280 rank 0
I0623 22:49:30.090185 139741818646912 executor.py:115] TRAIN Batch 6/600 loss 94.823212 loss_att 90.520363 loss_ctc 104.863190 lr 0.00006400 rank 0
I0623 22:49:42.500213 139741818646912 executor.py:115] TRAIN Batch 6/700 loss 66.801361 loss_att 63.721142 loss_ctc 73.988541 lr 0.00006520 rank 0
I0623 22:49:55.007419 139741818646912 executor.py:152] CV Batch 6/0 loss 41.794025 loss_att 40.083145 loss_ctc 45.786079 history loss 39.335553 rank 0
I0623 22:50:00.647436 139741818646912 train.py:275] Epoch 6 CV info cv_loss 89.63851777507305
I0623 22:50:00.647597 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/6.pt
I0623 22:50:00.948827 139741818646912 train.py:268] Epoch 7 TRAIN info lr 6.58e-05
I0623 22:50:00.951249 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:50:00.951386 139741818646912 executor.py:46] total epoch is 7.
I0623 22:50:09.516724 139741818646912 executor.py:115] TRAIN Batch 7/0 loss 40.034748 loss_att 38.376137 loss_ctc 43.904846 lr 0.00006600 rank 0
I0623 22:50:22.282837 139741818646912 executor.py:115] TRAIN Batch 7/100 loss 92.990494 loss_att 88.467346 loss_ctc 103.544495 lr 0.00006720 rank 0
I0623 22:50:34.234052 139741818646912 executor.py:115] TRAIN Batch 7/200 loss 65.917366 loss_att 62.804970 loss_ctc 73.179626 lr 0.00006840 rank 0
I0623 22:50:46.024686 139741818646912 executor.py:115] TRAIN Batch 7/300 loss 109.749695 loss_att 104.313797 loss_ctc 122.433479 lr 0.00006960 rank 0
I0623 22:50:58.058851 139741818646912 executor.py:115] TRAIN Batch 7/400 loss 81.730240 loss_att 77.697220 loss_ctc 91.140610 lr 0.00007100 rank 0
I0623 22:51:10.206541 139741818646912 executor.py:115] TRAIN Batch 7/500 loss 45.470249 loss_att 43.401573 loss_ctc 50.297161 lr 0.00007220 rank 0
I0623 22:51:21.526577 139741818646912 executor.py:115] TRAIN Batch 7/600 loss 88.999878 loss_att 84.294540 loss_ctc 99.979004 lr 0.00007340 rank 0
I0623 22:51:33.395344 139741818646912 executor.py:115] TRAIN Batch 7/700 loss 66.382523 loss_att 62.900352 loss_ctc 74.507584 lr 0.00007460 rank 0
I0623 22:51:45.287554 139741818646912 executor.py:152] CV Batch 7/0 loss 41.511978 loss_att 39.709084 loss_ctc 45.718727 history loss 39.070097 rank 0
I0623 22:51:50.632044 139741818646912 train.py:275] Epoch 7 CV info cv_loss 88.76227688306874
I0623 22:51:50.632229 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/7.pt
I0623 22:51:50.927670 139741818646912 train.py:268] Epoch 8 TRAIN info lr 7.52e-05
I0623 22:51:50.929033 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:51:50.929102 139741818646912 executor.py:46] total epoch is 8.
I0623 22:51:58.974363 139741818646912 executor.py:115] TRAIN Batch 8/0 loss 47.404274 loss_att 45.313030 loss_ctc 52.283844 lr 0.00007540 rank 0
I0623 22:52:12.063074 139741818646912 executor.py:115] TRAIN Batch 8/100 loss 105.421509 loss_att 99.432632 loss_ctc 119.395546 lr 0.00007660 rank 0
I0623 22:52:24.507175 139741818646912 executor.py:115] TRAIN Batch 8/200 loss 65.764687 loss_att 62.036022 loss_ctc 74.464905 lr 0.00007780 rank 0
I0623 22:52:36.649933 139741818646912 executor.py:115] TRAIN Batch 8/300 loss 110.528496 loss_att 104.775764 loss_ctc 123.951538 lr 0.00007900 rank 0
I0623 22:52:48.926015 139741818646912 executor.py:115] TRAIN Batch 8/400 loss 77.061050 loss_att 73.070282 loss_ctc 86.372833 lr 0.00008040 rank 0
I0623 22:53:01.848287 139741818646912 executor.py:115] TRAIN Batch 8/500 loss 40.187374 loss_att 38.224251 loss_ctc 44.767998 lr 0.00008160 rank 0
I0623 22:53:13.677678 139741818646912 executor.py:115] TRAIN Batch 8/600 loss 99.197983 loss_att 93.589279 loss_ctc 112.284943 lr 0.00008280 rank 0
I0623 22:53:25.850363 139741818646912 executor.py:115] TRAIN Batch 8/700 loss 65.446121 loss_att 61.545620 loss_ctc 74.547302 lr 0.00008400 rank 0
I0623 22:53:38.189137 139741818646912 executor.py:152] CV Batch 8/0 loss 41.183372 loss_att 39.340202 loss_ctc 45.484108 history loss 38.760821 rank 0
I0623 22:53:43.944316 139741818646912 train.py:275] Epoch 8 CV info cv_loss 87.65294924600924
I0623 22:53:43.944506 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/8.pt
I0623 22:53:44.556842 139741818646912 train.py:268] Epoch 9 TRAIN info lr 8.459999999999998e-05
I0623 22:53:44.558314 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:53:44.558383 139741818646912 executor.py:46] total epoch is 9.
I0623 22:53:52.902590 139741818646912 executor.py:115] TRAIN Batch 9/0 loss 41.581272 loss_att 39.467014 loss_ctc 46.514542 lr 0.00008480 rank 0
I0623 22:54:06.171658 139741818646912 executor.py:115] TRAIN Batch 9/100 loss 94.961731 loss_att 89.528290 loss_ctc 107.639748 lr 0.00008600 rank 0
I0623 22:54:18.944870 139741818646912 executor.py:115] TRAIN Batch 9/200 loss 67.183014 loss_att 63.103634 loss_ctc 76.701576 lr 0.00008720 rank 0
I0623 22:54:31.278926 139741818646912 executor.py:115] TRAIN Batch 9/300 loss 104.286247 loss_att 97.710449 loss_ctc 119.629761 lr 0.00008840 rank 0
I0623 22:54:43.215888 139741818646912 executor.py:115] TRAIN Batch 9/400 loss 75.796112 loss_att 71.829620 loss_ctc 85.051239 lr 0.00008980 rank 0
I0623 22:54:55.843295 139741818646912 executor.py:115] TRAIN Batch 9/500 loss 41.651215 loss_att 39.719875 loss_ctc 46.157665 lr 0.00009100 rank 0
I0623 22:55:07.425201 139741818646912 executor.py:115] TRAIN Batch 9/600 loss 100.461349 loss_att 94.339256 loss_ctc 114.746231 lr 0.00009220 rank 0
I0623 22:55:19.230520 139741818646912 executor.py:115] TRAIN Batch 9/700 loss 68.416504 loss_att 64.134430 loss_ctc 78.407997 lr 0.00009340 rank 0
I0623 22:55:31.286550 139741818646912 executor.py:152] CV Batch 9/0 loss 40.931591 loss_att 39.128208 loss_ctc 45.139481 history loss 38.523850 rank 0
I0623 22:55:36.846118 139741818646912 train.py:275] Epoch 9 CV info cv_loss 86.57559044379416
I0623 22:55:36.846345 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/9.pt
I0623 22:55:37.153242 139741818646912 train.py:268] Epoch 10 TRAIN info lr 9.4e-05
I0623 22:55:37.155735 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:55:37.155870 139741818646912 executor.py:46] total epoch is 10.
I0623 22:55:45.256178 139741818646912 executor.py:115] TRAIN Batch 10/0 loss 45.263863 loss_att 42.721649 loss_ctc 51.195702 lr 0.00009420 rank 0
I0623 22:55:58.804556 139741818646912 executor.py:115] TRAIN Batch 10/100 loss 90.763199 loss_att 84.882080 loss_ctc 104.485809 lr 0.00009540 rank 0
I0623 22:56:11.756555 139741818646912 executor.py:115] TRAIN Batch 10/200 loss 65.668182 loss_att 61.500469 loss_ctc 75.392853 lr 0.00009660 rank 0
I0623 22:56:24.103763 139741818646912 executor.py:115] TRAIN Batch 10/300 loss 105.173271 loss_att 96.766121 loss_ctc 124.789963 lr 0.00009780 rank 0
I0623 22:56:36.588316 139741818646912 executor.py:115] TRAIN Batch 10/400 loss 76.828354 loss_att 71.728081 loss_ctc 88.728981 lr 0.00009920 rank 0
I0623 22:56:49.465204 139741818646912 executor.py:115] TRAIN Batch 10/500 loss 45.334034 loss_att 42.743839 loss_ctc 51.377823 lr 0.00010040 rank 0
I0623 22:57:01.051654 139741818646912 executor.py:115] TRAIN Batch 10/600 loss 84.723656 loss_att 78.476036 loss_ctc 99.301430 lr 0.00010160 rank 0
I0623 22:57:13.433873 139741818646912 executor.py:115] TRAIN Batch 10/700 loss 66.363960 loss_att 61.830925 loss_ctc 76.941040 lr 0.00010280 rank 0
I0623 22:57:25.856523 139741818646912 executor.py:152] CV Batch 10/0 loss 40.758434 loss_att 38.811096 loss_ctc 45.302223 history loss 38.360879 rank 0
I0623 22:57:31.616598 139741818646912 train.py:275] Epoch 10 CV info cv_loss 85.56842769348613
I0623 22:57:31.616735 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/10.pt
I0623 22:57:31.919054 139741818646912 train.py:268] Epoch 11 TRAIN info lr 0.00010339999999999999
I0623 22:57:31.921632 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:57:31.921789 139741818646912 executor.py:46] total epoch is 11.
I0623 22:57:40.460437 139741818646912 executor.py:115] TRAIN Batch 11/0 loss 41.999977 loss_att 39.596558 loss_ctc 47.607960 lr 0.00010360 rank 0
I0623 22:57:53.728720 139741818646912 executor.py:115] TRAIN Batch 11/100 loss 84.111336 loss_att 77.649353 loss_ctc 99.189301 lr 0.00010480 rank 0
I0623 22:58:06.595659 139741818646912 executor.py:115] TRAIN Batch 11/200 loss 61.857147 loss_att 57.550682 loss_ctc 71.905563 lr 0.00010600 rank 0
I0623 22:58:18.856957 139741818646912 executor.py:115] TRAIN Batch 11/300 loss 107.783035 loss_att 99.486893 loss_ctc 127.140694 lr 0.00010720 rank 0
I0623 22:58:31.348059 139741818646912 executor.py:115] TRAIN Batch 11/400 loss 74.876244 loss_att 68.839828 loss_ctc 88.961220 lr 0.00010860 rank 0
I0623 22:58:44.403575 139741818646912 executor.py:115] TRAIN Batch 11/500 loss 41.298004 loss_att 38.840443 loss_ctc 47.032318 lr 0.00010980 rank 0
I0623 22:58:56.372826 139741818646912 executor.py:115] TRAIN Batch 11/600 loss 84.940773 loss_att 77.898682 loss_ctc 101.372314 lr 0.00011100 rank 0
I0623 22:59:08.712547 139741818646912 executor.py:115] TRAIN Batch 11/700 loss 60.386559 loss_att 55.659302 loss_ctc 71.416824 lr 0.00011220 rank 0
I0623 22:59:21.053111 139741818646912 executor.py:152] CV Batch 11/0 loss 40.478279 loss_att 38.510956 loss_ctc 45.068707 history loss 38.097204 rank 0
I0623 22:59:26.719278 139741818646912 train.py:275] Epoch 11 CV info cv_loss 84.69881897611545
I0623 22:59:26.719506 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/11.pt
I0623 22:59:27.018835 139741818646912 train.py:268] Epoch 12 TRAIN info lr 0.00011279999999999999
I0623 22:59:27.021160 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 22:59:27.021303 139741818646912 executor.py:46] total epoch is 12.
I0623 22:59:35.316013 139741818646912 executor.py:115] TRAIN Batch 12/0 loss 42.063408 loss_att 39.515770 loss_ctc 48.007896 lr 0.00011300 rank 0
I0623 22:59:48.214792 139741818646912 executor.py:115] TRAIN Batch 12/100 loss 88.181770 loss_att 81.462837 loss_ctc 103.859268 lr 0.00011420 rank 0
I0623 23:00:00.592837 139741818646912 executor.py:115] TRAIN Batch 12/200 loss 63.533100 loss_att 58.910507 loss_ctc 74.319153 lr 0.00011540 rank 0
I0623 23:00:12.423888 139741818646912 executor.py:115] TRAIN Batch 12/300 loss 109.372627 loss_att 100.627769 loss_ctc 129.777298 lr 0.00011660 rank 0
I0623 23:00:24.856119 139741818646912 executor.py:115] TRAIN Batch 12/400 loss 70.754158 loss_att 64.613396 loss_ctc 85.082603 lr 0.00011800 rank 0
I0623 23:00:37.707207 139741818646912 executor.py:115] TRAIN Batch 12/500 loss 42.354057 loss_att 39.434044 loss_ctc 49.167427 lr 0.00011920 rank 0
I0623 23:00:49.325681 139741818646912 executor.py:115] TRAIN Batch 12/600 loss 89.665199 loss_att 81.867554 loss_ctc 107.859703 lr 0.00012040 rank 0
I0623 23:01:01.835399 139741818646912 executor.py:115] TRAIN Batch 12/700 loss 57.255325 loss_att 52.102341 loss_ctc 69.278954 lr 0.00012160 rank 0
I0623 23:01:14.202509 139741818646912 executor.py:152] CV Batch 12/0 loss 39.904060 loss_att 37.914345 loss_ctc 44.546722 history loss 37.556763 rank 0
I0623 23:01:19.867038 139741818646912 train.py:275] Epoch 12 CV info cv_loss 83.73598346590414
I0623 23:01:19.867182 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/12.pt
I0623 23:01:20.168097 139741818646912 train.py:268] Epoch 13 TRAIN info lr 0.0001222
I0623 23:01:20.170475 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:01:20.170670 139741818646912 executor.py:46] total epoch is 13.
I0623 23:01:28.889764 139741818646912 executor.py:115] TRAIN Batch 13/0 loss 46.601425 loss_att 43.173149 loss_ctc 54.600739 lr 0.00012240 rank 0
I0623 23:01:42.347075 139741818646912 executor.py:115] TRAIN Batch 13/100 loss 81.996521 loss_att 75.585251 loss_ctc 96.956146 lr 0.00012360 rank 0
I0623 23:01:54.671173 139741818646912 executor.py:115] TRAIN Batch 13/200 loss 59.878372 loss_att 55.370781 loss_ctc 70.396088 lr 0.00012480 rank 0
I0623 23:02:06.312440 139741818646912 executor.py:115] TRAIN Batch 13/300 loss 100.352905 loss_att 91.846497 loss_ctc 120.201187 lr 0.00012600 rank 0
I0623 23:02:18.201366 139741818646912 executor.py:115] TRAIN Batch 13/400 loss 71.485367 loss_att 65.842712 loss_ctc 84.651550 lr 0.00012740 rank 0
I0623 23:02:30.319347 139741818646912 executor.py:115] TRAIN Batch 13/500 loss 43.553135 loss_att 40.539722 loss_ctc 50.584427 lr 0.00012860 rank 0
I0623 23:02:41.895341 139741818646912 executor.py:115] TRAIN Batch 13/600 loss 83.431923 loss_att 76.635704 loss_ctc 99.289772 lr 0.00012980 rank 0
I0623 23:02:54.381747 139741818646912 executor.py:115] TRAIN Batch 13/700 loss 59.339561 loss_att 54.240063 loss_ctc 71.238388 lr 0.00013100 rank 0
I0623 23:03:06.681549 139741818646912 executor.py:152] CV Batch 13/0 loss 39.358261 loss_att 37.516575 loss_ctc 43.655521 history loss 37.043069 rank 0
I0623 23:03:12.241337 139741818646912 train.py:275] Epoch 13 CV info cv_loss 82.57105382372264
I0623 23:03:12.241569 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/13.pt
I0623 23:03:12.550315 139741818646912 train.py:268] Epoch 14 TRAIN info lr 0.0001316
I0623 23:03:12.552636 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:03:12.552769 139741818646912 executor.py:46] total epoch is 14.
I0623 23:03:21.091592 139741818646912 executor.py:115] TRAIN Batch 14/0 loss 40.359634 loss_att 37.410950 loss_ctc 47.239906 lr 0.00013180 rank 0
I0623 23:03:34.445867 139741818646912 executor.py:115] TRAIN Batch 14/100 loss 87.367401 loss_att 80.838043 loss_ctc 102.602585 lr 0.00013300 rank 0
I0623 23:03:47.279538 139741818646912 executor.py:115] TRAIN Batch 14/200 loss 59.918919 loss_att 54.491669 loss_ctc 72.582497 lr 0.00013420 rank 0
I0623 23:03:59.368180 139741818646912 executor.py:115] TRAIN Batch 14/300 loss 106.948006 loss_att 98.218384 loss_ctc 127.317116 lr 0.00013540 rank 0
I0623 23:04:11.495622 139741818646912 executor.py:115] TRAIN Batch 14/400 loss 68.244995 loss_att 62.842476 loss_ctc 80.850891 lr 0.00013680 rank 0
I0623 23:04:24.080493 139741818646912 executor.py:115] TRAIN Batch 14/500 loss 38.842705 loss_att 36.139233 loss_ctc 45.150806 lr 0.00013800 rank 0
I0623 23:04:35.857706 139741818646912 executor.py:115] TRAIN Batch 14/600 loss 80.688515 loss_att 72.394279 loss_ctc 100.041725 lr 0.00013920 rank 0
I0623 23:04:48.481074 139741818646912 executor.py:115] TRAIN Batch 14/700 loss 57.967583 loss_att 53.782158 loss_ctc 67.733566 lr 0.00014040 rank 0
I0623 23:05:01.150417 139741818646912 executor.py:152] CV Batch 14/0 loss 38.881989 loss_att 37.261566 loss_ctc 42.662971 history loss 36.594813 rank 0
I0623 23:05:06.782113 139741818646912 train.py:275] Epoch 14 CV info cv_loss 81.24731453831015
I0623 23:05:06.782284 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/14.pt
I0623 23:05:06.893967 139741818646912 train.py:268] Epoch 15 TRAIN info lr 0.00014099999999999998
I0623 23:05:06.895224 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:05:06.895550 139741818646912 executor.py:46] total epoch is 15.
I0623 23:05:15.001969 139741818646912 executor.py:115] TRAIN Batch 15/0 loss 34.964302 loss_att 32.665668 loss_ctc 40.327785 lr 0.00014120 rank 0
I0623 23:05:28.097525 139741818646912 executor.py:115] TRAIN Batch 15/100 loss 81.465836 loss_att 74.512131 loss_ctc 97.691154 lr 0.00014240 rank 0
I0623 23:05:40.682119 139741818646912 executor.py:115] TRAIN Batch 15/200 loss 58.052879 loss_att 53.032612 loss_ctc 69.766830 lr 0.00014360 rank 0
I0623 23:05:53.154087 139741818646912 executor.py:115] TRAIN Batch 15/300 loss 107.596252 loss_att 100.102341 loss_ctc 125.082031 lr 0.00014480 rank 0
I0623 23:06:05.556836 139741818646912 executor.py:115] TRAIN Batch 15/400 loss 67.607193 loss_att 62.299305 loss_ctc 79.992256 lr 0.00014620 rank 0
I0623 23:06:18.250285 139741818646912 executor.py:115] TRAIN Batch 15/500 loss 34.961388 loss_att 33.147484 loss_ctc 39.193840 lr 0.00014740 rank 0
I0623 23:06:30.056204 139741818646912 executor.py:115] TRAIN Batch 15/600 loss 92.570801 loss_att 86.230453 loss_ctc 107.364944 lr 0.00014860 rank 0
I0623 23:06:42.334928 139741818646912 executor.py:115] TRAIN Batch 15/700 loss 58.530434 loss_att 54.667770 loss_ctc 67.543320 lr 0.00014980 rank 0
I0623 23:06:54.668857 139741818646912 executor.py:152] CV Batch 15/0 loss 37.773964 loss_att 36.602665 loss_ctc 40.506996 history loss 35.551966 rank 0
I0623 23:07:00.561466 139741818646912 train.py:275] Epoch 15 CV info cv_loss 78.87570460892522
I0623 23:07:00.561863 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/15.pt
I0623 23:07:00.683301 139741818646912 train.py:268] Epoch 16 TRAIN info lr 0.0001504
I0623 23:07:00.684607 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:07:00.684701 139741818646912 executor.py:46] total epoch is 16.
I0623 23:07:09.462643 139741818646912 executor.py:115] TRAIN Batch 16/0 loss 36.878265 loss_att 35.378948 loss_ctc 40.376675 lr 0.00015060 rank 0
I0623 23:07:22.539793 139741818646912 executor.py:115] TRAIN Batch 16/100 loss 75.851410 loss_att 69.928787 loss_ctc 89.670876 lr 0.00015180 rank 0
I0623 23:07:35.155904 139741818646912 executor.py:115] TRAIN Batch 16/200 loss 56.415306 loss_att 53.416023 loss_ctc 63.413631 lr 0.00015300 rank 0
I0623 23:07:46.423142 139741818646912 executor.py:115] TRAIN Batch 16/300 loss 91.982613 loss_att 87.071198 loss_ctc 103.442574 lr 0.00015420 rank 0
I0623 23:07:58.038222 139741818646912 executor.py:115] TRAIN Batch 16/400 loss 67.056854 loss_att 62.681133 loss_ctc 77.266861 lr 0.00015560 rank 0
I0623 23:08:10.698585 139741818646912 executor.py:115] TRAIN Batch 16/500 loss 31.507530 loss_att 30.634941 loss_ctc 33.543575 lr 0.00015680 rank 0
I0623 23:08:22.311071 139741818646912 executor.py:115] TRAIN Batch 16/600 loss 76.431969 loss_att 72.996582 loss_ctc 84.447861 lr 0.00015800 rank 0
I0623 23:08:34.408866 139741818646912 executor.py:115] TRAIN Batch 16/700 loss 55.724583 loss_att 53.711052 loss_ctc 60.422821 lr 0.00015920 rank 0
I0623 23:08:46.768842 139741818646912 executor.py:152] CV Batch 16/0 loss 35.999298 loss_att 36.280300 loss_ctc 35.343620 history loss 33.881692 rank 0
I0623 23:08:52.440820 139741818646912 train.py:275] Epoch 16 CV info cv_loss 75.72081493050256
I0623 23:08:52.441061 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/16.pt
I0623 23:08:52.556372 139741818646912 train.py:268] Epoch 17 TRAIN info lr 0.00015979999999999998
I0623 23:08:52.557656 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:08:52.557794 139741818646912 executor.py:46] total epoch is 17.
I0623 23:09:00.778721 139741818646912 executor.py:115] TRAIN Batch 17/0 loss 34.712219 loss_att 32.774094 loss_ctc 39.234505 lr 0.00016000 rank 0
I0623 23:09:13.962115 139741818646912 executor.py:115] TRAIN Batch 17/100 loss 77.767639 loss_att 74.265106 loss_ctc 85.940231 lr 0.00016120 rank 0
I0623 23:09:26.448934 139741818646912 executor.py:115] TRAIN Batch 17/200 loss 55.616764 loss_att 53.539928 loss_ctc 60.462708 lr 0.00016240 rank 0
I0623 23:09:38.744586 139741818646912 executor.py:115] TRAIN Batch 17/300 loss 94.523453 loss_att 92.004364 loss_ctc 100.401337 lr 0.00016360 rank 0
I0623 23:09:51.549232 139741818646912 executor.py:115] TRAIN Batch 17/400 loss 64.279701 loss_att 62.830132 loss_ctc 67.662025 lr 0.00016500 rank 0
I0623 23:10:04.608083 139741818646912 executor.py:115] TRAIN Batch 17/500 loss 37.819763 loss_att 36.471027 loss_ctc 40.966812 lr 0.00016620 rank 0
I0623 23:10:16.435598 139741818646912 executor.py:115] TRAIN Batch 17/600 loss 71.951126 loss_att 70.242973 loss_ctc 75.936798 lr 0.00016740 rank 0
I0623 23:10:28.817518 139741818646912 executor.py:115] TRAIN Batch 17/700 loss 49.831703 loss_att 47.907074 loss_ctc 54.322498 lr 0.00016860 rank 0
I0623 23:10:41.161711 139741818646912 executor.py:152] CV Batch 17/0 loss 34.938034 loss_att 36.436333 loss_ctc 31.441998 history loss 32.882856 rank 0
I0623 23:10:46.764026 139741818646912 train.py:275] Epoch 17 CV info cv_loss 73.06699713675691
I0623 23:10:46.764371 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/17.pt
I0623 23:10:46.880016 139741818646912 train.py:268] Epoch 18 TRAIN info lr 0.00016919999999999997
I0623 23:10:46.881374 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:10:46.881443 139741818646912 executor.py:46] total epoch is 18.
I0623 23:10:55.224342 139741818646912 executor.py:115] TRAIN Batch 18/0 loss 32.396458 loss_att 31.998219 loss_ctc 33.325684 lr 0.00016940 rank 0
I0623 23:11:08.247613 139741818646912 executor.py:115] TRAIN Batch 18/100 loss 64.885017 loss_att 64.885941 loss_ctc 64.882858 lr 0.00017060 rank 0
I0623 23:11:20.590475 139741818646912 executor.py:115] TRAIN Batch 18/200 loss 52.046528 loss_att 51.417793 loss_ctc 53.513573 lr 0.00017180 rank 0
I0623 23:11:32.636623 139741818646912 executor.py:115] TRAIN Batch 18/300 loss 89.303909 loss_att 86.991188 loss_ctc 94.700256 lr 0.00017300 rank 0
I0623 23:11:44.809543 139741818646912 executor.py:115] TRAIN Batch 18/400 loss 58.251114 loss_att 56.399776 loss_ctc 62.570896 lr 0.00017440 rank 0
I0623 23:11:57.602880 139741818646912 executor.py:115] TRAIN Batch 18/500 loss 37.262714 loss_att 35.805878 loss_ctc 40.662003 lr 0.00017560 rank 0
I0623 23:12:09.325413 139741818646912 executor.py:115] TRAIN Batch 18/600 loss 73.501602 loss_att 73.998199 loss_ctc 72.342857 lr 0.00017680 rank 0
I0623 23:12:21.675470 139741818646912 executor.py:115] TRAIN Batch 18/700 loss 47.792984 loss_att 48.627583 loss_ctc 45.845596 lr 0.00017800 rank 0
I0623 23:12:33.825309 139741818646912 executor.py:152] CV Batch 18/0 loss 34.153351 loss_att 36.189331 loss_ctc 29.402737 history loss 32.144330 rank 0
I0623 23:12:39.355111 139741818646912 train.py:275] Epoch 18 CV info cv_loss 71.20698997779735
I0623 23:12:39.355328 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/18.pt
I0623 23:12:39.465960 139741818646912 train.py:268] Epoch 19 TRAIN info lr 0.00017859999999999998
I0623 23:12:39.467344 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:12:39.467413 139741818646912 executor.py:46] total epoch is 19.
I0623 23:12:47.444897 139741818646912 executor.py:115] TRAIN Batch 19/0 loss 34.630043 loss_att 33.981285 loss_ctc 36.143814 lr 0.00017880 rank 0
I0623 23:13:00.268967 139741818646912 executor.py:115] TRAIN Batch 19/100 loss 64.926544 loss_att 64.901688 loss_ctc 64.984535 lr 0.00018000 rank 0
I0623 23:13:12.193155 139741818646912 executor.py:115] TRAIN Batch 19/200 loss 50.351948 loss_att 50.784760 loss_ctc 49.342052 lr 0.00018120 rank 0
I0623 23:13:23.481233 139741818646912 executor.py:115] TRAIN Batch 19/300 loss 88.400360 loss_att 88.881157 loss_ctc 87.278488 lr 0.00018240 rank 0
I0623 23:13:34.813598 139741818646912 executor.py:115] TRAIN Batch 19/400 loss 61.511002 loss_att 62.793911 loss_ctc 58.517540 lr 0.00018380 rank 0
I0623 23:13:46.908059 139741818646912 executor.py:115] TRAIN Batch 19/500 loss 37.827614 loss_att 38.381634 loss_ctc 36.534897 lr 0.00018500 rank 0
I0623 23:13:57.945143 139741818646912 executor.py:115] TRAIN Batch 19/600 loss 69.930168 loss_att 69.496033 loss_ctc 70.943153 lr 0.00018620 rank 0
I0623 23:14:09.622329 139741818646912 executor.py:115] TRAIN Batch 19/700 loss 49.029465 loss_att 50.235962 loss_ctc 46.214306 lr 0.00018740 rank 0
I0623 23:14:20.989550 139741818646912 executor.py:152] CV Batch 19/0 loss 33.342335 loss_att 36.400169 loss_ctc 26.207386 history loss 31.381021 rank 0
I0623 23:14:26.208657 139741818646912 train.py:275] Epoch 19 CV info cv_loss 69.37660028813023
I0623 23:14:26.208898 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/19.pt
I0623 23:14:26.321447 139741818646912 train.py:268] Epoch 20 TRAIN info lr 0.000188
I0623 23:14:26.322821 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:14:26.322894 139741818646912 executor.py:46] total epoch is 20.
I0623 23:14:34.060645 139741818646912 executor.py:115] TRAIN Batch 20/0 loss 31.085258 loss_att 31.491776 loss_ctc 30.136715 lr 0.00018820 rank 0
I0623 23:14:46.606544 139741818646912 executor.py:115] TRAIN Batch 20/100 loss 77.406570 loss_att 77.916473 loss_ctc 76.216797 lr 0.00018940 rank 0
I0623 23:14:58.208965 139741818646912 executor.py:115] TRAIN Batch 20/200 loss 51.768898 loss_att 51.438866 loss_ctc 52.538979 lr 0.00019060 rank 0
I0623 23:15:09.556050 139741818646912 executor.py:115] TRAIN Batch 20/300 loss 83.202286 loss_att 86.423019 loss_ctc 75.687256 lr 0.00019180 rank 0
I0623 23:15:21.138147 139741818646912 executor.py:115] TRAIN Batch 20/400 loss 51.878014 loss_att 52.042198 loss_ctc 51.494911 lr 0.00019320 rank 0
I0623 23:15:33.102359 139741818646912 executor.py:115] TRAIN Batch 20/500 loss 33.245335 loss_att 33.095192 loss_ctc 33.595673 lr 0.00019440 rank 0
I0623 23:15:43.916556 139741818646912 executor.py:115] TRAIN Batch 20/600 loss 68.174118 loss_att 69.373650 loss_ctc 65.375198 lr 0.00019560 rank 0
I0623 23:15:55.674611 139741818646912 executor.py:115] TRAIN Batch 20/700 loss 52.359062 loss_att 52.241009 loss_ctc 52.634521 lr 0.00019680 rank 0
I0623 23:16:07.076779 139741818646912 executor.py:152] CV Batch 20/0 loss 32.110207 loss_att 35.124821 loss_ctc 25.076115 history loss 30.221371 rank 0
I0623 23:16:12.300169 139741818646912 train.py:275] Epoch 20 CV info cv_loss 67.85885720209733
I0623 23:16:12.300501 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/20.pt
I0623 23:16:12.414904 139741818646912 train.py:268] Epoch 21 TRAIN info lr 0.0001974
I0623 23:16:12.416349 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:16:12.416435 139741818646912 executor.py:46] total epoch is 21.
I0623 23:16:20.220432 139741818646912 executor.py:115] TRAIN Batch 21/0 loss 37.601635 loss_att 37.848164 loss_ctc 37.026405 lr 0.00019760 rank 0
I0623 23:16:32.558092 139741818646912 executor.py:115] TRAIN Batch 21/100 loss 65.484344 loss_att 68.168549 loss_ctc 59.221191 lr 0.00019880 rank 0
I0623 23:16:44.255670 139741818646912 executor.py:115] TRAIN Batch 21/200 loss 47.728424 loss_att 48.405792 loss_ctc 46.147903 lr 0.00020000 rank 0
I0623 23:16:55.466470 139741818646912 executor.py:115] TRAIN Batch 21/300 loss 79.529572 loss_att 82.448189 loss_ctc 72.719467 lr 0.00020120 rank 0
I0623 23:17:06.990974 139741818646912 executor.py:115] TRAIN Batch 21/400 loss 54.939590 loss_att 55.592655 loss_ctc 53.415775 lr 0.00020260 rank 0
I0623 23:17:18.818650 139741818646912 executor.py:115] TRAIN Batch 21/500 loss 34.784443 loss_att 34.260582 loss_ctc 36.006790 lr 0.00020380 rank 0
I0623 23:17:29.813784 139741818646912 executor.py:115] TRAIN Batch 21/600 loss 61.631783 loss_att 63.719055 loss_ctc 56.761490 lr 0.00020500 rank 0
I0623 23:17:41.281336 139741818646912 executor.py:115] TRAIN Batch 21/700 loss 45.234001 loss_att 45.825539 loss_ctc 43.853748 lr 0.00020620 rank 0
I0623 23:17:53.173919 139741818646912 executor.py:152] CV Batch 21/0 loss 31.711931 loss_att 35.209682 loss_ctc 23.550510 history loss 29.846524 rank 0
I0623 23:17:58.595541 139741818646912 train.py:275] Epoch 21 CV info cv_loss 66.35471931239232
I0623 23:17:58.595780 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/21.pt
I0623 23:17:58.713449 139741818646912 train.py:268] Epoch 22 TRAIN info lr 0.00020679999999999999
I0623 23:17:58.714972 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:17:58.715042 139741818646912 executor.py:46] total epoch is 22.
I0623 23:18:06.777259 139741818646912 executor.py:115] TRAIN Batch 22/0 loss 32.081802 loss_att 33.153416 loss_ctc 29.581367 lr 0.00020700 rank 0
I0623 23:18:19.304554 139741818646912 executor.py:115] TRAIN Batch 22/100 loss 62.644375 loss_att 64.267235 loss_ctc 58.857697 lr 0.00020820 rank 0
I0623 23:18:31.414602 139741818646912 executor.py:115] TRAIN Batch 22/200 loss 49.060768 loss_att 51.548126 loss_ctc 43.256935 lr 0.00020940 rank 0
I0623 23:18:42.831507 139741818646912 executor.py:115] TRAIN Batch 22/300 loss 79.988434 loss_att 84.136597 loss_ctc 70.309372 lr 0.00021060 rank 0
I0623 23:18:54.208769 139741818646912 executor.py:115] TRAIN Batch 22/400 loss 57.598343 loss_att 60.166271 loss_ctc 51.606506 lr 0.00021200 rank 0
I0623 23:19:05.925474 139741818646912 executor.py:115] TRAIN Batch 22/500 loss 34.440262 loss_att 35.085697 loss_ctc 32.934250 lr 0.00021320 rank 0
I0623 23:19:16.896800 139741818646912 executor.py:115] TRAIN Batch 22/600 loss 61.076370 loss_att 62.640461 loss_ctc 57.426826 lr 0.00021440 rank 0
I0623 23:19:28.200181 139741818646912 executor.py:115] TRAIN Batch 22/700 loss 47.399975 loss_att 50.342922 loss_ctc 40.533104 lr 0.00021560 rank 0
I0623 23:19:39.564559 139741818646912 executor.py:152] CV Batch 22/0 loss 31.555174 loss_att 35.944252 loss_ctc 21.313990 history loss 29.698987 rank 0
I0623 23:19:44.770454 139741818646912 train.py:275] Epoch 22 CV info cv_loss 65.53387142375725
I0623 23:19:44.770687 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/22.pt
I0623 23:19:44.879359 139741818646912 train.py:268] Epoch 23 TRAIN info lr 0.00021619999999999997
I0623 23:19:44.880546 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:19:44.880611 139741818646912 executor.py:46] total epoch is 23.
I0623 23:19:52.756447 139741818646912 executor.py:115] TRAIN Batch 23/0 loss 29.142464 loss_att 29.369678 loss_ctc 28.612297 lr 0.00021640 rank 0
I0623 23:20:05.224955 139741818646912 executor.py:115] TRAIN Batch 23/100 loss 67.947861 loss_att 70.379456 loss_ctc 62.274128 lr 0.00021760 rank 0
I0623 23:20:17.116533 139741818646912 executor.py:115] TRAIN Batch 23/200 loss 46.486603 loss_att 46.960728 loss_ctc 45.380306 lr 0.00021880 rank 0
I0623 23:20:28.205540 139741818646912 executor.py:115] TRAIN Batch 23/300 loss 81.627487 loss_att 86.570160 loss_ctc 70.094604 lr 0.00022000 rank 0
I0623 23:20:39.489626 139741818646912 executor.py:115] TRAIN Batch 23/400 loss 51.792366 loss_att 55.075420 loss_ctc 44.131905 lr 0.00022140 rank 0
I0623 23:20:51.019372 139741818646912 executor.py:115] TRAIN Batch 23/500 loss 27.741228 loss_att 29.098572 loss_ctc 24.574097 lr 0.00022260 rank 0
I0623 23:21:02.110047 139741818646912 executor.py:115] TRAIN Batch 23/600 loss 65.563652 loss_att 69.498245 loss_ctc 56.382931 lr 0.00022380 rank 0
I0623 23:21:13.645295 139741818646912 executor.py:115] TRAIN Batch 23/700 loss 47.975014 loss_att 50.275391 loss_ctc 42.607471 lr 0.00022500 rank 0
I0623 23:21:25.109826 139741818646912 executor.py:152] CV Batch 23/0 loss 31.118271 loss_att 35.649757 loss_ctc 20.544802 history loss 29.287784 rank 0
I0623 23:21:30.324734 139741818646912 train.py:275] Epoch 23 CV info cv_loss 64.43510944928144
I0623 23:21:30.324967 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/23.pt
I0623 23:21:30.431041 139741818646912 train.py:268] Epoch 24 TRAIN info lr 0.00022559999999999998
I0623 23:21:30.432395 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:21:30.432463 139741818646912 executor.py:46] total epoch is 24.
I0623 23:21:38.249928 139741818646912 executor.py:115] TRAIN Batch 24/0 loss 30.998955 loss_att 32.009163 loss_ctc 28.641806 lr 0.00022580 rank 0
I0623 23:21:50.751683 139741818646912 executor.py:115] TRAIN Batch 24/100 loss 60.218628 loss_att 63.413204 loss_ctc 52.764614 lr 0.00022700 rank 0
I0623 23:22:02.538718 139741818646912 executor.py:115] TRAIN Batch 24/200 loss 49.685547 loss_att 52.590012 loss_ctc 42.908466 lr 0.00022820 rank 0
I0623 23:22:13.659998 139741818646912 executor.py:115] TRAIN Batch 24/300 loss 83.644676 loss_att 88.010773 loss_ctc 73.457123 lr 0.00022940 rank 0
I0623 23:22:25.008754 139741818646912 executor.py:115] TRAIN Batch 24/400 loss 55.915825 loss_att 60.081917 loss_ctc 46.194946 lr 0.00023080 rank 0
I0623 23:22:36.911191 139741818646912 executor.py:115] TRAIN Batch 24/500 loss 32.560795 loss_att 33.592617 loss_ctc 30.153214 lr 0.00023200 rank 0
I0623 23:22:47.942787 139741818646912 executor.py:115] TRAIN Batch 24/600 loss 60.072491 loss_att 64.116318 loss_ctc 50.636902 lr 0.00023320 rank 0
I0623 23:22:59.486236 139741818646912 executor.py:115] TRAIN Batch 24/700 loss 44.713848 loss_att 46.777256 loss_ctc 39.899231 lr 0.00023440 rank 0
I0623 23:23:11.220859 139741818646912 executor.py:152] CV Batch 24/0 loss 30.068811 loss_att 34.395535 loss_ctc 19.973125 history loss 28.300058 rank 0
I0623 23:23:16.459960 139741818646912 train.py:275] Epoch 24 CV info cv_loss 63.35624830055636
I0623 23:23:16.460107 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/24.pt
I0623 23:23:16.564430 139741818646912 train.py:268] Epoch 25 TRAIN info lr 0.000235
I0623 23:23:16.565884 139741818646912 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0623 23:23:16.565954 139741818646912 executor.py:46] total epoch is 25.
I0623 23:23:24.341837 139741818646912 executor.py:115] TRAIN Batch 25/0 loss 29.755955 loss_att 30.507370 loss_ctc 28.002653 lr 0.00023520 rank 0
I0623 23:23:36.485729 139741818646912 executor.py:115] TRAIN Batch 25/100 loss 69.154922 loss_att 73.487267 loss_ctc 59.046112 lr 0.00023640 rank 0
I0623 23:23:48.062117 139741818646912 executor.py:115] TRAIN Batch 25/200 loss 46.268322 loss_att 49.021790 loss_ctc 39.843567 lr 0.00023760 rank 0
I0623 23:23:59.396499 139741818646912 executor.py:115] TRAIN Batch 25/300 loss 79.021576 loss_att 85.625229 loss_ctc 63.613037 lr 0.00023880 rank 0
I0623 23:24:10.809537 139741818646912 executor.py:115] TRAIN Batch 25/400 loss 53.402420 loss_att 56.945797 loss_ctc 45.134541 lr 0.00024020 rank 0
I0623 23:24:22.958282 139741818646912 executor.py:115] TRAIN Batch 25/500 loss 26.649372 loss_att 27.507578 loss_ctc 24.646894 lr 0.00024140 rank 0
I0623 23:24:34.002980 139741818646912 executor.py:115] TRAIN Batch 25/600 loss 61.365051 loss_att 67.035789 loss_ctc 48.133320 lr 0.00024260 rank 0
I0623 23:24:45.608065 139741818646912 executor.py:115] TRAIN Batch 25/700 loss 44.386024 loss_att 47.455784 loss_ctc 37.223259 lr 0.00024380 rank 0
I0623 23:24:57.019366 139741818646912 executor.py:152] CV Batch 25/0 loss 29.804119 loss_att 34.722771 loss_ctc 18.327267 history loss 28.050936 rank 0
I0623 23:25:02.245790 139741818646912 train.py:275] Epoch 25 CV info cv_loss 62.655839833527786
I0623 23:25:02.245941 139741818646912 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/25.pt

