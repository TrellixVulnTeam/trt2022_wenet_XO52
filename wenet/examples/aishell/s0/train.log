I0626 00:29:09.723722 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/init.pt
I0626 00:29:10.417794 139724881797504 train.py:274] Epoch 0 TRAIN info lr 2e-07
I0626 00:29:10.422208 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:29:10.422329 139724881797504 executor.py:46] total epoch is 0.
I0626 00:29:19.601398 139724881797504 executor.py:115] TRAIN Batch 0/0 loss 144.640808 loss_att 49.087700 loss_ctc 367.598053 lr 0.00000020 rank 0
I0626 00:29:30.798437 139724881797504 executor.py:115] TRAIN Batch 0/100 loss 387.985962 loss_att 124.072510 loss_ctc 1003.783936 lr 0.00000140 rank 0
I0626 00:29:55.061312 139724881797504 executor.py:115] TRAIN Batch 0/200 loss 299.902771 loss_att 117.483620 loss_ctc 725.547424 lr 0.00000260 rank 0
I0626 00:30:06.372193 139724881797504 executor.py:115] TRAIN Batch 0/300 loss 228.695724 loss_att 108.018478 loss_ctc 510.275940 lr 0.00000380 rank 0
I0626 00:30:31.396976 139724881797504 executor.py:115] TRAIN Batch 0/400 loss 148.200256 loss_att 82.696190 loss_ctc 301.043091 lr 0.00000520 rank 0
I0626 00:30:56.223259 139724881797504 executor.py:115] TRAIN Batch 0/500 loss 72.174622 loss_att 51.448898 loss_ctc 120.534637 lr 0.00000640 rank 0
I0626 00:31:06.878454 139724881797504 executor.py:115] TRAIN Batch 0/600 loss 168.156433 loss_att 132.149979 loss_ctc 252.171478 lr 0.00000760 rank 0
I0626 00:31:18.387644 139724881797504 executor.py:115] TRAIN Batch 0/700 loss 137.999603 loss_att 122.328949 loss_ctc 174.564484 lr 0.00000880 rank 0
I0626 00:31:43.644660 139724881797504 executor.py:115] TRAIN Batch 0/800 loss 107.581985 loss_att 99.178108 loss_ctc 127.191048 lr 0.00001020 rank 0
I0626 00:32:08.398446 139724881797504 executor.py:115] TRAIN Batch 0/900 loss 80.628471 loss_att 75.684105 loss_ctc 92.165329 lr 0.00001140 rank 0
I0626 00:32:19.777892 139724881797504 executor.py:115] TRAIN Batch 0/1000 loss 45.841438 loss_att 43.833557 loss_ctc 50.526497 lr 0.00001260 rank 0
I0626 00:32:30.776692 139724881797504 executor.py:115] TRAIN Batch 0/1100 loss 137.315720 loss_att 130.010864 loss_ctc 154.360382 lr 0.00001380 rank 0
I0626 00:32:56.564077 139724881797504 executor.py:115] TRAIN Batch 0/1200 loss 120.786011 loss_att 115.030609 loss_ctc 134.215286 lr 0.00001520 rank 0
I0626 00:33:21.280716 139724881797504 executor.py:115] TRAIN Batch 0/1300 loss 105.290436 loss_att 100.916893 loss_ctc 115.495384 lr 0.00001640 rank 0
I0626 00:33:32.983286 139724881797504 executor.py:115] TRAIN Batch 0/1400 loss 91.065506 loss_att 87.644882 loss_ctc 99.046967 lr 0.00001760 rank 0
I0626 00:33:44.071458 139724881797504 executor.py:115] TRAIN Batch 0/1500 loss 111.720901 loss_att 107.276787 loss_ctc 122.090500 lr 0.00001880 rank 0
I0626 00:33:50.814649 139724881797504 executor.py:152] CV Batch 0/0 loss 49.253597 loss_att 47.661663 loss_ctc 52.968109 history loss 43.780975 rank 0
I0626 00:33:55.074811 139724881797504 executor.py:152] CV Batch 0/100 loss 133.780869 loss_att 127.961647 loss_ctc 147.359055 history loss 98.085260 rank 0
I0626 00:33:58.050859 139724881797504 train.py:288] Epoch 0 CV info cv_loss 108.96421574381624
I0626 00:33:58.051108 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/0.pt
I0626 00:33:58.758635 139724881797504 train.py:274] Epoch 1 TRAIN info lr 1.88e-05
I0626 00:33:58.764887 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:33:58.764979 139724881797504 executor.py:46] total epoch is 1.
I0626 00:34:06.493473 139724881797504 executor.py:115] TRAIN Batch 1/0 loss 53.492939 loss_att 51.993523 loss_ctc 56.991573 lr 0.00001900 rank 0
I0626 00:34:17.582162 139724881797504 executor.py:115] TRAIN Batch 1/100 loss 133.242935 loss_att 127.893700 loss_ctc 145.724487 lr 0.00002020 rank 0
I0626 00:34:28.734723 139724881797504 executor.py:115] TRAIN Batch 1/200 loss 104.384842 loss_att 100.063049 loss_ctc 114.469032 lr 0.00002140 rank 0
I0626 00:34:39.914609 139724881797504 executor.py:115] TRAIN Batch 1/300 loss 94.735542 loss_att 90.716171 loss_ctc 104.114075 lr 0.00002260 rank 0
I0626 00:34:51.329200 139724881797504 executor.py:115] TRAIN Batch 1/400 loss 78.797974 loss_att 76.013008 loss_ctc 85.296234 lr 0.00002400 rank 0
I0626 00:35:02.868974 139724881797504 executor.py:115] TRAIN Batch 1/500 loss 39.692822 loss_att 38.515274 loss_ctc 42.440430 lr 0.00002520 rank 0
I0626 00:35:13.411922 139724881797504 executor.py:115] TRAIN Batch 1/600 loss 136.116241 loss_att 130.645325 loss_ctc 148.881683 lr 0.00002640 rank 0
I0626 00:35:24.500895 139724881797504 executor.py:115] TRAIN Batch 1/700 loss 111.537842 loss_att 107.519630 loss_ctc 120.913666 lr 0.00002760 rank 0
I0626 00:35:36.050859 139724881797504 executor.py:115] TRAIN Batch 1/800 loss 88.464417 loss_att 85.194000 loss_ctc 96.095383 lr 0.00002900 rank 0
I0626 00:35:47.613113 139724881797504 executor.py:115] TRAIN Batch 1/900 loss 74.825027 loss_att 71.634987 loss_ctc 82.268463 lr 0.00003020 rank 0
I0626 00:35:59.117664 139724881797504 executor.py:115] TRAIN Batch 1/1000 loss 32.822220 loss_att 31.583097 loss_ctc 35.713501 lr 0.00003140 rank 0
I0626 00:36:09.793029 139724881797504 executor.py:115] TRAIN Batch 1/1100 loss 116.082565 loss_att 110.656433 loss_ctc 128.743530 lr 0.00003260 rank 0
I0626 00:36:21.121012 139724881797504 executor.py:115] TRAIN Batch 1/1200 loss 108.016548 loss_att 102.763412 loss_ctc 120.273865 lr 0.00003400 rank 0
I0626 00:36:32.509832 139724881797504 executor.py:115] TRAIN Batch 1/1300 loss 81.447815 loss_att 77.393860 loss_ctc 90.907043 lr 0.00003520 rank 0
I0626 00:36:43.821646 139724881797504 executor.py:115] TRAIN Batch 1/1400 loss 71.497253 loss_att 68.177589 loss_ctc 79.243149 lr 0.00003640 rank 0
I0626 00:36:54.901000 139724881797504 executor.py:115] TRAIN Batch 1/1500 loss 85.621086 loss_att 81.917801 loss_ctc 94.262077 lr 0.00003760 rank 0
I0626 00:37:00.499822 139724881797504 executor.py:152] CV Batch 1/0 loss 41.881126 loss_att 40.155853 loss_ctc 45.906757 history loss 37.227668 rank 0
I0626 00:37:04.574981 139724881797504 executor.py:152] CV Batch 1/100 loss 114.451675 loss_att 109.409538 loss_ctc 126.216637 history loss 84.042671 rank 0
I0626 00:37:07.554482 139724881797504 train.py:288] Epoch 1 CV info cv_loss 93.82935479663921
I0626 00:37:07.554656 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/1.pt
I0626 00:37:08.252750 139724881797504 train.py:274] Epoch 2 TRAIN info lr 3.76e-05
I0626 00:37:08.255748 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:37:08.255817 139724881797504 executor.py:46] total epoch is 2.
I0626 00:37:16.122819 139724881797504 executor.py:115] TRAIN Batch 2/0 loss 42.711468 loss_att 40.731476 loss_ctc 47.331444 lr 0.00003780 rank 0
I0626 00:37:26.973425 139724881797504 executor.py:115] TRAIN Batch 2/100 loss 124.243729 loss_att 118.602577 loss_ctc 137.406418 lr 0.00003900 rank 0
I0626 00:37:38.002923 139724881797504 executor.py:115] TRAIN Batch 2/200 loss 99.197342 loss_att 94.424927 loss_ctc 110.332962 lr 0.00004020 rank 0
I0626 00:37:49.220746 139724881797504 executor.py:115] TRAIN Batch 2/300 loss 80.672485 loss_att 76.913208 loss_ctc 89.444122 lr 0.00004140 rank 0
I0626 00:38:00.854134 139724881797504 executor.py:115] TRAIN Batch 2/400 loss 64.685684 loss_att 61.772495 loss_ctc 71.483124 lr 0.00004280 rank 0
I0626 00:38:12.512285 139724881797504 executor.py:115] TRAIN Batch 2/500 loss 36.897751 loss_att 35.236053 loss_ctc 40.775040 lr 0.00004400 rank 0
I0626 00:38:23.028169 139724881797504 executor.py:115] TRAIN Batch 2/600 loss 111.216858 loss_att 106.307465 loss_ctc 122.672127 lr 0.00004520 rank 0
I0626 00:38:34.268527 139724881797504 executor.py:115] TRAIN Batch 2/700 loss 95.612511 loss_att 91.505638 loss_ctc 105.195198 lr 0.00004640 rank 0
I0626 00:38:46.105446 139724881797504 executor.py:115] TRAIN Batch 2/800 loss 74.695641 loss_att 71.383850 loss_ctc 82.423157 lr 0.00004780 rank 0
I0626 00:38:57.873470 139724881797504 executor.py:115] TRAIN Batch 2/900 loss 65.994827 loss_att 63.154549 loss_ctc 72.622154 lr 0.00004900 rank 0
I0626 00:39:09.120166 139724881797504 executor.py:115] TRAIN Batch 2/1000 loss 41.432446 loss_att 39.786240 loss_ctc 45.273590 lr 0.00005020 rank 0
I0626 00:39:19.949426 139724881797504 executor.py:115] TRAIN Batch 2/1100 loss 121.931099 loss_att 116.648361 loss_ctc 134.257477 lr 0.00005140 rank 0
I0626 00:39:31.331270 139724881797504 executor.py:115] TRAIN Batch 2/1200 loss 88.145096 loss_att 84.369453 loss_ctc 96.954926 lr 0.00005280 rank 0
I0626 00:39:42.605158 139724881797504 executor.py:115] TRAIN Batch 2/1300 loss 81.336632 loss_att 77.860542 loss_ctc 89.447510 lr 0.00005400 rank 0
I0626 00:39:53.803314 139724881797504 executor.py:115] TRAIN Batch 2/1400 loss 69.330452 loss_att 66.187592 loss_ctc 76.663788 lr 0.00005520 rank 0
I0626 00:40:04.991697 139724881797504 executor.py:115] TRAIN Batch 2/1500 loss 92.306595 loss_att 88.315781 loss_ctc 101.618500 lr 0.00005640 rank 0
I0626 00:40:10.568948 139724881797504 executor.py:152] CV Batch 2/0 loss 40.499851 loss_att 38.851410 loss_ctc 44.346218 history loss 35.999868 rank 0
I0626 00:40:14.603400 139724881797504 executor.py:152] CV Batch 2/100 loss 109.609863 loss_att 104.637497 loss_ctc 121.212036 history loss 80.782365 rank 0
I0626 00:40:17.574044 139724881797504 train.py:288] Epoch 2 CV info cv_loss 90.33341097249405
I0626 00:40:17.574252 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/2.pt
I0626 00:40:18.263092 139724881797504 train.py:274] Epoch 3 TRAIN info lr 5.6399999999999995e-05
I0626 00:40:18.268403 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:40:18.268534 139724881797504 executor.py:46] total epoch is 3.
I0626 00:40:26.106608 139724881797504 executor.py:115] TRAIN Batch 3/0 loss 38.881424 loss_att 37.367798 loss_ctc 42.413223 lr 0.00005660 rank 0
I0626 00:40:37.023791 139724881797504 executor.py:115] TRAIN Batch 3/100 loss 116.032043 loss_att 110.509827 loss_ctc 128.917221 lr 0.00005780 rank 0
I0626 00:40:48.034643 139724881797504 executor.py:115] TRAIN Batch 3/200 loss 92.442429 loss_att 88.130363 loss_ctc 102.503906 lr 0.00005900 rank 0
I0626 00:40:59.694673 139724881797504 executor.py:115] TRAIN Batch 3/300 loss 72.003212 loss_att 68.692116 loss_ctc 79.729095 lr 0.00006020 rank 0
I0626 00:41:11.346143 139724881797504 executor.py:115] TRAIN Batch 3/400 loss 68.374252 loss_att 65.247025 loss_ctc 75.671127 lr 0.00006160 rank 0
I0626 00:41:22.934717 139724881797504 executor.py:115] TRAIN Batch 3/500 loss 35.071690 loss_att 33.746986 loss_ctc 38.162659 lr 0.00006280 rank 0
I0626 00:41:33.194668 139724881797504 executor.py:115] TRAIN Batch 3/600 loss 113.753891 loss_att 108.457672 loss_ctc 126.111717 lr 0.00006400 rank 0
I0626 00:41:44.118786 139724881797504 executor.py:115] TRAIN Batch 3/700 loss 88.947220 loss_att 84.389572 loss_ctc 99.581718 lr 0.00006520 rank 0
I0626 00:41:55.476990 139724881797504 executor.py:115] TRAIN Batch 3/800 loss 82.934258 loss_att 79.171669 loss_ctc 91.713638 lr 0.00006660 rank 0
I0626 00:42:07.049218 139724881797504 executor.py:115] TRAIN Batch 3/900 loss 67.362900 loss_att 64.286865 loss_ctc 74.540306 lr 0.00006780 rank 0
I0626 00:42:18.475489 139724881797504 executor.py:115] TRAIN Batch 3/1000 loss 37.528172 loss_att 36.118599 loss_ctc 40.817177 lr 0.00006900 rank 0
I0626 00:42:29.193921 139724881797504 executor.py:115] TRAIN Batch 3/1100 loss 103.027008 loss_att 98.194504 loss_ctc 114.302834 lr 0.00007020 rank 0
I0626 00:42:40.573090 139724881797504 executor.py:115] TRAIN Batch 3/1200 loss 95.595810 loss_att 91.065140 loss_ctc 106.167358 lr 0.00007160 rank 0
I0626 00:42:52.080916 139724881797504 executor.py:115] TRAIN Batch 3/1300 loss 83.338379 loss_att 78.832321 loss_ctc 93.852524 lr 0.00007280 rank 0
I0626 00:43:03.687285 139724881797504 executor.py:115] TRAIN Batch 3/1400 loss 63.520142 loss_att 60.273296 loss_ctc 71.096107 lr 0.00007400 rank 0
I0626 00:43:14.633071 139724881797504 executor.py:115] TRAIN Batch 3/1500 loss 91.194710 loss_att 86.725830 loss_ctc 101.622101 lr 0.00007520 rank 0
I0626 00:43:20.238897 139724881797504 executor.py:152] CV Batch 3/0 loss 40.508183 loss_att 38.841572 loss_ctc 44.396942 history loss 36.007273 rank 0
I0626 00:43:24.244400 139724881797504 executor.py:152] CV Batch 3/100 loss 107.982819 loss_att 102.102852 loss_ctc 121.702744 history loss 79.429863 rank 0
I0626 00:43:27.232409 139724881797504 train.py:288] Epoch 3 CV info cv_loss 88.92657888110334
I0626 00:43:27.232586 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/3.pt
I0626 00:43:27.902750 139724881797504 train.py:274] Epoch 4 TRAIN info lr 7.52e-05
I0626 00:43:27.908220 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:43:27.908615 139724881797504 executor.py:46] total epoch is 4.
I0626 00:43:35.730649 139724881797504 executor.py:115] TRAIN Batch 4/0 loss 37.734135 loss_att 36.140144 loss_ctc 41.453442 lr 0.00007540 rank 0
I0626 00:43:46.627625 139724881797504 executor.py:115] TRAIN Batch 4/100 loss 117.145782 loss_att 111.359573 loss_ctc 130.646942 lr 0.00007660 rank 0
I0626 00:43:57.704701 139724881797504 executor.py:115] TRAIN Batch 4/200 loss 102.538857 loss_att 97.436966 loss_ctc 114.443283 lr 0.00007780 rank 0
I0626 00:44:08.810276 139724881797504 executor.py:115] TRAIN Batch 4/300 loss 76.466125 loss_att 71.774574 loss_ctc 87.413078 lr 0.00007900 rank 0
I0626 00:44:20.574918 139724881797504 executor.py:115] TRAIN Batch 4/400 loss 68.081879 loss_att 64.232574 loss_ctc 77.063599 lr 0.00008040 rank 0
I0626 00:44:32.043000 139724881797504 executor.py:115] TRAIN Batch 4/500 loss 38.084431 loss_att 36.883492 loss_ctc 40.886620 lr 0.00008160 rank 0
I0626 00:44:42.681720 139724881797504 executor.py:115] TRAIN Batch 4/600 loss 110.177505 loss_att 104.664459 loss_ctc 123.041267 lr 0.00008280 rank 0
I0626 00:44:54.054056 139724881797504 executor.py:115] TRAIN Batch 4/700 loss 89.346878 loss_att 84.660553 loss_ctc 100.281631 lr 0.00008400 rank 0
I0626 00:45:05.764237 139724881797504 executor.py:115] TRAIN Batch 4/800 loss 69.325623 loss_att 64.870323 loss_ctc 79.721329 lr 0.00008540 rank 0
I0626 00:45:17.232444 139724881797504 executor.py:115] TRAIN Batch 4/900 loss 63.733730 loss_att 60.541840 loss_ctc 71.181473 lr 0.00008660 rank 0
I0626 00:45:28.410032 139724881797504 executor.py:115] TRAIN Batch 4/1000 loss 36.120491 loss_att 34.751842 loss_ctc 39.314007 lr 0.00008780 rank 0
I0626 00:45:39.727141 139724881797504 executor.py:115] TRAIN Batch 4/1100 loss 119.872467 loss_att 113.333794 loss_ctc 135.129379 lr 0.00008900 rank 0
I0626 00:45:51.086071 139724881797504 executor.py:115] TRAIN Batch 4/1200 loss 83.077545 loss_att 78.195869 loss_ctc 94.468117 lr 0.00009040 rank 0
I0626 00:46:02.411869 139724881797504 executor.py:115] TRAIN Batch 4/1300 loss 71.034378 loss_att 66.546555 loss_ctc 81.505966 lr 0.00009160 rank 0
I0626 00:46:13.584332 139724881797504 executor.py:115] TRAIN Batch 4/1400 loss 60.253624 loss_att 56.520863 loss_ctc 68.963409 lr 0.00009280 rank 0
I0626 00:46:24.564127 139724881797504 executor.py:115] TRAIN Batch 4/1500 loss 98.915886 loss_att 92.291229 loss_ctc 114.373413 lr 0.00009400 rank 0
I0626 00:46:30.112810 139724881797504 executor.py:152] CV Batch 4/0 loss 40.067234 loss_att 38.356499 loss_ctc 44.058952 history loss 35.615319 rank 0
I0626 00:46:34.151887 139724881797504 executor.py:152] CV Batch 4/100 loss 105.455681 loss_att 98.991920 loss_ctc 120.537796 history loss 77.595945 rank 0
I0626 00:46:37.117590 139724881797504 train.py:288] Epoch 4 CV info cv_loss 86.94638900597114
I0626 00:46:37.117814 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/4.pt
I0626 00:46:37.799737 139724881797504 train.py:274] Epoch 5 TRAIN info lr 9.4e-05
I0626 00:46:37.805220 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:46:37.806000 139724881797504 executor.py:46] total epoch is 5.
I0626 00:46:45.631023 139724881797504 executor.py:115] TRAIN Batch 5/0 loss 42.584400 loss_att 40.399353 loss_ctc 47.682838 lr 0.00009420 rank 0
I0626 00:46:57.112481 139724881797504 executor.py:115] TRAIN Batch 5/100 loss 111.644547 loss_att 105.266769 loss_ctc 126.526009 lr 0.00009540 rank 0
I0626 00:47:08.393449 139724881797504 executor.py:115] TRAIN Batch 5/200 loss 96.328400 loss_att 90.623825 loss_ctc 109.639084 lr 0.00009660 rank 0
I0626 00:47:19.872323 139724881797504 executor.py:115] TRAIN Batch 5/300 loss 76.404747 loss_att 71.317406 loss_ctc 88.275200 lr 0.00009780 rank 0
I0626 00:47:31.462074 139724881797504 executor.py:115] TRAIN Batch 5/400 loss 56.066666 loss_att 50.945290 loss_ctc 68.016548 lr 0.00009920 rank 0
I0626 00:47:43.040360 139724881797504 executor.py:115] TRAIN Batch 5/500 loss 33.258568 loss_att 31.550961 loss_ctc 37.242989 lr 0.00010040 rank 0
I0626 00:47:53.950191 139724881797504 executor.py:115] TRAIN Batch 5/600 loss 115.406242 loss_att 107.182434 loss_ctc 134.595123 lr 0.00010160 rank 0
I0626 00:48:05.530639 139724881797504 executor.py:115] TRAIN Batch 5/700 loss 81.881355 loss_att 75.776215 loss_ctc 96.126686 lr 0.00010280 rank 0
I0626 00:48:17.019600 139724881797504 executor.py:115] TRAIN Batch 5/800 loss 76.340790 loss_att 70.020386 loss_ctc 91.088402 lr 0.00010420 rank 0
I0626 00:48:28.484357 139724881797504 executor.py:115] TRAIN Batch 5/900 loss 63.916279 loss_att 59.591377 loss_ctc 74.007721 lr 0.00010540 rank 0
I0626 00:48:39.765732 139724881797504 executor.py:115] TRAIN Batch 5/1000 loss 38.905880 loss_att 36.920853 loss_ctc 43.537609 lr 0.00010660 rank 0
I0626 00:48:50.564107 139724881797504 executor.py:115] TRAIN Batch 5/1100 loss 106.917862 loss_att 98.206329 loss_ctc 127.244751 lr 0.00010780 rank 0
I0626 00:49:01.742130 139724881797504 executor.py:115] TRAIN Batch 5/1200 loss 87.694168 loss_att 81.109451 loss_ctc 103.058502 lr 0.00010920 rank 0
I0626 00:49:12.997961 139724881797504 executor.py:115] TRAIN Batch 5/1300 loss 78.667152 loss_att 73.986061 loss_ctc 89.589691 lr 0.00011040 rank 0
I0626 00:49:24.189034 139724881797504 executor.py:115] TRAIN Batch 5/1400 loss 57.896343 loss_att 53.029663 loss_ctc 69.251938 lr 0.00011160 rank 0
I0626 00:49:35.207828 139724881797504 executor.py:115] TRAIN Batch 5/1500 loss 70.599556 loss_att 65.742966 loss_ctc 81.931602 lr 0.00011280 rank 0
I0626 00:49:40.772249 139724881797504 executor.py:152] CV Batch 5/0 loss 39.267632 loss_att 37.290260 loss_ctc 43.881493 history loss 34.904561 rank 0
I0626 00:49:44.871382 139724881797504 executor.py:152] CV Batch 5/100 loss 102.949783 loss_att 95.490852 loss_ctc 120.353958 history loss 75.790240 rank 0
I0626 00:49:47.855323 139724881797504 train.py:288] Epoch 5 CV info cv_loss 84.9245000441259
I0626 00:49:47.855562 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/5.pt
I0626 00:49:48.542854 139724881797504 train.py:274] Epoch 6 TRAIN info lr 0.00011279999999999999
I0626 00:49:48.548574 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:49:48.548926 139724881797504 executor.py:46] total epoch is 6.
I0626 00:49:56.341328 139724881797504 executor.py:115] TRAIN Batch 6/0 loss 29.749241 loss_att 28.004070 loss_ctc 33.821308 lr 0.00011300 rank 0
I0626 00:50:07.182065 139724881797504 executor.py:115] TRAIN Batch 6/100 loss 104.616318 loss_att 96.736839 loss_ctc 123.001770 lr 0.00011420 rank 0
I0626 00:50:18.282189 139724881797504 executor.py:115] TRAIN Batch 6/200 loss 82.894249 loss_att 75.038147 loss_ctc 101.225143 lr 0.00011540 rank 0
I0626 00:50:29.547933 139724881797504 executor.py:115] TRAIN Batch 6/300 loss 67.732208 loss_att 60.747475 loss_ctc 84.029915 lr 0.00011660 rank 0
I0626 00:50:41.040722 139724881797504 executor.py:115] TRAIN Batch 6/400 loss 57.242039 loss_att 52.295624 loss_ctc 68.783669 lr 0.00011800 rank 0
I0626 00:50:52.810096 139724881797504 executor.py:115] TRAIN Batch 6/500 loss 43.579006 loss_att 41.371429 loss_ctc 48.730015 lr 0.00011920 rank 0
I0626 00:51:03.578163 139724881797504 executor.py:115] TRAIN Batch 6/600 loss 92.883766 loss_att 83.933739 loss_ctc 113.767166 lr 0.00012040 rank 0
I0626 00:51:14.928943 139724881797504 executor.py:115] TRAIN Batch 6/700 loss 86.419800 loss_att 77.668686 loss_ctc 106.839050 lr 0.00012160 rank 0
I0626 00:51:26.480206 139724881797504 executor.py:115] TRAIN Batch 6/800 loss 74.901886 loss_att 68.287834 loss_ctc 90.334686 lr 0.00012300 rank 0
I0626 00:51:37.903976 139724881797504 executor.py:115] TRAIN Batch 6/900 loss 55.092804 loss_att 49.776081 loss_ctc 67.498489 lr 0.00012420 rank 0
I0626 00:51:49.146846 139724881797504 executor.py:115] TRAIN Batch 6/1000 loss 36.837769 loss_att 34.000221 loss_ctc 43.458717 lr 0.00012540 rank 0
I0626 00:51:59.767326 139724881797504 executor.py:115] TRAIN Batch 6/1100 loss 95.687256 loss_att 86.436859 loss_ctc 117.271507 lr 0.00012660 rank 0
I0626 00:52:11.032988 139724881797504 executor.py:115] TRAIN Batch 6/1200 loss 88.181152 loss_att 81.117744 loss_ctc 104.662445 lr 0.00012800 rank 0
I0626 00:52:22.580014 139724881797504 executor.py:115] TRAIN Batch 6/1300 loss 71.492058 loss_att 64.840210 loss_ctc 87.013046 lr 0.00012920 rank 0
I0626 00:52:33.967391 139724881797504 executor.py:115] TRAIN Batch 6/1400 loss 59.648350 loss_att 53.376076 loss_ctc 74.283661 lr 0.00013040 rank 0
I0626 00:52:45.233787 139724881797504 executor.py:115] TRAIN Batch 6/1500 loss 74.935272 loss_att 69.130119 loss_ctc 88.480637 lr 0.00013160 rank 0
I0626 00:52:50.794239 139724881797504 executor.py:152] CV Batch 6/0 loss 39.106922 loss_att 36.757668 loss_ctc 44.588509 history loss 34.761709 rank 0
I0626 00:52:54.800314 139724881797504 executor.py:152] CV Batch 6/100 loss 100.758224 loss_att 92.582664 loss_ctc 119.834557 history loss 74.308488 rank 0
I0626 00:52:57.777232 139724881797504 train.py:288] Epoch 6 CV info cv_loss 83.21780438100384
I0626 00:52:57.777481 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/6.pt
I0626 00:52:58.458358 139724881797504 train.py:274] Epoch 7 TRAIN info lr 0.0001316
I0626 00:52:58.463687 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:52:58.463826 139724881797504 executor.py:46] total epoch is 7.
I0626 00:53:06.247900 139724881797504 executor.py:115] TRAIN Batch 7/0 loss 39.221043 loss_att 36.708286 loss_ctc 45.084141 lr 0.00013180 rank 0
I0626 00:53:16.880591 139724881797504 executor.py:115] TRAIN Batch 7/100 loss 103.555290 loss_att 94.410706 loss_ctc 124.892654 lr 0.00013300 rank 0
I0626 00:53:28.309753 139724881797504 executor.py:115] TRAIN Batch 7/200 loss 73.402588 loss_att 66.755989 loss_ctc 88.911316 lr 0.00013420 rank 0
I0626 00:53:39.605901 139724881797504 executor.py:115] TRAIN Batch 7/300 loss 69.650558 loss_att 63.146919 loss_ctc 84.825729 lr 0.00013540 rank 0
I0626 00:53:51.052304 139724881797504 executor.py:115] TRAIN Batch 7/400 loss 69.280205 loss_att 63.409931 loss_ctc 82.977516 lr 0.00013680 rank 0
I0626 00:54:02.641366 139724881797504 executor.py:115] TRAIN Batch 7/500 loss 37.889481 loss_att 35.696808 loss_ctc 43.005718 lr 0.00013800 rank 0
I0626 00:54:13.279272 139724881797504 executor.py:115] TRAIN Batch 7/600 loss 97.660248 loss_att 88.962555 loss_ctc 117.954872 lr 0.00013920 rank 0
I0626 00:54:24.851108 139724881797504 executor.py:115] TRAIN Batch 7/700 loss 82.920586 loss_att 74.338181 loss_ctc 102.946198 lr 0.00014040 rank 0
I0626 00:54:36.672098 139724881797504 executor.py:115] TRAIN Batch 7/800 loss 79.783859 loss_att 72.261864 loss_ctc 97.335190 lr 0.00014180 rank 0
I0626 00:54:48.421866 139724881797504 executor.py:115] TRAIN Batch 7/900 loss 61.821648 loss_att 56.839016 loss_ctc 73.447777 lr 0.00014300 rank 0
I0626 00:54:59.469012 139724881797504 executor.py:115] TRAIN Batch 7/1000 loss 37.004002 loss_att 34.484272 loss_ctc 42.883369 lr 0.00014420 rank 0
I0626 00:55:10.340812 139724881797504 executor.py:115] TRAIN Batch 7/1100 loss 101.908585 loss_att 92.203156 loss_ctc 124.554596 lr 0.00014540 rank 0
I0626 00:55:21.542091 139724881797504 executor.py:115] TRAIN Batch 7/1200 loss 73.373596 loss_att 65.024162 loss_ctc 92.855591 lr 0.00014680 rank 0
I0626 00:55:32.592790 139724881797504 executor.py:115] TRAIN Batch 7/1300 loss 71.641449 loss_att 63.788834 loss_ctc 89.964203 lr 0.00014800 rank 0
I0626 00:55:43.807991 139724881797504 executor.py:115] TRAIN Batch 7/1400 loss 63.352577 loss_att 57.521252 loss_ctc 76.959000 lr 0.00014920 rank 0
I0626 00:55:54.830404 139724881797504 executor.py:115] TRAIN Batch 7/1500 loss 71.328217 loss_att 61.925701 loss_ctc 93.267418 lr 0.00015040 rank 0
I0626 00:56:00.438165 139724881797504 executor.py:152] CV Batch 7/0 loss 38.059517 loss_att 35.863411 loss_ctc 43.183769 history loss 33.830682 rank 0
I0626 00:56:04.493779 139724881797504 executor.py:152] CV Batch 7/100 loss 99.828644 loss_att 91.148308 loss_ctc 120.082771 history loss 73.191615 rank 0
I0626 00:56:07.486401 139724881797504 train.py:288] Epoch 7 CV info cv_loss 82.03851612248124
I0626 00:56:07.486544 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/7.pt
I0626 00:56:08.174274 139724881797504 train.py:274] Epoch 8 TRAIN info lr 0.0001504
I0626 00:56:08.180567 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:56:08.180718 139724881797504 executor.py:46] total epoch is 8.
I0626 00:56:15.963575 139724881797504 executor.py:115] TRAIN Batch 8/0 loss 40.472313 loss_att 37.169746 loss_ctc 48.178299 lr 0.00015060 rank 0
I0626 00:56:27.074118 139724881797504 executor.py:115] TRAIN Batch 8/100 loss 103.300003 loss_att 92.820984 loss_ctc 127.751068 lr 0.00015180 rank 0
I0626 00:56:38.276003 139724881797504 executor.py:115] TRAIN Batch 8/200 loss 74.929474 loss_att 67.214180 loss_ctc 92.931808 lr 0.00015300 rank 0
I0626 00:56:49.844320 139724881797504 executor.py:115] TRAIN Batch 8/300 loss 71.829727 loss_att 63.983070 loss_ctc 90.138596 lr 0.00015420 rank 0
I0626 00:57:01.413715 139724881797504 executor.py:115] TRAIN Batch 8/400 loss 59.833504 loss_att 53.932930 loss_ctc 73.601501 lr 0.00015560 rank 0
I0626 00:57:13.128679 139724881797504 executor.py:115] TRAIN Batch 8/500 loss 35.139557 loss_att 32.418438 loss_ctc 41.488831 lr 0.00015680 rank 0
I0626 00:57:23.716646 139724881797504 executor.py:115] TRAIN Batch 8/600 loss 115.284119 loss_att 102.354248 loss_ctc 145.453827 lr 0.00015800 rank 0
I0626 00:57:34.638600 139724881797504 executor.py:115] TRAIN Batch 8/700 loss 83.216408 loss_att 75.297455 loss_ctc 101.693970 lr 0.00015920 rank 0
I0626 00:57:45.851767 139724881797504 executor.py:115] TRAIN Batch 8/800 loss 73.695320 loss_att 65.161697 loss_ctc 93.607117 lr 0.00016060 rank 0
I0626 00:57:57.168693 139724881797504 executor.py:115] TRAIN Batch 8/900 loss 59.556267 loss_att 54.558819 loss_ctc 71.216972 lr 0.00016180 rank 0
I0626 00:58:08.301844 139724881797504 executor.py:115] TRAIN Batch 8/1000 loss 37.840553 loss_att 35.121159 loss_ctc 44.185806 lr 0.00016300 rank 0
I0626 00:58:18.981327 139724881797504 executor.py:115] TRAIN Batch 8/1100 loss 102.536949 loss_att 90.013733 loss_ctc 131.757782 lr 0.00016420 rank 0
I0626 00:58:30.234819 139724881797504 executor.py:115] TRAIN Batch 8/1200 loss 85.728203 loss_att 75.931908 loss_ctc 108.586212 lr 0.00016560 rank 0
I0626 00:58:41.473803 139724881797504 executor.py:115] TRAIN Batch 8/1300 loss 66.574013 loss_att 58.923439 loss_ctc 84.425346 lr 0.00016680 rank 0
I0626 00:58:52.559367 139724881797504 executor.py:115] TRAIN Batch 8/1400 loss 64.465469 loss_att 58.206196 loss_ctc 79.070435 lr 0.00016800 rank 0
I0626 00:59:03.587374 139724881797504 executor.py:115] TRAIN Batch 8/1500 loss 91.491653 loss_att 81.839783 loss_ctc 114.012695 lr 0.00016920 rank 0
I0626 00:59:09.153987 139724881797504 executor.py:152] CV Batch 8/0 loss 37.742622 loss_att 35.391914 loss_ctc 43.227608 history loss 33.548998 rank 0
I0626 00:59:13.232116 139724881797504 executor.py:152] CV Batch 8/100 loss 98.772438 loss_att 89.679688 loss_ctc 119.988853 history loss 72.362479 rank 0
I0626 00:59:16.196995 139724881797504 train.py:288] Epoch 8 CV info cv_loss 81.11547453321948
I0626 00:59:16.197170 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/8.pt
I0626 00:59:16.461163 139724881797504 train.py:274] Epoch 9 TRAIN info lr 0.00016919999999999997
I0626 00:59:16.463997 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 00:59:16.464062 139724881797504 executor.py:46] total epoch is 9.
I0626 00:59:24.270244 139724881797504 executor.py:115] TRAIN Batch 9/0 loss 33.305138 loss_att 30.561440 loss_ctc 39.707100 lr 0.00016940 rank 0
I0626 00:59:35.882177 139724881797504 executor.py:115] TRAIN Batch 9/100 loss 94.562103 loss_att 83.257980 loss_ctc 120.938377 lr 0.00017060 rank 0
I0626 00:59:47.021891 139724881797504 executor.py:115] TRAIN Batch 9/200 loss 79.623230 loss_att 70.804489 loss_ctc 100.200287 lr 0.00017180 rank 0
I0626 00:59:58.339371 139724881797504 executor.py:115] TRAIN Batch 9/300 loss 64.275299 loss_att 55.771194 loss_ctc 84.118195 lr 0.00017300 rank 0
I0626 01:00:09.866368 139724881797504 executor.py:115] TRAIN Batch 9/400 loss 56.086349 loss_att 49.599762 loss_ctc 71.221725 lr 0.00017440 rank 0
I0626 01:00:21.706756 139724881797504 executor.py:115] TRAIN Batch 9/500 loss 33.852402 loss_att 31.481346 loss_ctc 39.384861 lr 0.00017560 rank 0
I0626 01:00:32.556488 139724881797504 executor.py:115] TRAIN Batch 9/600 loss 101.567467 loss_att 91.470833 loss_ctc 125.126282 lr 0.00017680 rank 0
I0626 01:00:43.917579 139724881797504 executor.py:115] TRAIN Batch 9/700 loss 71.805809 loss_att 64.113167 loss_ctc 89.755310 lr 0.00017800 rank 0
I0626 01:00:55.488843 139724881797504 executor.py:115] TRAIN Batch 9/800 loss 66.587234 loss_att 60.051758 loss_ctc 81.836685 lr 0.00017940 rank 0
I0626 01:01:07.005280 139724881797504 executor.py:115] TRAIN Batch 9/900 loss 57.290237 loss_att 51.028236 loss_ctc 71.901581 lr 0.00018060 rank 0
I0626 01:01:18.467183 139724881797504 executor.py:115] TRAIN Batch 9/1000 loss 33.120892 loss_att 31.031239 loss_ctc 37.996750 lr 0.00018180 rank 0
I0626 01:01:29.396116 139724881797504 executor.py:115] TRAIN Batch 9/1100 loss 100.327232 loss_att 89.929008 loss_ctc 124.589752 lr 0.00018300 rank 0
I0626 01:01:40.864428 139724881797504 executor.py:115] TRAIN Batch 9/1200 loss 82.638557 loss_att 72.743362 loss_ctc 105.727341 lr 0.00018440 rank 0
I0626 01:01:52.317284 139724881797504 executor.py:115] TRAIN Batch 9/1300 loss 69.300827 loss_att 62.675194 loss_ctc 84.760635 lr 0.00018560 rank 0
I0626 01:02:03.793255 139724881797504 executor.py:115] TRAIN Batch 9/1400 loss 54.910141 loss_att 50.076607 loss_ctc 66.188385 lr 0.00018680 rank 0
I0626 01:02:14.699288 139724881797504 executor.py:115] TRAIN Batch 9/1500 loss 60.467308 loss_att 53.868542 loss_ctc 75.864426 lr 0.00018800 rank 0
I0626 01:02:20.281662 139724881797504 executor.py:152] CV Batch 9/0 loss 37.829239 loss_att 35.580006 loss_ctc 43.077454 history loss 33.625990 rank 0
I0626 01:02:24.315843 139724881797504 executor.py:152] CV Batch 9/100 loss 98.388428 loss_att 88.906418 loss_ctc 120.513092 history loss 71.938872 rank 0
I0626 01:02:27.288599 139724881797504 train.py:288] Epoch 9 CV info cv_loss 80.60242849218204
I0626 01:02:27.288803 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/9.pt
I0626 01:02:27.552471 139724881797504 train.py:274] Epoch 10 TRAIN info lr 0.000188
I0626 01:02:27.555406 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:02:27.555468 139724881797504 executor.py:46] total epoch is 10.
I0626 01:02:35.290604 139724881797504 executor.py:115] TRAIN Batch 10/0 loss 33.052147 loss_att 29.976921 loss_ctc 40.227669 lr 0.00018820 rank 0
I0626 01:02:45.895805 139724881797504 executor.py:115] TRAIN Batch 10/100 loss 103.776352 loss_att 93.363205 loss_ctc 128.073700 lr 0.00018940 rank 0
I0626 01:02:57.292260 139724881797504 executor.py:115] TRAIN Batch 10/200 loss 77.431702 loss_att 67.890793 loss_ctc 99.693810 lr 0.00019060 rank 0
I0626 01:03:08.462005 139724881797504 executor.py:115] TRAIN Batch 10/300 loss 69.324608 loss_att 61.647110 loss_ctc 87.238770 lr 0.00019180 rank 0
I0626 01:03:19.806763 139724881797504 executor.py:115] TRAIN Batch 10/400 loss 53.061050 loss_att 47.723965 loss_ctc 65.514252 lr 0.00019320 rank 0
I0626 01:03:31.616763 139724881797504 executor.py:115] TRAIN Batch 10/500 loss 27.687819 loss_att 26.223661 loss_ctc 31.104181 lr 0.00019440 rank 0
I0626 01:03:42.050180 139724881797504 executor.py:115] TRAIN Batch 10/600 loss 101.645752 loss_att 92.092484 loss_ctc 123.936707 lr 0.00019560 rank 0
I0626 01:03:53.435466 139724881797504 executor.py:115] TRAIN Batch 10/700 loss 73.765175 loss_att 63.558643 loss_ctc 97.580414 lr 0.00019680 rank 0
I0626 01:04:04.880219 139724881797504 executor.py:115] TRAIN Batch 10/800 loss 69.552345 loss_att 63.066002 loss_ctc 84.687134 lr 0.00019820 rank 0
I0626 01:04:16.599502 139724881797504 executor.py:115] TRAIN Batch 10/900 loss 56.205189 loss_att 51.045151 loss_ctc 68.245277 lr 0.00019940 rank 0
I0626 01:04:28.021729 139724881797504 executor.py:115] TRAIN Batch 10/1000 loss 32.092800 loss_att 28.700039 loss_ctc 40.009247 lr 0.00020060 rank 0
I0626 01:04:38.868925 139724881797504 executor.py:115] TRAIN Batch 10/1100 loss 105.351151 loss_att 93.739258 loss_ctc 132.445557 lr 0.00020180 rank 0
I0626 01:04:50.455929 139724881797504 executor.py:115] TRAIN Batch 10/1200 loss 74.108994 loss_att 63.596718 loss_ctc 98.637634 lr 0.00020320 rank 0
I0626 01:05:01.532098 139724881797504 executor.py:115] TRAIN Batch 10/1300 loss 60.860634 loss_att 52.155533 loss_ctc 81.172539 lr 0.00020440 rank 0
I0626 01:05:12.499179 139724881797504 executor.py:115] TRAIN Batch 10/1400 loss 66.223495 loss_att 59.832527 loss_ctc 81.135765 lr 0.00020560 rank 0
I0626 01:05:23.332414 139724881797504 executor.py:115] TRAIN Batch 10/1500 loss 80.690353 loss_att 72.138695 loss_ctc 100.644211 lr 0.00020680 rank 0
I0626 01:05:28.893607 139724881797504 executor.py:152] CV Batch 10/0 loss 37.077271 loss_att 34.178207 loss_ctc 43.841743 history loss 32.957574 rank 0
I0626 01:05:32.979140 139724881797504 executor.py:152] CV Batch 10/100 loss 97.258713 loss_att 87.515564 loss_ctc 119.992737 history loss 71.138495 rank 0
I0626 01:05:35.950756 139724881797504 train.py:288] Epoch 10 CV info cv_loss 79.78844878362428
I0626 01:05:35.950969 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/10.pt
I0626 01:05:36.210788 139724881797504 train.py:274] Epoch 11 TRAIN info lr 0.00020679999999999999
I0626 01:05:36.213900 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:05:36.213961 139724881797504 executor.py:46] total epoch is 11.
I0626 01:05:44.004477 139724881797504 executor.py:115] TRAIN Batch 11/0 loss 30.858532 loss_att 27.421139 loss_ctc 38.879116 lr 0.00020700 rank 0
I0626 01:05:54.903793 139724881797504 executor.py:115] TRAIN Batch 11/100 loss 93.023811 loss_att 82.385117 loss_ctc 117.847420 lr 0.00020820 rank 0
I0626 01:06:05.753612 139724881797504 executor.py:115] TRAIN Batch 11/200 loss 80.090591 loss_att 71.293686 loss_ctc 100.616714 lr 0.00020940 rank 0
I0626 01:06:16.959492 139724881797504 executor.py:115] TRAIN Batch 11/300 loss 64.747902 loss_att 55.282513 loss_ctc 86.833817 lr 0.00021060 rank 0
I0626 01:06:28.457026 139724881797504 executor.py:115] TRAIN Batch 11/400 loss 60.976585 loss_att 54.731438 loss_ctc 75.548599 lr 0.00021200 rank 0
I0626 01:06:39.907575 139724881797504 executor.py:115] TRAIN Batch 11/500 loss 29.991087 loss_att 27.202425 loss_ctc 36.497967 lr 0.00021320 rank 0
I0626 01:06:50.733588 139724881797504 executor.py:115] TRAIN Batch 11/600 loss 92.679100 loss_att 81.084213 loss_ctc 119.733826 lr 0.00021440 rank 0
I0626 01:07:01.848688 139724881797504 executor.py:115] TRAIN Batch 11/700 loss 71.325645 loss_att 60.546211 loss_ctc 96.477661 lr 0.00021560 rank 0
I0626 01:07:13.127607 139724881797504 executor.py:115] TRAIN Batch 11/800 loss 71.320389 loss_att 62.075272 loss_ctc 92.892334 lr 0.00021700 rank 0
I0626 01:07:24.652888 139724881797504 executor.py:115] TRAIN Batch 11/900 loss 58.885193 loss_att 53.061150 loss_ctc 72.474632 lr 0.00021820 rank 0
I0626 01:07:35.814337 139724881797504 executor.py:115] TRAIN Batch 11/1000 loss 33.988541 loss_att 31.055496 loss_ctc 40.832302 lr 0.00021940 rank 0
I0626 01:07:46.492752 139724881797504 executor.py:115] TRAIN Batch 11/1100 loss 97.358627 loss_att 84.267815 loss_ctc 127.903847 lr 0.00022060 rank 0
I0626 01:07:57.783639 139724881797504 executor.py:115] TRAIN Batch 11/1200 loss 81.976944 loss_att 73.022186 loss_ctc 102.871384 lr 0.00022200 rank 0
I0626 01:08:09.343594 139724881797504 executor.py:115] TRAIN Batch 11/1300 loss 67.432381 loss_att 58.418789 loss_ctc 88.464088 lr 0.00022320 rank 0
I0626 01:08:20.707733 139724881797504 executor.py:115] TRAIN Batch 11/1400 loss 58.260715 loss_att 51.857983 loss_ctc 73.200424 lr 0.00022440 rank 0
I0626 01:08:31.887418 139724881797504 executor.py:115] TRAIN Batch 11/1500 loss 66.599800 loss_att 58.121109 loss_ctc 86.383408 lr 0.00022560 rank 0
I0626 01:08:37.469767 139724881797504 executor.py:152] CV Batch 11/0 loss 35.391953 loss_att 31.960032 loss_ctc 43.399761 history loss 31.459513 rank 0
I0626 01:08:41.437653 139724881797504 executor.py:152] CV Batch 11/100 loss 96.402634 loss_att 86.437721 loss_ctc 119.654083 history loss 70.251018 rank 0
I0626 01:08:44.410884 139724881797504 train.py:288] Epoch 11 CV info cv_loss 79.19263033923451
I0626 01:08:44.411055 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/11.pt
I0626 01:08:44.669044 139724881797504 train.py:274] Epoch 12 TRAIN info lr 0.00022559999999999998
I0626 01:08:44.672059 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:08:44.672130 139724881797504 executor.py:46] total epoch is 12.
I0626 01:08:52.455857 139724881797504 executor.py:115] TRAIN Batch 12/0 loss 35.983059 loss_att 33.601612 loss_ctc 41.539764 lr 0.00022580 rank 0
I0626 01:09:03.021801 139724881797504 executor.py:115] TRAIN Batch 12/100 loss 90.315552 loss_att 78.161125 loss_ctc 118.675888 lr 0.00022700 rank 0
I0626 01:09:14.260616 139724881797504 executor.py:115] TRAIN Batch 12/200 loss 69.451317 loss_att 58.130936 loss_ctc 95.865532 lr 0.00022820 rank 0
I0626 01:09:25.498176 139724881797504 executor.py:115] TRAIN Batch 12/300 loss 66.512558 loss_att 56.619766 loss_ctc 89.595734 lr 0.00022940 rank 0
I0626 01:09:37.294407 139724881797504 executor.py:115] TRAIN Batch 12/400 loss 60.105415 loss_att 55.103100 loss_ctc 71.777489 lr 0.00023080 rank 0
I0626 01:09:49.094326 139724881797504 executor.py:115] TRAIN Batch 12/500 loss 42.784916 loss_att 37.949501 loss_ctc 54.067551 lr 0.00023200 rank 0
I0626 01:09:59.541178 139724881797504 executor.py:115] TRAIN Batch 12/600 loss 105.019516 loss_att 91.095627 loss_ctc 137.508575 lr 0.00023320 rank 0
I0626 01:10:10.815139 139724881797504 executor.py:115] TRAIN Batch 12/700 loss 80.779144 loss_att 70.028290 loss_ctc 105.864464 lr 0.00023440 rank 0
I0626 01:10:22.454120 139724881797504 executor.py:115] TRAIN Batch 12/800 loss 61.608192 loss_att 53.661526 loss_ctc 80.150406 lr 0.00023580 rank 0
I0626 01:10:33.960624 139724881797504 executor.py:115] TRAIN Batch 12/900 loss 49.742188 loss_att 43.201275 loss_ctc 65.004311 lr 0.00023700 rank 0
I0626 01:10:45.020984 139724881797504 executor.py:115] TRAIN Batch 12/1000 loss 31.831734 loss_att 27.788883 loss_ctc 41.265053 lr 0.00023820 rank 0
I0626 01:10:55.625218 139724881797504 executor.py:115] TRAIN Batch 12/1100 loss 93.145737 loss_att 78.927925 loss_ctc 126.320648 lr 0.00023940 rank 0
I0626 01:11:07.118979 139724881797504 executor.py:115] TRAIN Batch 12/1200 loss 74.119049 loss_att 63.237068 loss_ctc 99.510353 lr 0.00024080 rank 0
I0626 01:11:18.293959 139724881797504 executor.py:115] TRAIN Batch 12/1300 loss 70.658989 loss_att 62.051624 loss_ctc 90.742828 lr 0.00024200 rank 0
I0626 01:11:29.428662 139724881797504 executor.py:115] TRAIN Batch 12/1400 loss 48.833344 loss_att 41.241547 loss_ctc 66.547546 lr 0.00024320 rank 0
I0626 01:11:40.391084 139724881797504 executor.py:115] TRAIN Batch 12/1500 loss 67.350235 loss_att 60.197674 loss_ctc 84.039543 lr 0.00024440 rank 0
I0626 01:11:45.956566 139724881797504 executor.py:152] CV Batch 12/0 loss 37.406120 loss_att 35.046013 loss_ctc 42.913029 history loss 33.249885 rank 0
I0626 01:11:50.027160 139724881797504 executor.py:152] CV Batch 12/100 loss 96.224510 loss_att 86.087769 loss_ctc 119.876907 history loss 70.602621 rank 0
I0626 01:11:53.014829 139724881797504 train.py:288] Epoch 12 CV info cv_loss 79.22911370990165
I0626 01:11:53.015045 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/12.pt
I0626 01:11:53.275670 139724881797504 train.py:274] Epoch 13 TRAIN info lr 0.0002444
I0626 01:11:53.278694 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:11:53.278754 139724881797504 executor.py:46] total epoch is 13.
I0626 01:12:01.188776 139724881797504 executor.py:115] TRAIN Batch 13/0 loss 31.870045 loss_att 28.059517 loss_ctc 40.761276 lr 0.00024460 rank 0
I0626 01:12:12.267067 139724881797504 executor.py:115] TRAIN Batch 13/100 loss 88.371506 loss_att 79.290237 loss_ctc 109.561142 lr 0.00024580 rank 0
I0626 01:12:23.306611 139724881797504 executor.py:115] TRAIN Batch 13/200 loss 74.611397 loss_att 65.493729 loss_ctc 95.885956 lr 0.00024700 rank 0
I0626 01:12:34.752355 139724881797504 executor.py:115] TRAIN Batch 13/300 loss 66.313171 loss_att 55.965637 loss_ctc 90.457420 lr 0.00024820 rank 0
I0626 01:12:46.497145 139724881797504 executor.py:115] TRAIN Batch 13/400 loss 60.640022 loss_att 55.116577 loss_ctc 73.528061 lr 0.00024960 rank 0
I0626 01:12:58.113679 139724881797504 executor.py:115] TRAIN Batch 13/500 loss 34.279182 loss_att 29.263702 loss_ctc 45.981976 lr 0.00025080 rank 0
I0626 01:13:08.662771 139724881797504 executor.py:115] TRAIN Batch 13/600 loss 90.235855 loss_att 76.699303 loss_ctc 121.821152 lr 0.00025200 rank 0
I0626 01:13:20.013443 139724881797504 executor.py:115] TRAIN Batch 13/700 loss 74.494026 loss_att 65.270226 loss_ctc 96.016228 lr 0.00025320 rank 0
I0626 01:13:31.386403 139724881797504 executor.py:115] TRAIN Batch 13/800 loss 66.254242 loss_att 58.272511 loss_ctc 84.878296 lr 0.00025460 rank 0
I0626 01:13:43.176940 139724881797504 executor.py:115] TRAIN Batch 13/900 loss 54.521942 loss_att 48.824734 loss_ctc 67.815430 lr 0.00025580 rank 0
I0626 01:13:54.751809 139724881797504 executor.py:115] TRAIN Batch 13/1000 loss 35.361824 loss_att 30.483938 loss_ctc 46.743557 lr 0.00025700 rank 0
I0626 01:14:05.673089 139724881797504 executor.py:115] TRAIN Batch 13/1100 loss 97.935898 loss_att 86.196434 loss_ctc 125.327965 lr 0.00025820 rank 0
I0626 01:14:17.260310 139724881797504 executor.py:115] TRAIN Batch 13/1200 loss 76.755180 loss_att 65.893883 loss_ctc 102.098206 lr 0.00025960 rank 0
I0626 01:14:28.363296 139724881797504 executor.py:115] TRAIN Batch 13/1300 loss 65.834152 loss_att 56.533005 loss_ctc 87.536842 lr 0.00026080 rank 0
I0626 01:14:39.322131 139724881797504 executor.py:115] TRAIN Batch 13/1400 loss 62.565086 loss_att 55.349934 loss_ctc 79.400436 lr 0.00026200 rank 0
I0626 01:14:50.372127 139724881797504 executor.py:115] TRAIN Batch 13/1500 loss 73.231903 loss_att 60.990242 loss_ctc 101.795776 lr 0.00026320 rank 0
I0626 01:14:55.945272 139724881797504 executor.py:152] CV Batch 13/0 loss 36.350220 loss_att 33.247147 loss_ctc 43.590721 history loss 32.311306 rank 0
I0626 01:14:59.869900 139724881797504 executor.py:152] CV Batch 13/100 loss 95.454315 loss_att 84.831459 loss_ctc 120.240982 history loss 69.807148 rank 0
I0626 01:15:02.865982 139724881797504 train.py:288] Epoch 13 CV info cv_loss 78.3809268203384
I0626 01:15:02.866237 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/13.pt
I0626 01:15:03.127055 139724881797504 train.py:274] Epoch 14 TRAIN info lr 0.0002632
I0626 01:15:03.130134 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:15:03.130195 139724881797504 executor.py:46] total epoch is 14.
I0626 01:15:10.862198 139724881797504 executor.py:115] TRAIN Batch 14/0 loss 36.097446 loss_att 32.247173 loss_ctc 45.081421 lr 0.00026340 rank 0
I0626 01:15:21.863257 139724881797504 executor.py:115] TRAIN Batch 14/100 loss 88.595680 loss_att 75.712257 loss_ctc 118.656990 lr 0.00026460 rank 0
I0626 01:15:33.008677 139724881797504 executor.py:115] TRAIN Batch 14/200 loss 76.560570 loss_att 64.957840 loss_ctc 103.633606 lr 0.00026580 rank 0
I0626 01:15:44.534858 139724881797504 executor.py:115] TRAIN Batch 14/300 loss 63.222008 loss_att 52.764881 loss_ctc 87.621956 lr 0.00026700 rank 0
I0626 01:15:56.201296 139724881797504 executor.py:115] TRAIN Batch 14/400 loss 57.151741 loss_att 49.698975 loss_ctc 74.541534 lr 0.00026840 rank 0
I0626 01:16:07.674424 139724881797504 executor.py:115] TRAIN Batch 14/500 loss 33.026047 loss_att 30.588245 loss_ctc 38.714252 lr 0.00026960 rank 0
I0626 01:16:18.102170 139724881797504 executor.py:115] TRAIN Batch 14/600 loss 93.126854 loss_att 77.912720 loss_ctc 128.626495 lr 0.00027080 rank 0
I0626 01:16:29.358436 139724881797504 executor.py:115] TRAIN Batch 14/700 loss 76.530319 loss_att 65.944473 loss_ctc 101.230621 lr 0.00027200 rank 0
I0626 01:16:40.900515 139724881797504 executor.py:115] TRAIN Batch 14/800 loss 73.068932 loss_att 62.328480 loss_ctc 98.129990 lr 0.00027340 rank 0
I0626 01:16:52.405896 139724881797504 executor.py:115] TRAIN Batch 14/900 loss 57.174355 loss_att 48.874695 loss_ctc 76.540237 lr 0.00027460 rank 0
I0626 01:17:03.597132 139724881797504 executor.py:115] TRAIN Batch 14/1000 loss 32.520309 loss_att 29.116753 loss_ctc 40.461945 lr 0.00027580 rank 0
I0626 01:17:14.362886 139724881797504 executor.py:115] TRAIN Batch 14/1100 loss 99.834976 loss_att 87.421082 loss_ctc 128.800720 lr 0.00027700 rank 0
I0626 01:17:25.738123 139724881797504 executor.py:115] TRAIN Batch 14/1200 loss 76.139038 loss_att 63.843521 loss_ctc 104.828568 lr 0.00027840 rank 0
I0626 01:17:37.323724 139724881797504 executor.py:115] TRAIN Batch 14/1300 loss 65.469055 loss_att 58.219734 loss_ctc 82.384140 lr 0.00027960 rank 0
I0626 01:17:48.498811 139724881797504 executor.py:115] TRAIN Batch 14/1400 loss 55.011673 loss_att 47.513329 loss_ctc 72.507812 lr 0.00028080 rank 0
I0626 01:17:59.767553 139724881797504 executor.py:115] TRAIN Batch 14/1500 loss 74.915688 loss_att 66.218521 loss_ctc 95.209076 lr 0.00028200 rank 0
I0626 01:18:05.365402 139724881797504 executor.py:152] CV Batch 14/0 loss 34.485203 loss_att 30.495068 loss_ctc 43.795525 history loss 30.653514 rank 0
I0626 01:18:09.429229 139724881797504 executor.py:152] CV Batch 14/100 loss 94.390335 loss_att 83.599770 loss_ctc 119.568336 history loss 69.217142 rank 0
I0626 01:18:12.430600 139724881797504 train.py:288] Epoch 14 CV info cv_loss 78.2951632258115
I0626 01:18:12.430858 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/14.pt
I0626 01:18:12.689946 139724881797504 train.py:274] Epoch 15 TRAIN info lr 0.00028199999999999997
I0626 01:18:12.693037 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:18:12.693116 139724881797504 executor.py:46] total epoch is 15.
I0626 01:18:20.407872 139724881797504 executor.py:115] TRAIN Batch 15/0 loss 29.763224 loss_att 26.421200 loss_ctc 37.561283 lr 0.00028220 rank 0
I0626 01:18:31.142999 139724881797504 executor.py:115] TRAIN Batch 15/100 loss 99.664955 loss_att 86.635223 loss_ctc 130.067657 lr 0.00028340 rank 0
I0626 01:18:42.270868 139724881797504 executor.py:115] TRAIN Batch 15/200 loss 64.790100 loss_att 53.821930 loss_ctc 90.382507 lr 0.00028460 rank 0
I0626 01:18:53.917880 139724881797504 executor.py:115] TRAIN Batch 15/300 loss 60.969902 loss_att 54.240982 loss_ctc 76.670723 lr 0.00028580 rank 0
I0626 01:19:05.669759 139724881797504 executor.py:115] TRAIN Batch 15/400 loss 51.271450 loss_att 43.203823 loss_ctc 70.095909 lr 0.00028720 rank 0
I0626 01:19:17.112805 139724881797504 executor.py:115] TRAIN Batch 15/500 loss 37.466125 loss_att 33.654121 loss_ctc 46.360809 lr 0.00028840 rank 0
I0626 01:19:27.759401 139724881797504 executor.py:115] TRAIN Batch 15/600 loss 89.444550 loss_att 75.781631 loss_ctc 121.324684 lr 0.00028960 rank 0
I0626 01:19:39.211174 139724881797504 executor.py:115] TRAIN Batch 15/700 loss 70.106796 loss_att 58.214622 loss_ctc 97.855202 lr 0.00029080 rank 0
I0626 01:19:50.569708 139724881797504 executor.py:115] TRAIN Batch 15/800 loss 70.545357 loss_att 61.894077 loss_ctc 90.731659 lr 0.00029220 rank 0
I0626 01:20:02.219036 139724881797504 executor.py:115] TRAIN Batch 15/900 loss 50.227142 loss_att 43.026993 loss_ctc 67.027496 lr 0.00029340 rank 0
I0626 01:20:13.728692 139724881797504 executor.py:115] TRAIN Batch 15/1000 loss 29.999004 loss_att 26.749584 loss_ctc 37.580982 lr 0.00029460 rank 0
I0626 01:20:25.180672 139724881797504 executor.py:115] TRAIN Batch 15/1100 loss 100.332108 loss_att 87.899475 loss_ctc 129.341568 lr 0.00029580 rank 0
I0626 01:20:36.680723 139724881797504 executor.py:115] TRAIN Batch 15/1200 loss 74.699020 loss_att 63.573322 loss_ctc 100.658997 lr 0.00029720 rank 0
I0626 01:20:48.214519 139724881797504 executor.py:115] TRAIN Batch 15/1300 loss 68.931999 loss_att 59.564728 loss_ctc 90.788956 lr 0.00029840 rank 0
I0626 01:20:59.639615 139724881797504 executor.py:115] TRAIN Batch 15/1400 loss 55.880627 loss_att 49.729179 loss_ctc 70.234001 lr 0.00029960 rank 0
I0626 01:21:10.903001 139724881797504 executor.py:115] TRAIN Batch 15/1500 loss 56.679596 loss_att 48.781586 loss_ctc 75.108276 lr 0.00030080 rank 0
I0626 01:21:16.470463 139724881797504 executor.py:152] CV Batch 15/0 loss 36.452564 loss_att 32.722404 loss_ctc 45.156265 history loss 32.402279 rank 0
I0626 01:21:20.466570 139724881797504 executor.py:152] CV Batch 15/100 loss 93.851669 loss_att 82.910950 loss_ctc 119.380035 history loss 69.071748 rank 0
I0626 01:21:23.448082 139724881797504 train.py:288] Epoch 15 CV info cv_loss 77.72651596761465
I0626 01:21:23.448328 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/15.pt
I0626 01:21:23.713096 139724881797504 train.py:274] Epoch 16 TRAIN info lr 0.0003008
I0626 01:21:23.716174 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:21:23.716244 139724881797504 executor.py:46] total epoch is 16.
I0626 01:21:31.620363 139724881797504 executor.py:115] TRAIN Batch 16/0 loss 30.320827 loss_att 25.996918 loss_ctc 40.409946 lr 0.00030100 rank 0
I0626 01:21:42.557741 139724881797504 executor.py:115] TRAIN Batch 16/100 loss 86.295486 loss_att 69.278641 loss_ctc 126.001472 lr 0.00030220 rank 0
I0626 01:21:53.656681 139724881797504 executor.py:115] TRAIN Batch 16/200 loss 80.015915 loss_att 69.324394 loss_ctc 104.962784 lr 0.00030340 rank 0
I0626 01:22:04.825119 139724881797504 executor.py:115] TRAIN Batch 16/300 loss 59.964706 loss_att 50.381218 loss_ctc 82.326164 lr 0.00030460 rank 0
I0626 01:22:16.504234 139724881797504 executor.py:115] TRAIN Batch 16/400 loss 51.434437 loss_att 43.645985 loss_ctc 69.607491 lr 0.00030600 rank 0
I0626 01:22:28.009838 139724881797504 executor.py:115] TRAIN Batch 16/500 loss 34.014153 loss_att 30.267818 loss_ctc 42.755592 lr 0.00030720 rank 0
I0626 01:22:38.773367 139724881797504 executor.py:115] TRAIN Batch 16/600 loss 83.332336 loss_att 70.770798 loss_ctc 112.642578 lr 0.00030840 rank 0
I0626 01:22:50.005627 139724881797504 executor.py:115] TRAIN Batch 16/700 loss 69.044922 loss_att 54.949043 loss_ctc 101.935303 lr 0.00030960 rank 0
I0626 01:23:01.508207 139724881797504 executor.py:115] TRAIN Batch 16/800 loss 64.078438 loss_att 55.613464 loss_ctc 83.830032 lr 0.00031100 rank 0
I0626 01:23:12.912305 139724881797504 executor.py:115] TRAIN Batch 16/900 loss 54.407253 loss_att 46.729507 loss_ctc 72.321991 lr 0.00031220 rank 0
I0626 01:23:24.247863 139724881797504 executor.py:115] TRAIN Batch 16/1000 loss 28.838287 loss_att 26.504032 loss_ctc 34.284882 lr 0.00031340 rank 0
I0626 01:23:35.148910 139724881797504 executor.py:115] TRAIN Batch 16/1100 loss 91.705635 loss_att 77.746109 loss_ctc 124.277863 lr 0.00031460 rank 0
I0626 01:23:46.924833 139724881797504 executor.py:115] TRAIN Batch 16/1200 loss 76.950653 loss_att 63.320610 loss_ctc 108.754089 lr 0.00031600 rank 0
I0626 01:23:58.161389 139724881797504 executor.py:115] TRAIN Batch 16/1300 loss 62.263763 loss_att 53.387218 loss_ctc 82.975708 lr 0.00031720 rank 0
I0626 01:24:09.632778 139724881797504 executor.py:115] TRAIN Batch 16/1400 loss 53.310734 loss_att 45.849957 loss_ctc 70.719208 lr 0.00031840 rank 0
I0626 01:24:20.601084 139724881797504 executor.py:115] TRAIN Batch 16/1500 loss 68.879929 loss_att 57.648643 loss_ctc 95.086266 lr 0.00031960 rank 0
I0626 01:24:26.210081 139724881797504 executor.py:152] CV Batch 16/0 loss 36.619308 loss_att 34.131466 loss_ctc 42.424271 history loss 32.550496 rank 0
I0626 01:24:30.257362 139724881797504 executor.py:152] CV Batch 16/100 loss 93.967407 loss_att 82.920311 loss_ctc 119.743973 history loss 69.186851 rank 0
I0626 01:24:33.238005 139724881797504 train.py:288] Epoch 16 CV info cv_loss 77.65028365375441
I0626 01:24:33.238280 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/16.pt
I0626 01:24:33.504292 139724881797504 train.py:274] Epoch 17 TRAIN info lr 0.00031959999999999996
I0626 01:24:33.507249 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:24:33.507309 139724881797504 executor.py:46] total epoch is 17.
I0626 01:24:41.212542 139724881797504 executor.py:115] TRAIN Batch 17/0 loss 29.639181 loss_att 26.549267 loss_ctc 36.848984 lr 0.00031980 rank 0
I0626 01:24:52.133393 139724881797504 executor.py:115] TRAIN Batch 17/100 loss 82.855316 loss_att 66.121643 loss_ctc 121.900543 lr 0.00032100 rank 0
I0626 01:25:03.376628 139724881797504 executor.py:115] TRAIN Batch 17/200 loss 73.978851 loss_att 62.508343 loss_ctc 100.743378 lr 0.00032220 rank 0
I0626 01:25:14.656080 139724881797504 executor.py:115] TRAIN Batch 17/300 loss 65.921638 loss_att 56.701607 loss_ctc 87.435043 lr 0.00032340 rank 0
I0626 01:25:26.572691 139724881797504 executor.py:115] TRAIN Batch 17/400 loss 48.154594 loss_att 40.105690 loss_ctc 66.935364 lr 0.00032480 rank 0
I0626 01:25:37.974440 139724881797504 executor.py:115] TRAIN Batch 17/500 loss 28.178694 loss_att 24.725813 loss_ctc 36.235413 lr 0.00032600 rank 0
I0626 01:25:48.661906 139724881797504 executor.py:115] TRAIN Batch 17/600 loss 93.322258 loss_att 78.704803 loss_ctc 127.429657 lr 0.00032720 rank 0
I0626 01:26:00.287609 139724881797504 executor.py:115] TRAIN Batch 17/700 loss 81.800735 loss_att 68.890167 loss_ctc 111.925377 lr 0.00032840 rank 0
I0626 01:26:12.127416 139724881797504 executor.py:115] TRAIN Batch 17/800 loss 67.030128 loss_att 56.093475 loss_ctc 92.548981 lr 0.00032980 rank 0
I0626 01:26:23.882878 139724881797504 executor.py:115] TRAIN Batch 17/900 loss 54.551285 loss_att 46.959457 loss_ctc 72.265541 lr 0.00033100 rank 0
I0626 01:26:35.341315 139724881797504 executor.py:115] TRAIN Batch 17/1000 loss 28.692675 loss_att 23.838188 loss_ctc 40.019810 lr 0.00033220 rank 0
I0626 01:26:45.851706 139724881797504 executor.py:115] TRAIN Batch 17/1100 loss 87.795441 loss_att 72.362633 loss_ctc 123.805313 lr 0.00033340 rank 0
I0626 01:26:57.182267 139724881797504 executor.py:115] TRAIN Batch 17/1200 loss 66.784515 loss_att 57.000668 loss_ctc 89.613480 lr 0.00033480 rank 0
I0626 01:27:08.622694 139724881797504 executor.py:115] TRAIN Batch 17/1300 loss 60.427528 loss_att 51.539322 loss_ctc 81.166664 lr 0.00033600 rank 0
I0626 01:27:20.029709 139724881797504 executor.py:115] TRAIN Batch 17/1400 loss 48.535282 loss_att 39.542358 loss_ctc 69.518768 lr 0.00033720 rank 0
I0626 01:27:31.090324 139724881797504 executor.py:115] TRAIN Batch 17/1500 loss 58.483963 loss_att 48.453960 loss_ctc 81.887314 lr 0.00033840 rank 0
I0626 01:27:36.667122 139724881797504 executor.py:152] CV Batch 17/0 loss 35.800037 loss_att 32.656136 loss_ctc 43.135803 history loss 31.822255 rank 0
I0626 01:27:40.659823 139724881797504 executor.py:152] CV Batch 17/100 loss 92.972595 loss_att 81.945465 loss_ctc 118.702568 history loss 68.526981 rank 0
I0626 01:27:43.647532 139724881797504 train.py:288] Epoch 17 CV info cv_loss 77.15451969804252
I0626 01:27:43.647707 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/17.pt
I0626 01:27:43.905757 139724881797504 train.py:274] Epoch 18 TRAIN info lr 0.00033839999999999993
I0626 01:27:43.908868 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:27:43.908940 139724881797504 executor.py:46] total epoch is 18.
I0626 01:27:51.731549 139724881797504 executor.py:115] TRAIN Batch 18/0 loss 33.130852 loss_att 28.487837 loss_ctc 43.964554 lr 0.00033860 rank 0
I0626 01:28:03.190419 139724881797504 executor.py:115] TRAIN Batch 18/100 loss 89.492989 loss_att 74.636627 loss_ctc 124.157829 lr 0.00033980 rank 0
I0626 01:28:14.269444 139724881797504 executor.py:115] TRAIN Batch 18/200 loss 65.055069 loss_att 52.817642 loss_ctc 93.609070 lr 0.00034100 rank 0
I0626 01:28:25.564019 139724881797504 executor.py:115] TRAIN Batch 18/300 loss 58.891907 loss_att 47.439163 loss_ctc 85.614975 lr 0.00034220 rank 0
I0626 01:28:37.293191 139724881797504 executor.py:115] TRAIN Batch 18/400 loss 51.076843 loss_att 44.141350 loss_ctc 67.259651 lr 0.00034360 rank 0
I0626 01:28:48.872609 139724881797504 executor.py:115] TRAIN Batch 18/500 loss 31.683901 loss_att 28.455345 loss_ctc 39.217197 lr 0.00034480 rank 0
I0626 01:28:59.533668 139724881797504 executor.py:115] TRAIN Batch 18/600 loss 76.573013 loss_att 63.473183 loss_ctc 107.139275 lr 0.00034600 rank 0
I0626 01:29:11.349593 139724881797504 executor.py:115] TRAIN Batch 18/700 loss 60.940010 loss_att 48.994335 loss_ctc 88.813240 lr 0.00034720 rank 0
I0626 01:29:23.093092 139724881797504 executor.py:115] TRAIN Batch 18/800 loss 64.116783 loss_att 54.015862 loss_ctc 87.685593 lr 0.00034860 rank 0
I0626 01:29:34.812630 139724881797504 executor.py:115] TRAIN Batch 18/900 loss 54.438515 loss_att 47.347198 loss_ctc 70.984909 lr 0.00034980 rank 0
I0626 01:29:46.428050 139724881797504 executor.py:115] TRAIN Batch 18/1000 loss 36.225021 loss_att 30.137234 loss_ctc 50.429855 lr 0.00035100 rank 0
I0626 01:29:57.368593 139724881797504 executor.py:115] TRAIN Batch 18/1100 loss 95.252914 loss_att 82.033363 loss_ctc 126.098526 lr 0.00035220 rank 0
I0626 01:30:08.645829 139724881797504 executor.py:115] TRAIN Batch 18/1200 loss 71.873291 loss_att 61.283470 loss_ctc 96.582870 lr 0.00035360 rank 0
I0626 01:30:20.005827 139724881797504 executor.py:115] TRAIN Batch 18/1300 loss 65.467758 loss_att 55.530472 loss_ctc 88.654755 lr 0.00035480 rank 0
I0626 01:30:31.375635 139724881797504 executor.py:115] TRAIN Batch 18/1400 loss 50.339581 loss_att 41.960403 loss_ctc 69.890991 lr 0.00035600 rank 0
I0626 01:30:42.288252 139724881797504 executor.py:115] TRAIN Batch 18/1500 loss 61.587173 loss_att 50.819836 loss_ctc 86.710968 lr 0.00035720 rank 0
I0626 01:30:47.857311 139724881797504 executor.py:152] CV Batch 18/0 loss 34.986164 loss_att 31.665310 loss_ctc 42.734829 history loss 31.098813 rank 0
I0626 01:30:51.787178 139724881797504 executor.py:152] CV Batch 18/100 loss 92.639359 loss_att 81.512283 loss_ctc 118.602539 history loss 68.039613 rank 0
I0626 01:30:54.780197 139724881797504 train.py:288] Epoch 18 CV info cv_loss 76.86800736442613
I0626 01:30:54.780378 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/18.pt
I0626 01:30:55.039356 139724881797504 train.py:274] Epoch 19 TRAIN info lr 0.00035719999999999995
I0626 01:30:55.042372 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:30:55.042433 139724881797504 executor.py:46] total epoch is 19.
I0626 01:31:02.836540 139724881797504 executor.py:115] TRAIN Batch 19/0 loss 27.774120 loss_att 24.265495 loss_ctc 35.960915 lr 0.00035740 rank 0
I0626 01:31:13.951700 139724881797504 executor.py:115] TRAIN Batch 19/100 loss 91.505280 loss_att 76.373268 loss_ctc 126.813286 lr 0.00035860 rank 0
I0626 01:31:25.200583 139724881797504 executor.py:115] TRAIN Batch 19/200 loss 69.503700 loss_att 58.841904 loss_ctc 94.381226 lr 0.00035980 rank 0
I0626 01:31:36.795195 139724881797504 executor.py:115] TRAIN Batch 19/300 loss 57.275791 loss_att 47.584019 loss_ctc 79.889931 lr 0.00036100 rank 0
I0626 01:31:48.292388 139724881797504 executor.py:115] TRAIN Batch 19/400 loss 54.981094 loss_att 47.620110 loss_ctc 72.156731 lr 0.00036240 rank 0
I0626 01:31:59.812383 139724881797504 executor.py:115] TRAIN Batch 19/500 loss 27.956863 loss_att 22.065210 loss_ctc 41.704048 lr 0.00036360 rank 0
I0626 01:32:10.173277 139724881797504 executor.py:115] TRAIN Batch 19/600 loss 73.630089 loss_att 60.749359 loss_ctc 103.685112 lr 0.00036480 rank 0
I0626 01:32:21.443320 139724881797504 executor.py:115] TRAIN Batch 19/700 loss 65.818878 loss_att 53.218002 loss_ctc 95.220932 lr 0.00036600 rank 0
I0626 01:32:32.770720 139724881797504 executor.py:115] TRAIN Batch 19/800 loss 63.229263 loss_att 52.508362 loss_ctc 88.244705 lr 0.00036740 rank 0
I0626 01:32:44.338287 139724881797504 executor.py:115] TRAIN Batch 19/900 loss 47.874962 loss_att 39.934391 loss_ctc 66.402962 lr 0.00036860 rank 0
I0626 01:32:55.686514 139724881797504 executor.py:115] TRAIN Batch 19/1000 loss 31.456993 loss_att 27.359499 loss_ctc 41.017811 lr 0.00036980 rank 0
I0626 01:33:06.628398 139724881797504 executor.py:115] TRAIN Batch 19/1100 loss 87.679245 loss_att 72.413208 loss_ctc 123.300003 lr 0.00037100 rank 0
I0626 01:33:18.013385 139724881797504 executor.py:115] TRAIN Batch 19/1200 loss 68.055862 loss_att 54.382622 loss_ctc 99.960106 lr 0.00037240 rank 0
I0626 01:33:29.551256 139724881797504 executor.py:115] TRAIN Batch 19/1300 loss 51.212570 loss_att 40.059349 loss_ctc 77.236755 lr 0.00037360 rank 0
I0626 01:33:40.796631 139724881797504 executor.py:115] TRAIN Batch 19/1400 loss 48.836052 loss_att 39.108025 loss_ctc 71.534790 lr 0.00037480 rank 0
I0626 01:33:51.691149 139724881797504 executor.py:115] TRAIN Batch 19/1500 loss 71.758911 loss_att 58.784355 loss_ctc 102.032860 lr 0.00037600 rank 0
I0626 01:33:57.242517 139724881797504 executor.py:152] CV Batch 19/0 loss 36.501663 loss_att 34.048988 loss_ctc 42.224564 history loss 32.445923 rank 0
I0626 01:34:01.299710 139724881797504 executor.py:152] CV Batch 19/100 loss 93.011337 loss_att 82.042915 loss_ctc 118.604324 history loss 68.523856 rank 0
I0626 01:34:04.287412 139724881797504 train.py:288] Epoch 19 CV info cv_loss 76.9928728570825
I0626 01:34:04.287655 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/19.pt
I0626 01:34:04.543844 139724881797504 train.py:274] Epoch 20 TRAIN info lr 0.000376
I0626 01:34:04.549182 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:34:04.549834 139724881797504 executor.py:46] total epoch is 20.
I0626 01:34:12.281654 139724881797504 executor.py:115] TRAIN Batch 20/0 loss 27.232573 loss_att 23.110764 loss_ctc 36.850128 lr 0.00037620 rank 0
I0626 01:34:23.426150 139724881797504 executor.py:115] TRAIN Batch 20/100 loss 81.882851 loss_att 70.347412 loss_ctc 108.798874 lr 0.00037740 rank 0
I0626 01:34:34.518101 139724881797504 executor.py:115] TRAIN Batch 20/200 loss 76.598640 loss_att 64.387016 loss_ctc 105.092422 lr 0.00037860 rank 0
I0626 01:34:45.943077 139724881797504 executor.py:115] TRAIN Batch 20/300 loss 54.097317 loss_att 43.183910 loss_ctc 79.561928 lr 0.00037980 rank 0
I0626 01:34:57.529193 139724881797504 executor.py:115] TRAIN Batch 20/400 loss 47.266136 loss_att 36.731033 loss_ctc 71.848038 lr 0.00038120 rank 0
I0626 01:35:09.160961 139724881797504 executor.py:115] TRAIN Batch 20/500 loss 32.597836 loss_att 28.633251 loss_ctc 41.848534 lr 0.00038240 rank 0
I0626 01:35:19.484881 139724881797504 executor.py:115] TRAIN Batch 20/600 loss 77.312973 loss_att 61.378899 loss_ctc 114.492477 lr 0.00038360 rank 0
I0626 01:35:30.587219 139724881797504 executor.py:115] TRAIN Batch 20/700 loss 66.059868 loss_att 52.817898 loss_ctc 96.957809 lr 0.00038480 rank 0
I0626 01:35:42.108389 139724881797504 executor.py:115] TRAIN Batch 20/800 loss 57.679218 loss_att 47.441185 loss_ctc 81.567963 lr 0.00038620 rank 0
I0626 01:35:53.575510 139724881797504 executor.py:115] TRAIN Batch 20/900 loss 50.798950 loss_att 42.843941 loss_ctc 69.360634 lr 0.00038740 rank 0
I0626 01:36:04.599972 139724881797504 executor.py:115] TRAIN Batch 20/1000 loss 32.096424 loss_att 29.290863 loss_ctc 38.642738 lr 0.00038860 rank 0
I0626 01:36:15.236099 139724881797504 executor.py:115] TRAIN Batch 20/1100 loss 95.608398 loss_att 79.318817 loss_ctc 133.617432 lr 0.00038980 rank 0
I0626 01:36:26.686194 139724881797504 executor.py:115] TRAIN Batch 20/1200 loss 65.113808 loss_att 53.939934 loss_ctc 91.186180 lr 0.00039120 rank 0
I0626 01:36:38.470561 139724881797504 executor.py:115] TRAIN Batch 20/1300 loss 62.802055 loss_att 52.426018 loss_ctc 87.012817 lr 0.00039240 rank 0
I0626 01:36:49.792706 139724881797504 executor.py:115] TRAIN Batch 20/1400 loss 52.348942 loss_att 43.342239 loss_ctc 73.364578 lr 0.00039360 rank 0
I0626 01:37:00.962809 139724881797504 executor.py:115] TRAIN Batch 20/1500 loss 69.243553 loss_att 56.100533 loss_ctc 99.910606 lr 0.00039480 rank 0
I0626 01:37:06.554160 139724881797504 executor.py:152] CV Batch 20/0 loss 35.756554 loss_att 33.145008 loss_ctc 41.850151 history loss 31.783603 rank 0
I0626 01:37:10.708111 139724881797504 executor.py:152] CV Batch 20/100 loss 91.868896 loss_att 80.669884 loss_ctc 117.999939 history loss 67.989451 rank 0
I0626 01:37:13.703125 139724881797504 train.py:288] Epoch 20 CV info cv_loss 76.70357280765808
I0626 01:37:13.703485 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/20.pt
I0626 01:37:13.966351 139724881797504 train.py:274] Epoch 21 TRAIN info lr 0.0003948
I0626 01:37:13.969496 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:37:13.969570 139724881797504 executor.py:46] total epoch is 21.
I0626 01:37:21.837628 139724881797504 executor.py:115] TRAIN Batch 21/0 loss 31.117826 loss_att 26.696068 loss_ctc 41.435265 lr 0.00039500 rank 0
I0626 01:37:32.646980 139724881797504 executor.py:115] TRAIN Batch 21/100 loss 91.561478 loss_att 75.279129 loss_ctc 129.553619 lr 0.00039620 rank 0
I0626 01:37:43.800942 139724881797504 executor.py:115] TRAIN Batch 21/200 loss 71.176193 loss_att 57.476707 loss_ctc 103.141647 lr 0.00039740 rank 0
I0626 01:37:55.117257 139724881797504 executor.py:115] TRAIN Batch 21/300 loss 54.147373 loss_att 44.366699 loss_ctc 76.968948 lr 0.00039860 rank 0
I0626 01:38:06.688318 139724881797504 executor.py:115] TRAIN Batch 21/400 loss 50.490841 loss_att 42.225338 loss_ctc 69.777016 lr 0.00040000 rank 0
I0626 01:38:18.628772 139724881797504 executor.py:115] TRAIN Batch 21/500 loss 27.873026 loss_att 22.851667 loss_ctc 39.589527 lr 0.00040120 rank 0
I0626 01:38:29.307348 139724881797504 executor.py:115] TRAIN Batch 21/600 loss 71.753464 loss_att 57.809517 loss_ctc 104.289352 lr 0.00040240 rank 0
I0626 01:38:40.594301 139724881797504 executor.py:115] TRAIN Batch 21/700 loss 69.694717 loss_att 56.395382 loss_ctc 100.726486 lr 0.00040360 rank 0
I0626 01:38:51.946302 139724881797504 executor.py:115] TRAIN Batch 21/800 loss 58.483147 loss_att 46.393059 loss_ctc 86.693359 lr 0.00040500 rank 0
I0626 01:39:03.509943 139724881797504 executor.py:115] TRAIN Batch 21/900 loss 51.086891 loss_att 42.779591 loss_ctc 70.470596 lr 0.00040620 rank 0
I0626 01:39:14.735970 139724881797504 executor.py:115] TRAIN Batch 21/1000 loss 30.640133 loss_att 25.784771 loss_ctc 41.969307 lr 0.00040740 rank 0
I0626 01:39:25.545210 139724881797504 executor.py:115] TRAIN Batch 21/1100 loss 80.163498 loss_att 65.229767 loss_ctc 115.008865 lr 0.00040860 rank 0
I0626 01:39:36.779212 139724881797504 executor.py:115] TRAIN Batch 21/1200 loss 76.240990 loss_att 61.244926 loss_ctc 111.231796 lr 0.00041000 rank 0
I0626 01:39:48.186599 139724881797504 executor.py:115] TRAIN Batch 21/1300 loss 61.863251 loss_att 51.962646 loss_ctc 84.964661 lr 0.00041120 rank 0
I0626 01:39:59.533233 139724881797504 executor.py:115] TRAIN Batch 21/1400 loss 43.302582 loss_att 32.707588 loss_ctc 68.024231 lr 0.00041240 rank 0
I0626 01:40:10.464972 139724881797504 executor.py:115] TRAIN Batch 21/1500 loss 63.992638 loss_att 55.418266 loss_ctc 83.999512 lr 0.00041360 rank 0
I0626 01:40:16.036573 139724881797504 executor.py:152] CV Batch 21/0 loss 35.581459 loss_att 32.586967 loss_ctc 42.568611 history loss 31.627964 rank 0
I0626 01:40:20.044259 139724881797504 executor.py:152] CV Batch 21/100 loss 91.152061 loss_att 80.125160 loss_ctc 116.881493 history loss 67.767522 rank 0
I0626 01:40:23.045123 139724881797504 train.py:288] Epoch 21 CV info cv_loss 76.39438843710474
I0626 01:40:23.045360 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/21.pt
I0626 01:40:23.311034 139724881797504 train.py:274] Epoch 22 TRAIN info lr 0.00041359999999999997
I0626 01:40:23.314150 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:40:23.314213 139724881797504 executor.py:46] total epoch is 22.
I0626 01:40:31.219748 139724881797504 executor.py:115] TRAIN Batch 22/0 loss 29.679688 loss_att 24.921478 loss_ctc 40.782173 lr 0.00041380 rank 0
I0626 01:40:42.148796 139724881797504 executor.py:115] TRAIN Batch 22/100 loss 83.563385 loss_att 68.976234 loss_ctc 117.600067 lr 0.00041500 rank 0
I0626 01:40:53.027043 139724881797504 executor.py:115] TRAIN Batch 22/200 loss 63.814667 loss_att 50.244568 loss_ctc 95.478241 lr 0.00041620 rank 0
I0626 01:41:03.913062 139724881797504 executor.py:115] TRAIN Batch 22/300 loss 53.599052 loss_att 44.973492 loss_ctc 73.725357 lr 0.00041740 rank 0
I0626 01:41:15.199657 139724881797504 executor.py:115] TRAIN Batch 22/400 loss 51.111946 loss_att 41.921646 loss_ctc 72.555969 lr 0.00041880 rank 0
I0626 01:41:26.523081 139724881797504 executor.py:115] TRAIN Batch 22/500 loss 30.904675 loss_att 23.745152 loss_ctc 47.610226 lr 0.00042000 rank 0
I0626 01:41:37.305852 139724881797504 executor.py:115] TRAIN Batch 22/600 loss 81.507896 loss_att 66.008911 loss_ctc 117.672188 lr 0.00042120 rank 0
I0626 01:41:48.386751 139724881797504 executor.py:115] TRAIN Batch 22/700 loss 66.632187 loss_att 54.792168 loss_ctc 94.258896 lr 0.00042240 rank 0
I0626 01:41:59.969354 139724881797504 executor.py:115] TRAIN Batch 22/800 loss 59.898209 loss_att 49.903351 loss_ctc 83.219528 lr 0.00042380 rank 0
I0626 01:42:11.680468 139724881797504 executor.py:115] TRAIN Batch 22/900 loss 48.533863 loss_att 40.781605 loss_ctc 66.622467 lr 0.00042500 rank 0
I0626 01:42:23.059830 139724881797504 executor.py:115] TRAIN Batch 22/1000 loss 30.322918 loss_att 26.677082 loss_ctc 38.829865 lr 0.00042620 rank 0
I0626 01:42:33.894794 139724881797504 executor.py:115] TRAIN Batch 22/1100 loss 90.677536 loss_att 74.701538 loss_ctc 127.954849 lr 0.00042740 rank 0
I0626 01:42:45.051293 139724881797504 executor.py:115] TRAIN Batch 22/1200 loss 71.649490 loss_att 56.280266 loss_ctc 107.511024 lr 0.00042880 rank 0
I0626 01:42:56.427767 139724881797504 executor.py:115] TRAIN Batch 22/1300 loss 54.511139 loss_att 43.875511 loss_ctc 79.327606 lr 0.00043000 rank 0
I0626 01:43:07.650359 139724881797504 executor.py:115] TRAIN Batch 22/1400 loss 48.993164 loss_att 40.292519 loss_ctc 69.294662 lr 0.00043120 rank 0
I0626 01:43:18.708234 139724881797504 executor.py:115] TRAIN Batch 22/1500 loss 83.296036 loss_att 66.952240 loss_ctc 121.431580 lr 0.00043240 rank 0
I0626 01:43:24.295309 139724881797504 executor.py:152] CV Batch 22/0 loss 35.914330 loss_att 33.145779 loss_ctc 42.374283 history loss 31.923848 rank 0
I0626 01:43:28.258402 139724881797504 executor.py:152] CV Batch 22/100 loss 91.486481 loss_att 80.706802 loss_ctc 116.639053 history loss 67.765710 rank 0
I0626 01:43:31.234486 139724881797504 train.py:288] Epoch 22 CV info cv_loss 76.40841385175194
I0626 01:43:31.234733 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/22.pt
I0626 01:43:31.494326 139724881797504 train.py:274] Epoch 23 TRAIN info lr 0.00043239999999999994
I0626 01:43:31.497398 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:43:31.497465 139724881797504 executor.py:46] total epoch is 23.
I0626 01:43:39.290612 139724881797504 executor.py:115] TRAIN Batch 23/0 loss 26.702116 loss_att 21.153574 loss_ctc 39.648712 lr 0.00043260 rank 0
I0626 01:43:50.374782 139724881797504 executor.py:115] TRAIN Batch 23/100 loss 86.661606 loss_att 69.663864 loss_ctc 126.322998 lr 0.00043380 rank 0
I0626 01:44:01.260240 139724881797504 executor.py:115] TRAIN Batch 23/200 loss 71.756760 loss_att 58.801483 loss_ctc 101.985748 lr 0.00043500 rank 0
I0626 01:44:12.358228 139724881797504 executor.py:115] TRAIN Batch 23/300 loss 59.391418 loss_att 47.597115 loss_ctc 86.911469 lr 0.00043620 rank 0
I0626 01:44:23.594286 139724881797504 executor.py:115] TRAIN Batch 23/400 loss 44.553719 loss_att 34.883347 loss_ctc 67.117920 lr 0.00043760 rank 0
I0626 01:44:35.261690 139724881797504 executor.py:115] TRAIN Batch 23/500 loss 26.686119 loss_att 22.455685 loss_ctc 36.557133 lr 0.00043880 rank 0
I0626 01:44:45.689076 139724881797504 executor.py:115] TRAIN Batch 23/600 loss 77.740883 loss_att 60.979843 loss_ctc 116.849976 lr 0.00044000 rank 0
I0626 01:44:56.916154 139724881797504 executor.py:115] TRAIN Batch 23/700 loss 70.193550 loss_att 57.077389 loss_ctc 100.797928 lr 0.00044120 rank 0
I0626 01:45:08.395815 139724881797504 executor.py:115] TRAIN Batch 23/800 loss 53.154320 loss_att 41.604752 loss_ctc 80.103317 lr 0.00044260 rank 0
I0626 01:45:19.661491 139724881797504 executor.py:115] TRAIN Batch 23/900 loss 54.896439 loss_att 44.749397 loss_ctc 78.572861 lr 0.00044380 rank 0
I0626 01:45:31.214610 139724881797504 executor.py:115] TRAIN Batch 23/1000 loss 27.681984 loss_att 24.818668 loss_ctc 34.363052 lr 0.00044500 rank 0
I0626 01:45:42.167347 139724881797504 executor.py:115] TRAIN Batch 23/1100 loss 82.725060 loss_att 67.541443 loss_ctc 118.153488 lr 0.00044620 rank 0
I0626 01:45:53.460178 139724881797504 executor.py:115] TRAIN Batch 23/1200 loss 66.477623 loss_att 52.383461 loss_ctc 99.363991 lr 0.00044760 rank 0
I0626 01:46:04.912348 139724881797504 executor.py:115] TRAIN Batch 23/1300 loss 53.572464 loss_att 42.732868 loss_ctc 78.864853 lr 0.00044880 rank 0
I0626 01:46:16.018000 139724881797504 executor.py:115] TRAIN Batch 23/1400 loss 48.121811 loss_att 37.649712 loss_ctc 72.556717 lr 0.00045000 rank 0
I0626 01:46:27.156799 139724881797504 executor.py:115] TRAIN Batch 23/1500 loss 50.101440 loss_att 37.045330 loss_ctc 80.565704 lr 0.00045120 rank 0
I0626 01:46:32.747254 139724881797504 executor.py:152] CV Batch 23/0 loss 35.888771 loss_att 33.933578 loss_ctc 40.450890 history loss 31.901130 rank 0
I0626 01:46:36.835580 139724881797504 executor.py:152] CV Batch 23/100 loss 91.871223 loss_att 81.201889 loss_ctc 116.766342 history loss 67.621377 rank 0
I0626 01:46:39.819929 139724881797504 train.py:288] Epoch 23 CV info cv_loss 76.16330347906353
I0626 01:46:39.820095 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/23.pt
I0626 01:46:40.082637 139724881797504 train.py:274] Epoch 24 TRAIN info lr 0.00045119999999999996
I0626 01:46:40.085664 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:46:40.085728 139724881797504 executor.py:46] total epoch is 24.
I0626 01:46:47.981040 139724881797504 executor.py:115] TRAIN Batch 24/0 loss 28.750093 loss_att 24.587811 loss_ctc 38.462086 lr 0.00045140 rank 0
I0626 01:46:58.891270 139724881797504 executor.py:115] TRAIN Batch 24/100 loss 80.673843 loss_att 65.562653 loss_ctc 115.933304 lr 0.00045260 rank 0
I0626 01:47:09.882672 139724881797504 executor.py:115] TRAIN Batch 24/200 loss 67.865311 loss_att 54.464493 loss_ctc 99.133873 lr 0.00045380 rank 0
I0626 01:47:21.370752 139724881797504 executor.py:115] TRAIN Batch 24/300 loss 58.180065 loss_att 48.350067 loss_ctc 81.116722 lr 0.00045500 rank 0
I0626 01:47:32.950102 139724881797504 executor.py:115] TRAIN Batch 24/400 loss 48.276070 loss_att 38.496910 loss_ctc 71.094116 lr 0.00045640 rank 0
I0626 01:47:44.597721 139724881797504 executor.py:115] TRAIN Batch 24/500 loss 23.414871 loss_att 20.639172 loss_ctc 29.891499 lr 0.00045760 rank 0
I0626 01:47:55.468278 139724881797504 executor.py:115] TRAIN Batch 24/600 loss 73.570084 loss_att 59.060841 loss_ctc 107.425003 lr 0.00045880 rank 0
I0626 01:48:06.637363 139724881797504 executor.py:115] TRAIN Batch 24/700 loss 70.544403 loss_att 55.221714 loss_ctc 106.297333 lr 0.00046000 rank 0
I0626 01:48:18.342129 139724881797504 executor.py:115] TRAIN Batch 24/800 loss 54.278893 loss_att 41.711971 loss_ctc 83.601700 lr 0.00046140 rank 0
I0626 01:48:29.758639 139724881797504 executor.py:115] TRAIN Batch 24/900 loss 49.458084 loss_att 38.781818 loss_ctc 74.369370 lr 0.00046260 rank 0
I0626 01:48:40.718903 139724881797504 executor.py:115] TRAIN Batch 24/1000 loss 31.917992 loss_att 26.586866 loss_ctc 44.357281 lr 0.00046380 rank 0
I0626 01:48:51.410770 139724881797504 executor.py:115] TRAIN Batch 24/1100 loss 74.240463 loss_att 58.114544 loss_ctc 111.867599 lr 0.00046500 rank 0
I0626 01:49:02.464988 139724881797504 executor.py:115] TRAIN Batch 24/1200 loss 74.353279 loss_att 62.394779 loss_ctc 102.256447 lr 0.00046640 rank 0
I0626 01:49:13.808738 139724881797504 executor.py:115] TRAIN Batch 24/1300 loss 54.793442 loss_att 43.446293 loss_ctc 81.270111 lr 0.00046760 rank 0
I0626 01:49:25.276674 139724881797504 executor.py:115] TRAIN Batch 24/1400 loss 48.272537 loss_att 39.254742 loss_ctc 69.314056 lr 0.00046880 rank 0
I0626 01:49:36.299128 139724881797504 executor.py:115] TRAIN Batch 24/1500 loss 69.344269 loss_att 56.197388 loss_ctc 100.020325 lr 0.00047000 rank 0
I0626 01:49:41.878082 139724881797504 executor.py:152] CV Batch 24/0 loss 33.835541 loss_att 30.889225 loss_ctc 40.710281 history loss 30.076036 rank 0
I0626 01:49:45.819140 139724881797504 executor.py:152] CV Batch 24/100 loss 90.737946 loss_att 79.676727 loss_ctc 116.547470 history loss 66.927423 rank 0
I0626 01:49:48.788767 139724881797504 train.py:288] Epoch 24 CV info cv_loss 76.16722791609195
I0626 01:49:48.788940 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/24.pt
I0626 01:49:49.050004 139724881797504 train.py:274] Epoch 25 TRAIN info lr 0.00047
I0626 01:49:49.053153 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:49:49.053228 139724881797504 executor.py:46] total epoch is 25.
I0626 01:49:56.820833 139724881797504 executor.py:115] TRAIN Batch 25/0 loss 31.045326 loss_att 25.911961 loss_ctc 43.023178 lr 0.00047020 rank 0
I0626 01:50:08.103857 139724881797504 executor.py:115] TRAIN Batch 25/100 loss 76.490067 loss_att 59.395741 loss_ctc 116.376816 lr 0.00047140 rank 0
I0626 01:50:19.673323 139724881797504 executor.py:115] TRAIN Batch 25/200 loss 71.619606 loss_att 58.015705 loss_ctc 103.362045 lr 0.00047260 rank 0
I0626 01:50:31.399713 139724881797504 executor.py:115] TRAIN Batch 25/300 loss 55.644043 loss_att 45.177082 loss_ctc 80.066948 lr 0.00047380 rank 0
I0626 01:50:43.223817 139724881797504 executor.py:115] TRAIN Batch 25/400 loss 44.438305 loss_att 33.994576 loss_ctc 68.807007 lr 0.00047520 rank 0
I0626 01:50:54.813987 139724881797504 executor.py:115] TRAIN Batch 25/500 loss 28.295654 loss_att 22.890882 loss_ctc 40.906792 lr 0.00047640 rank 0
I0626 01:51:05.156208 139724881797504 executor.py:115] TRAIN Batch 25/600 loss 82.175529 loss_att 67.458878 loss_ctc 116.514381 lr 0.00047760 rank 0
I0626 01:51:16.264383 139724881797504 executor.py:115] TRAIN Batch 25/700 loss 68.187057 loss_att 53.963398 loss_ctc 101.375580 lr 0.00047880 rank 0
I0626 01:51:27.723719 139724881797504 executor.py:115] TRAIN Batch 25/800 loss 54.131554 loss_att 41.552692 loss_ctc 83.482239 lr 0.00048020 rank 0
I0626 01:51:39.289166 139724881797504 executor.py:115] TRAIN Batch 25/900 loss 48.300316 loss_att 38.124012 loss_ctc 72.045021 lr 0.00048140 rank 0
I0626 01:51:50.349394 139724881797504 executor.py:115] TRAIN Batch 25/1000 loss 25.569935 loss_att 21.298841 loss_ctc 35.535820 lr 0.00048260 rank 0
I0626 01:52:01.098097 139724881797504 executor.py:115] TRAIN Batch 25/1100 loss 81.158096 loss_att 66.921135 loss_ctc 114.377663 lr 0.00048380 rank 0
I0626 01:52:12.362761 139724881797504 executor.py:115] TRAIN Batch 25/1200 loss 64.531555 loss_att 50.571701 loss_ctc 97.104553 lr 0.00048520 rank 0
I0626 01:52:23.520168 139724881797504 executor.py:115] TRAIN Batch 25/1300 loss 51.996254 loss_att 41.239426 loss_ctc 77.095520 lr 0.00048640 rank 0
I0626 01:52:34.615555 139724881797504 executor.py:115] TRAIN Batch 25/1400 loss 41.452171 loss_att 29.290760 loss_ctc 69.828789 lr 0.00048760 rank 0
I0626 01:52:45.515795 139724881797504 executor.py:115] TRAIN Batch 25/1500 loss 53.992111 loss_att 43.692364 loss_ctc 78.024857 lr 0.00048880 rank 0
I0626 01:52:51.086129 139724881797504 executor.py:152] CV Batch 25/0 loss 36.478165 loss_att 33.906624 loss_ctc 42.478432 history loss 32.425035 rank 0
I0626 01:52:55.084711 139724881797504 executor.py:152] CV Batch 25/100 loss 91.317963 loss_att 80.618652 loss_ctc 116.283005 history loss 67.868640 rank 0
I0626 01:52:58.066815 139724881797504 train.py:288] Epoch 25 CV info cv_loss 76.4356959490034
I0626 01:52:58.066955 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/25.pt
I0626 01:52:58.325023 139724881797504 train.py:274] Epoch 26 TRAIN info lr 0.0004888
I0626 01:52:58.327869 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:52:58.327938 139724881797504 executor.py:46] total epoch is 26.
I0626 01:53:06.271259 139724881797504 executor.py:115] TRAIN Batch 26/0 loss 28.179996 loss_att 23.901464 loss_ctc 38.163235 lr 0.00048900 rank 0
I0626 01:53:17.393986 139724881797504 executor.py:115] TRAIN Batch 26/100 loss 82.945801 loss_att 63.770500 loss_ctc 127.688171 lr 0.00049020 rank 0
I0626 01:53:28.271282 139724881797504 executor.py:115] TRAIN Batch 26/200 loss 60.389107 loss_att 46.780052 loss_ctc 92.143570 lr 0.00049140 rank 0
I0626 01:53:39.652124 139724881797504 executor.py:115] TRAIN Batch 26/300 loss 52.678024 loss_att 41.436817 loss_ctc 78.907509 lr 0.00049260 rank 0
I0626 01:53:51.254355 139724881797504 executor.py:115] TRAIN Batch 26/400 loss 43.992481 loss_att 34.906200 loss_ctc 65.193802 lr 0.00049400 rank 0
I0626 01:54:02.841147 139724881797504 executor.py:115] TRAIN Batch 26/500 loss 29.017860 loss_att 23.509163 loss_ctc 41.871490 lr 0.00049520 rank 0
I0626 01:54:13.362620 139724881797504 executor.py:115] TRAIN Batch 26/600 loss 78.622574 loss_att 61.757030 loss_ctc 117.975510 lr 0.00049640 rank 0
I0626 01:54:24.520896 139724881797504 executor.py:115] TRAIN Batch 26/700 loss 67.274750 loss_att 52.892288 loss_ctc 100.833832 lr 0.00049760 rank 0
I0626 01:54:36.031042 139724881797504 executor.py:115] TRAIN Batch 26/800 loss 58.405891 loss_att 45.489731 loss_ctc 88.543587 lr 0.00049900 rank 0
I0626 01:54:47.659191 139724881797504 executor.py:115] TRAIN Batch 26/900 loss 49.010521 loss_att 39.311089 loss_ctc 71.642532 lr 0.00050020 rank 0
I0626 01:54:58.721660 139724881797504 executor.py:115] TRAIN Batch 26/1000 loss 27.387255 loss_att 23.991085 loss_ctc 35.311653 lr 0.00050140 rank 0
I0626 01:55:09.324823 139724881797504 executor.py:115] TRAIN Batch 26/1100 loss 86.245956 loss_att 69.157883 loss_ctc 126.118134 lr 0.00050260 rank 0
I0626 01:55:20.646476 139724881797504 executor.py:115] TRAIN Batch 26/1200 loss 68.754929 loss_att 52.589905 loss_ctc 106.473312 lr 0.00050400 rank 0
I0626 01:55:32.198664 139724881797504 executor.py:115] TRAIN Batch 26/1300 loss 52.132057 loss_att 42.277851 loss_ctc 75.125206 lr 0.00050520 rank 0
I0626 01:55:43.285150 139724881797504 executor.py:115] TRAIN Batch 26/1400 loss 46.442829 loss_att 36.777351 loss_ctc 68.995605 lr 0.00050640 rank 0
I0626 01:55:54.140334 139724881797504 executor.py:115] TRAIN Batch 26/1500 loss 68.063499 loss_att 54.650108 loss_ctc 99.361412 lr 0.00050760 rank 0
I0626 01:55:59.733901 139724881797504 executor.py:152] CV Batch 26/0 loss 35.021225 loss_att 32.710934 loss_ctc 40.411900 history loss 31.129978 rank 0
I0626 01:56:03.665378 139724881797504 executor.py:152] CV Batch 26/100 loss 91.819504 loss_att 81.875313 loss_ctc 115.022606 history loss 67.310053 rank 0
I0626 01:56:06.643422 139724881797504 train.py:288] Epoch 26 CV info cv_loss 75.93842704863458
I0626 01:56:06.643647 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/26.pt
I0626 01:56:06.902178 139724881797504 train.py:274] Epoch 27 TRAIN info lr 0.0005076
I0626 01:56:06.905221 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:56:06.905284 139724881797504 executor.py:46] total epoch is 27.
I0626 01:56:14.745899 139724881797504 executor.py:115] TRAIN Batch 27/0 loss 22.272148 loss_att 18.414251 loss_ctc 31.273911 lr 0.00050780 rank 0
I0626 01:56:25.584293 139724881797504 executor.py:115] TRAIN Batch 27/100 loss 72.627846 loss_att 56.094345 loss_ctc 111.206017 lr 0.00050900 rank 0
I0626 01:56:36.607105 139724881797504 executor.py:115] TRAIN Batch 27/200 loss 65.884552 loss_att 52.031807 loss_ctc 98.207626 lr 0.00051020 rank 0
I0626 01:56:47.870388 139724881797504 executor.py:115] TRAIN Batch 27/300 loss 55.985023 loss_att 43.061035 loss_ctc 86.140984 lr 0.00051140 rank 0
I0626 01:56:59.663605 139724881797504 executor.py:115] TRAIN Batch 27/400 loss 45.755024 loss_att 35.703941 loss_ctc 69.207550 lr 0.00051280 rank 0
I0626 01:57:11.156641 139724881797504 executor.py:115] TRAIN Batch 27/500 loss 26.944134 loss_att 21.605091 loss_ctc 39.401901 lr 0.00051400 rank 0
I0626 01:57:21.626008 139724881797504 executor.py:115] TRAIN Batch 27/600 loss 81.998535 loss_att 66.541336 loss_ctc 118.065323 lr 0.00051520 rank 0
I0626 01:57:32.818979 139724881797504 executor.py:115] TRAIN Batch 27/700 loss 62.725536 loss_att 48.953499 loss_ctc 94.860291 lr 0.00051640 rank 0
I0626 01:57:44.302922 139724881797504 executor.py:115] TRAIN Batch 27/800 loss 50.305382 loss_att 40.199268 loss_ctc 73.886307 lr 0.00051780 rank 0
I0626 01:57:55.484304 139724881797504 executor.py:115] TRAIN Batch 27/900 loss 45.023464 loss_att 35.141220 loss_ctc 68.082031 lr 0.00051900 rank 0
I0626 01:58:06.506830 139724881797504 executor.py:115] TRAIN Batch 27/1000 loss 27.116901 loss_att 24.187778 loss_ctc 33.951527 lr 0.00052020 rank 0
I0626 01:58:17.292954 139724881797504 executor.py:115] TRAIN Batch 27/1100 loss 84.730682 loss_att 68.034927 loss_ctc 123.687454 lr 0.00052140 rank 0
I0626 01:58:28.746964 139724881797504 executor.py:115] TRAIN Batch 27/1200 loss 75.338448 loss_att 61.290703 loss_ctc 108.116508 lr 0.00052280 rank 0
I0626 01:58:40.401893 139724881797504 executor.py:115] TRAIN Batch 27/1300 loss 51.590359 loss_att 39.585564 loss_ctc 79.601547 lr 0.00052400 rank 0
I0626 01:58:51.993786 139724881797504 executor.py:115] TRAIN Batch 27/1400 loss 45.931908 loss_att 38.368507 loss_ctc 63.579838 lr 0.00052520 rank 0
I0626 01:59:02.941348 139724881797504 executor.py:115] TRAIN Batch 27/1500 loss 76.960861 loss_att 60.485016 loss_ctc 115.404503 lr 0.00052640 rank 0
I0626 01:59:08.534673 139724881797504 executor.py:152] CV Batch 27/0 loss 35.546982 loss_att 33.071163 loss_ctc 41.323883 history loss 31.597317 rank 0
I0626 01:59:12.462031 139724881797504 executor.py:152] CV Batch 27/100 loss 91.817261 loss_att 82.116516 loss_ctc 114.452316 history loss 67.489010 rank 0
I0626 01:59:15.441660 139724881797504 train.py:288] Epoch 27 CV info cv_loss 76.19756481652483
I0626 01:59:15.441879 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/27.pt
I0626 01:59:15.700680 139724881797504 train.py:274] Epoch 28 TRAIN info lr 0.0005264
I0626 01:59:15.703895 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 01:59:15.703967 139724881797504 executor.py:46] total epoch is 28.
I0626 01:59:23.537476 139724881797504 executor.py:115] TRAIN Batch 28/0 loss 27.726904 loss_att 20.871059 loss_ctc 43.723873 lr 0.00052660 rank 0
I0626 01:59:34.472433 139724881797504 executor.py:115] TRAIN Batch 28/100 loss 84.140121 loss_att 66.582420 loss_ctc 125.108093 lr 0.00052780 rank 0
I0626 01:59:45.420132 139724881797504 executor.py:115] TRAIN Batch 28/200 loss 57.566387 loss_att 44.173481 loss_ctc 88.816498 lr 0.00052900 rank 0
I0626 01:59:56.941701 139724881797504 executor.py:115] TRAIN Batch 28/300 loss 53.119865 loss_att 41.628132 loss_ctc 79.933914 lr 0.00053020 rank 0
I0626 02:00:08.538010 139724881797504 executor.py:115] TRAIN Batch 28/400 loss 42.880005 loss_att 32.549881 loss_ctc 66.983635 lr 0.00053160 rank 0
I0626 02:00:20.136965 139724881797504 executor.py:115] TRAIN Batch 28/500 loss 25.020475 loss_att 19.562609 loss_ctc 37.755497 lr 0.00053280 rank 0
I0626 02:00:30.928993 139724881797504 executor.py:115] TRAIN Batch 28/600 loss 77.135262 loss_att 56.999626 loss_ctc 124.118401 lr 0.00053400 rank 0
I0626 02:00:42.444689 139724881797504 executor.py:115] TRAIN Batch 28/700 loss 65.900581 loss_att 53.588249 loss_ctc 94.629364 lr 0.00053520 rank 0
I0626 02:00:54.000739 139724881797504 executor.py:115] TRAIN Batch 28/800 loss 54.651237 loss_att 42.326363 loss_ctc 83.409271 lr 0.00053660 rank 0
I0626 02:01:05.502346 139724881797504 executor.py:115] TRAIN Batch 28/900 loss 48.141502 loss_att 37.365276 loss_ctc 73.286026 lr 0.00053780 rank 0
I0626 02:01:16.473423 139724881797504 executor.py:115] TRAIN Batch 28/1000 loss 21.492161 loss_att 16.943707 loss_ctc 32.105217 lr 0.00053900 rank 0
I0626 02:01:27.473393 139724881797504 executor.py:115] TRAIN Batch 28/1100 loss 72.250061 loss_att 55.337109 loss_ctc 111.713623 lr 0.00054020 rank 0
I0626 02:01:38.741227 139724881797504 executor.py:115] TRAIN Batch 28/1200 loss 71.078491 loss_att 54.795830 loss_ctc 109.071381 lr 0.00054160 rank 0
I0626 02:01:50.304454 139724881797504 executor.py:115] TRAIN Batch 28/1300 loss 51.637524 loss_att 39.705296 loss_ctc 79.479385 lr 0.00054280 rank 0
I0626 02:02:01.573843 139724881797504 executor.py:115] TRAIN Batch 28/1400 loss 44.610065 loss_att 36.204369 loss_ctc 64.223358 lr 0.00054400 rank 0
I0626 02:02:12.611401 139724881797504 executor.py:115] TRAIN Batch 28/1500 loss 62.883415 loss_att 49.690376 loss_ctc 93.667160 lr 0.00054520 rank 0
I0626 02:02:18.198814 139724881797504 executor.py:152] CV Batch 28/0 loss 34.694714 loss_att 31.414185 loss_ctc 42.349281 history loss 30.839745 rank 0
I0626 02:02:22.287762 139724881797504 executor.py:152] CV Batch 28/100 loss 90.782318 loss_att 81.023911 loss_ctc 113.551926 history loss 67.782861 rank 0
I0626 02:02:25.256773 139724881797504 train.py:288] Epoch 28 CV info cv_loss 76.35974438408414
I0626 02:02:25.256948 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/28.pt
I0626 02:02:25.515610 139724881797504 train.py:274] Epoch 29 TRAIN info lr 0.0005451999999999999
I0626 02:02:25.518606 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:02:25.518666 139724881797504 executor.py:46] total epoch is 29.
I0626 02:02:33.387987 139724881797504 executor.py:115] TRAIN Batch 29/0 loss 26.869633 loss_att 22.222229 loss_ctc 37.713577 lr 0.00054540 rank 0
I0626 02:02:44.180525 139724881797504 executor.py:115] TRAIN Batch 29/100 loss 73.076355 loss_att 54.829117 loss_ctc 115.653252 lr 0.00054660 rank 0
I0626 02:02:55.525556 139724881797504 executor.py:115] TRAIN Batch 29/200 loss 59.660843 loss_att 45.048523 loss_ctc 93.756248 lr 0.00054780 rank 0
I0626 02:03:06.900924 139724881797504 executor.py:115] TRAIN Batch 29/300 loss 46.317947 loss_att 32.304077 loss_ctc 79.016983 lr 0.00054900 rank 0
I0626 02:03:18.268186 139724881797504 executor.py:115] TRAIN Batch 29/400 loss 41.199459 loss_att 31.519855 loss_ctc 63.785202 lr 0.00055040 rank 0
I0626 02:03:29.731414 139724881797504 executor.py:115] TRAIN Batch 29/500 loss 26.518978 loss_att 21.545078 loss_ctc 38.124744 lr 0.00055160 rank 0
I0626 02:03:40.248685 139724881797504 executor.py:115] TRAIN Batch 29/600 loss 71.252518 loss_att 52.588493 loss_ctc 114.801926 lr 0.00055280 rank 0
I0626 02:03:51.642769 139724881797504 executor.py:115] TRAIN Batch 29/700 loss 59.966770 loss_att 44.646957 loss_ctc 95.712997 lr 0.00055400 rank 0
I0626 02:04:03.084776 139724881797504 executor.py:115] TRAIN Batch 29/800 loss 50.055950 loss_att 37.916698 loss_ctc 78.380867 lr 0.00055540 rank 0
I0626 02:04:14.420376 139724881797504 executor.py:115] TRAIN Batch 29/900 loss 43.040443 loss_att 33.257023 loss_ctc 65.868423 lr 0.00055660 rank 0
I0626 02:04:25.580844 139724881797504 executor.py:115] TRAIN Batch 29/1000 loss 24.200661 loss_att 19.535038 loss_ctc 35.087112 lr 0.00055780 rank 0
I0626 02:04:36.458803 139724881797504 executor.py:115] TRAIN Batch 29/1100 loss 68.416649 loss_att 50.807659 loss_ctc 109.504288 lr 0.00055900 rank 0
I0626 02:04:47.777637 139724881797504 executor.py:115] TRAIN Batch 29/1200 loss 53.229752 loss_att 37.921009 loss_ctc 88.950150 lr 0.00056040 rank 0
I0626 02:04:59.066109 139724881797504 executor.py:115] TRAIN Batch 29/1300 loss 50.638618 loss_att 37.523529 loss_ctc 81.240494 lr 0.00056160 rank 0
I0626 02:05:10.414810 139724881797504 executor.py:115] TRAIN Batch 29/1400 loss 48.034744 loss_att 37.067066 loss_ctc 73.625999 lr 0.00056280 rank 0
I0626 02:05:21.668364 139724881797504 executor.py:115] TRAIN Batch 29/1500 loss 59.530991 loss_att 46.610229 loss_ctc 89.679443 lr 0.00056400 rank 0
I0626 02:05:27.215060 139724881797504 executor.py:152] CV Batch 29/0 loss 33.582703 loss_att 31.535278 loss_ctc 38.360023 history loss 29.851291 rank 0
I0626 02:05:31.133934 139724881797504 executor.py:152] CV Batch 29/100 loss 90.000961 loss_att 81.882828 loss_ctc 108.943283 history loss 65.884579 rank 0
I0626 02:05:34.107920 139724881797504 train.py:288] Epoch 29 CV info cv_loss 74.87397655372592
I0626 02:05:34.108085 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/29.pt
I0626 02:05:34.368983 139724881797504 train.py:274] Epoch 30 TRAIN info lr 0.0005639999999999999
I0626 02:05:34.371908 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:05:34.371979 139724881797504 executor.py:46] total epoch is 30.
I0626 02:05:42.391880 139724881797504 executor.py:115] TRAIN Batch 30/0 loss 24.619722 loss_att 21.070658 loss_ctc 32.900871 lr 0.00056420 rank 0
I0626 02:05:53.338194 139724881797504 executor.py:115] TRAIN Batch 30/100 loss 76.453186 loss_att 60.130352 loss_ctc 114.539803 lr 0.00056540 rank 0
I0626 02:06:04.562045 139724881797504 executor.py:115] TRAIN Batch 30/200 loss 60.555443 loss_att 46.172649 loss_ctc 94.115295 lr 0.00056660 rank 0
I0626 02:06:15.873621 139724881797504 executor.py:115] TRAIN Batch 30/300 loss 46.576378 loss_att 33.578449 loss_ctc 76.904877 lr 0.00056780 rank 0
I0626 02:06:27.330127 139724881797504 executor.py:115] TRAIN Batch 30/400 loss 43.205742 loss_att 33.084129 loss_ctc 66.822838 lr 0.00056920 rank 0
I0626 02:06:39.024390 139724881797504 executor.py:115] TRAIN Batch 30/500 loss 25.069857 loss_att 19.673634 loss_ctc 37.661041 lr 0.00057040 rank 0
I0626 02:06:49.428222 139724881797504 executor.py:115] TRAIN Batch 30/600 loss 68.879234 loss_att 51.840981 loss_ctc 108.635162 lr 0.00057160 rank 0
I0626 02:07:00.813092 139724881797504 executor.py:115] TRAIN Batch 30/700 loss 58.098312 loss_att 44.545872 loss_ctc 89.720680 lr 0.00057280 rank 0
I0626 02:07:12.519901 139724881797504 executor.py:115] TRAIN Batch 30/800 loss 50.687714 loss_att 37.934273 loss_ctc 80.445740 lr 0.00057420 rank 0
I0626 02:07:24.032175 139724881797504 executor.py:115] TRAIN Batch 30/900 loss 44.945992 loss_att 34.652935 loss_ctc 68.963120 lr 0.00057540 rank 0
I0626 02:07:35.432172 139724881797504 executor.py:115] TRAIN Batch 30/1000 loss 23.444818 loss_att 19.621321 loss_ctc 32.366314 lr 0.00057660 rank 0
I0626 02:07:46.404144 139724881797504 executor.py:115] TRAIN Batch 30/1100 loss 78.874695 loss_att 62.791145 loss_ctc 116.402985 lr 0.00057780 rank 0
I0626 02:07:57.772716 139724881797504 executor.py:115] TRAIN Batch 30/1200 loss 61.713390 loss_att 47.861229 loss_ctc 94.035095 lr 0.00057920 rank 0
I0626 02:08:09.067037 139724881797504 executor.py:115] TRAIN Batch 30/1300 loss 52.152142 loss_att 40.795998 loss_ctc 78.649811 lr 0.00058040 rank 0
I0626 02:08:20.522700 139724881797504 executor.py:115] TRAIN Batch 30/1400 loss 40.841034 loss_att 30.376076 loss_ctc 65.259270 lr 0.00058160 rank 0
I0626 02:08:31.452587 139724881797504 executor.py:115] TRAIN Batch 30/1500 loss 49.786545 loss_att 40.607315 loss_ctc 71.204750 lr 0.00058280 rank 0
I0626 02:08:37.014911 139724881797504 executor.py:152] CV Batch 30/0 loss 34.220428 loss_att 32.248825 loss_ctc 38.820831 history loss 30.418159 rank 0
I0626 02:08:41.087627 139724881797504 executor.py:152] CV Batch 30/100 loss 89.744789 loss_att 82.649887 loss_ctc 106.299561 history loss 65.370449 rank 0
I0626 02:08:44.058751 139724881797504 train.py:288] Epoch 30 CV info cv_loss 73.99989369958743
I0626 02:08:44.058912 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/30.pt
I0626 02:08:44.317893 139724881797504 train.py:274] Epoch 31 TRAIN info lr 0.0005828
I0626 02:08:44.320938 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:08:44.321012 139724881797504 executor.py:46] total epoch is 31.
I0626 02:08:52.186186 139724881797504 executor.py:115] TRAIN Batch 31/0 loss 26.426958 loss_att 21.815319 loss_ctc 37.187447 lr 0.00058300 rank 0
I0626 02:09:03.032657 139724881797504 executor.py:115] TRAIN Batch 31/100 loss 68.431076 loss_att 52.324417 loss_ctc 106.013268 lr 0.00058420 rank 0
I0626 02:09:14.248332 139724881797504 executor.py:115] TRAIN Batch 31/200 loss 57.618511 loss_att 44.556816 loss_ctc 88.095795 lr 0.00058540 rank 0
I0626 02:09:25.569064 139724881797504 executor.py:115] TRAIN Batch 31/300 loss 50.011581 loss_att 38.273834 loss_ctc 77.399658 lr 0.00058660 rank 0
I0626 02:09:36.998653 139724881797504 executor.py:115] TRAIN Batch 31/400 loss 38.889145 loss_att 30.554810 loss_ctc 58.335930 lr 0.00058800 rank 0
I0626 02:09:48.763507 139724881797504 executor.py:115] TRAIN Batch 31/500 loss 33.014278 loss_att 27.236076 loss_ctc 46.496754 lr 0.00058920 rank 0
I0626 02:09:59.369540 139724881797504 executor.py:115] TRAIN Batch 31/600 loss 78.717651 loss_att 66.766083 loss_ctc 106.604660 lr 0.00059040 rank 0
I0626 02:10:10.538647 139724881797504 executor.py:115] TRAIN Batch 31/700 loss 41.312668 loss_att 30.486589 loss_ctc 66.573517 lr 0.00059160 rank 0
I0626 02:10:22.180102 139724881797504 executor.py:115] TRAIN Batch 31/800 loss 44.770241 loss_att 35.416618 loss_ctc 66.595352 lr 0.00059300 rank 0
I0626 02:10:33.781260 139724881797504 executor.py:115] TRAIN Batch 31/900 loss 40.075855 loss_att 29.585205 loss_ctc 64.554039 lr 0.00059420 rank 0
I0626 02:10:45.072261 139724881797504 executor.py:115] TRAIN Batch 31/1000 loss 24.952148 loss_att 21.552387 loss_ctc 32.884922 lr 0.00059540 rank 0
I0626 02:10:56.169866 139724881797504 executor.py:115] TRAIN Batch 31/1100 loss 72.743851 loss_att 61.718697 loss_ctc 98.469200 lr 0.00059660 rank 0
I0626 02:11:07.587159 139724881797504 executor.py:115] TRAIN Batch 31/1200 loss 58.980942 loss_att 46.674477 loss_ctc 87.696014 lr 0.00059800 rank 0
I0626 02:11:19.068928 139724881797504 executor.py:115] TRAIN Batch 31/1300 loss 49.025146 loss_att 37.583839 loss_ctc 75.721527 lr 0.00059920 rank 0
I0626 02:11:30.140678 139724881797504 executor.py:115] TRAIN Batch 31/1400 loss 37.059326 loss_att 28.553118 loss_ctc 56.907135 lr 0.00060040 rank 0
I0626 02:11:41.173504 139724881797504 executor.py:115] TRAIN Batch 31/1500 loss 41.481674 loss_att 32.691200 loss_ctc 61.992767 lr 0.00060160 rank 0
I0626 02:11:46.762359 139724881797504 executor.py:152] CV Batch 31/0 loss 32.288967 loss_att 31.432812 loss_ctc 34.286659 history loss 28.701304 rank 0
I0626 02:11:50.899996 139724881797504 executor.py:152] CV Batch 31/100 loss 86.280083 loss_att 83.057755 loss_ctc 93.798859 history loss 62.794764 rank 0
I0626 02:11:53.882602 139724881797504 train.py:288] Epoch 31 CV info cv_loss 71.31059835877188
I0626 02:11:53.882773 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/31.pt
I0626 02:11:54.142748 139724881797504 train.py:274] Epoch 32 TRAIN info lr 0.0006016
I0626 02:11:54.145818 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:11:54.145879 139724881797504 executor.py:46] total epoch is 32.
I0626 02:12:01.979062 139724881797504 executor.py:115] TRAIN Batch 32/0 loss 28.770571 loss_att 23.827911 loss_ctc 40.303444 lr 0.00060180 rank 0
I0626 02:12:13.084800 139724881797504 executor.py:115] TRAIN Batch 32/100 loss 71.914253 loss_att 60.756439 loss_ctc 97.949158 lr 0.00060300 rank 0
I0626 02:12:24.128160 139724881797504 executor.py:115] TRAIN Batch 32/200 loss 53.547157 loss_att 41.750217 loss_ctc 81.073349 lr 0.00060420 rank 0
I0626 02:12:35.549780 139724881797504 executor.py:115] TRAIN Batch 32/300 loss 41.977943 loss_att 30.909889 loss_ctc 67.803391 lr 0.00060540 rank 0
I0626 02:12:47.038407 139724881797504 executor.py:115] TRAIN Batch 32/400 loss 39.092712 loss_att 30.008232 loss_ctc 60.289841 lr 0.00060680 rank 0
I0626 02:12:58.802279 139724881797504 executor.py:115] TRAIN Batch 32/500 loss 31.149807 loss_att 25.054119 loss_ctc 45.373077 lr 0.00060800 rank 0
I0626 02:13:09.155188 139724881797504 executor.py:115] TRAIN Batch 32/600 loss 68.496696 loss_att 53.644787 loss_ctc 103.151154 lr 0.00060920 rank 0
I0626 02:13:20.299549 139724881797504 executor.py:115] TRAIN Batch 32/700 loss 57.842201 loss_att 45.312023 loss_ctc 87.079285 lr 0.00061040 rank 0
I0626 02:13:32.067990 139724881797504 executor.py:115] TRAIN Batch 32/800 loss 49.435005 loss_att 38.580727 loss_ctc 74.761658 lr 0.00061180 rank 0
I0626 02:13:43.423224 139724881797504 executor.py:115] TRAIN Batch 32/900 loss 34.622116 loss_att 26.261879 loss_ctc 54.129345 lr 0.00061300 rank 0
I0626 02:13:54.512083 139724881797504 executor.py:115] TRAIN Batch 32/1000 loss 20.390137 loss_att 16.417761 loss_ctc 29.659014 lr 0.00061420 rank 0
I0626 02:14:05.462660 139724881797504 executor.py:115] TRAIN Batch 32/1100 loss 66.170349 loss_att 53.263992 loss_ctc 96.285187 lr 0.00061540 rank 0
I0626 02:14:16.875474 139724881797504 executor.py:115] TRAIN Batch 32/1200 loss 56.782776 loss_att 45.268234 loss_ctc 83.650047 lr 0.00061680 rank 0
I0626 02:14:28.381563 139724881797504 executor.py:115] TRAIN Batch 32/1300 loss 54.063759 loss_att 44.887978 loss_ctc 75.473923 lr 0.00061800 rank 0
I0626 02:14:39.660779 139724881797504 executor.py:115] TRAIN Batch 32/1400 loss 34.789639 loss_att 28.844479 loss_ctc 48.661674 lr 0.00061920 rank 0
I0626 02:14:50.675384 139724881797504 executor.py:115] TRAIN Batch 32/1500 loss 42.773399 loss_att 35.826397 loss_ctc 58.983063 lr 0.00062040 rank 0
I0626 02:14:56.273985 139724881797504 executor.py:152] CV Batch 32/0 loss 33.139420 loss_att 32.415466 loss_ctc 34.828644 history loss 29.457262 rank 0
I0626 02:15:00.265346 139724881797504 executor.py:152] CV Batch 32/100 loss 85.731133 loss_att 84.058693 loss_ctc 89.633484 history loss 61.948211 rank 0
I0626 02:15:03.255720 139724881797504 train.py:288] Epoch 32 CV info cv_loss 70.11737497120743
I0626 02:15:03.256063 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/32.pt
I0626 02:15:03.520608 139724881797504 train.py:274] Epoch 33 TRAIN info lr 0.0006204
I0626 02:15:03.523644 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:15:03.523716 139724881797504 executor.py:46] total epoch is 33.
I0626 02:15:11.370087 139724881797504 executor.py:115] TRAIN Batch 33/0 loss 23.948013 loss_att 19.014425 loss_ctc 35.459717 lr 0.00062060 rank 0
I0626 02:15:22.230228 139724881797504 executor.py:115] TRAIN Batch 33/100 loss 63.277763 loss_att 50.671654 loss_ctc 92.692009 lr 0.00062180 rank 0
I0626 02:15:33.245540 139724881797504 executor.py:115] TRAIN Batch 33/200 loss 49.564964 loss_att 40.000946 loss_ctc 71.880997 lr 0.00062300 rank 0
I0626 02:15:44.459112 139724881797504 executor.py:115] TRAIN Batch 33/300 loss 42.851761 loss_att 32.786110 loss_ctc 66.338272 lr 0.00062420 rank 0
I0626 02:15:55.982135 139724881797504 executor.py:115] TRAIN Batch 33/400 loss 39.986343 loss_att 31.513334 loss_ctc 59.756691 lr 0.00062560 rank 0
I0626 02:16:07.664304 139724881797504 executor.py:115] TRAIN Batch 33/500 loss 21.129768 loss_att 17.683147 loss_ctc 29.171883 lr 0.00062680 rank 0
I0626 02:16:18.436426 139724881797504 executor.py:115] TRAIN Batch 33/600 loss 55.914642 loss_att 41.437954 loss_ctc 89.693573 lr 0.00062800 rank 0
I0626 02:16:29.781798 139724881797504 executor.py:115] TRAIN Batch 33/700 loss 47.990120 loss_att 39.436054 loss_ctc 67.949600 lr 0.00062920 rank 0
I0626 02:16:41.425683 139724881797504 executor.py:115] TRAIN Batch 33/800 loss 48.400024 loss_att 38.102654 loss_ctc 72.427216 lr 0.00063060 rank 0
I0626 02:16:52.996619 139724881797504 executor.py:115] TRAIN Batch 33/900 loss 34.575989 loss_att 27.035006 loss_ctc 52.171623 lr 0.00063180 rank 0
I0626 02:17:04.348852 139724881797504 executor.py:115] TRAIN Batch 33/1000 loss 19.113579 loss_att 15.790374 loss_ctc 26.867722 lr 0.00063300 rank 0
I0626 02:17:15.532396 139724881797504 executor.py:115] TRAIN Batch 33/1100 loss 64.738419 loss_att 54.890228 loss_ctc 87.717514 lr 0.00063420 rank 0
I0626 02:17:26.944290 139724881797504 executor.py:115] TRAIN Batch 33/1200 loss 52.577370 loss_att 42.572662 loss_ctc 75.921677 lr 0.00063560 rank 0
I0626 02:17:38.284134 139724881797504 executor.py:115] TRAIN Batch 33/1300 loss 45.914436 loss_att 36.892769 loss_ctc 66.964996 lr 0.00063680 rank 0
I0626 02:17:49.486237 139724881797504 executor.py:115] TRAIN Batch 33/1400 loss 40.031292 loss_att 34.588665 loss_ctc 52.730755 lr 0.00063800 rank 0
I0626 02:18:00.518613 139724881797504 executor.py:115] TRAIN Batch 33/1500 loss 51.547729 loss_att 44.650661 loss_ctc 67.640892 lr 0.00063920 rank 0
I0626 02:18:06.113770 139724881797504 executor.py:152] CV Batch 33/0 loss 31.169321 loss_att 31.043411 loss_ctc 31.463112 history loss 27.706063 rank 0
I0626 02:18:10.125048 139724881797504 executor.py:152] CV Batch 33/100 loss 82.931290 loss_att 83.112953 loss_ctc 82.507401 history loss 60.547803 rank 0
I0626 02:18:13.101844 139724881797504 train.py:288] Epoch 33 CV info cv_loss 68.63253429596523
I0626 02:18:13.102015 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/33.pt
I0626 02:18:13.364473 139724881797504 train.py:274] Epoch 34 TRAIN info lr 0.0006391999999999999
I0626 02:18:13.367181 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:18:13.367240 139724881797504 executor.py:46] total epoch is 34.
I0626 02:18:21.216172 139724881797504 executor.py:115] TRAIN Batch 34/0 loss 23.578918 loss_att 19.567568 loss_ctc 32.938736 lr 0.00063940 rank 0
I0626 02:18:31.625194 139724881797504 executor.py:115] TRAIN Batch 34/100 loss 55.540916 loss_att 46.554173 loss_ctc 76.509979 lr 0.00064060 rank 0
I0626 02:18:43.001046 139724881797504 executor.py:115] TRAIN Batch 34/200 loss 49.971355 loss_att 39.273659 loss_ctc 74.932648 lr 0.00064180 rank 0
I0626 02:18:54.423795 139724881797504 executor.py:115] TRAIN Batch 34/300 loss 38.370392 loss_att 30.845678 loss_ctc 55.928055 lr 0.00064300 rank 0
I0626 02:19:05.690271 139724881797504 executor.py:115] TRAIN Batch 34/400 loss 33.889378 loss_att 25.758720 loss_ctc 52.860916 lr 0.00064440 rank 0
I0626 02:19:17.241710 139724881797504 executor.py:115] TRAIN Batch 34/500 loss 21.633213 loss_att 18.574883 loss_ctc 28.769321 lr 0.00064560 rank 0
I0626 02:19:28.117941 139724881797504 executor.py:115] TRAIN Batch 34/600 loss 59.711594 loss_att 49.698872 loss_ctc 83.074600 lr 0.00064680 rank 0
I0626 02:19:39.377952 139724881797504 executor.py:115] TRAIN Batch 34/700 loss 45.500015 loss_att 36.081120 loss_ctc 67.477448 lr 0.00064800 rank 0
I0626 02:19:50.785104 139724881797504 executor.py:115] TRAIN Batch 34/800 loss 40.757263 loss_att 31.301277 loss_ctc 62.821236 lr 0.00064940 rank 0
I0626 02:20:02.086380 139724881797504 executor.py:115] TRAIN Batch 34/900 loss 38.484612 loss_att 27.845543 loss_ctc 63.309097 lr 0.00065060 rank 0
I0626 02:20:13.171765 139724881797504 executor.py:115] TRAIN Batch 34/1000 loss 21.905928 loss_att 17.112617 loss_ctc 33.090317 lr 0.00065180 rank 0
I0626 02:20:23.688370 139724881797504 executor.py:115] TRAIN Batch 34/1100 loss 67.131493 loss_att 58.331074 loss_ctc 87.665787 lr 0.00065300 rank 0
I0626 02:20:34.854554 139724881797504 executor.py:115] TRAIN Batch 34/1200 loss 52.993694 loss_att 43.156651 loss_ctc 75.946793 lr 0.00065440 rank 0
I0626 02:20:45.965737 139724881797504 executor.py:115] TRAIN Batch 34/1300 loss 45.484764 loss_att 35.470261 loss_ctc 68.851944 lr 0.00065560 rank 0
I0626 02:20:57.149895 139724881797504 executor.py:115] TRAIN Batch 34/1400 loss 35.886688 loss_att 28.591322 loss_ctc 52.909210 lr 0.00065680 rank 0
I0626 02:21:08.715290 139724881797504 executor.py:115] TRAIN Batch 34/1500 loss 40.291660 loss_att 33.728096 loss_ctc 55.606640 lr 0.00065800 rank 0
I0626 02:21:14.562337 139724881797504 executor.py:152] CV Batch 34/0 loss 31.120836 loss_att 32.112965 loss_ctc 28.805870 history loss 27.662966 rank 0
I0626 02:21:18.600204 139724881797504 executor.py:152] CV Batch 34/100 loss 81.943512 loss_att 83.844925 loss_ctc 77.506874 history loss 59.757886 rank 0
I0626 02:21:21.591921 139724881797504 train.py:288] Epoch 34 CV info cv_loss 67.79852057468284
I0626 02:21:21.592131 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/34.pt
I0626 02:21:21.850938 139724881797504 train.py:274] Epoch 35 TRAIN info lr 0.000658
I0626 02:21:21.853862 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:21:21.854119 139724881797504 executor.py:46] total epoch is 35.
I0626 02:21:29.559162 139724881797504 executor.py:115] TRAIN Batch 35/0 loss 22.925484 loss_att 19.951591 loss_ctc 29.864561 lr 0.00065820 rank 0
I0626 02:21:40.572110 139724881797504 executor.py:115] TRAIN Batch 35/100 loss 48.837051 loss_att 38.395218 loss_ctc 73.201324 lr 0.00065940 rank 0
I0626 02:21:51.470370 139724881797504 executor.py:115] TRAIN Batch 35/200 loss 43.505215 loss_att 35.538082 loss_ctc 62.095188 lr 0.00066060 rank 0
I0626 02:22:02.523219 139724881797504 executor.py:115] TRAIN Batch 35/300 loss 41.543800 loss_att 30.955023 loss_ctc 66.250946 lr 0.00066180 rank 0
I0626 02:22:14.115629 139724881797504 executor.py:115] TRAIN Batch 35/400 loss 36.572662 loss_att 28.467907 loss_ctc 55.483765 lr 0.00066320 rank 0
I0626 02:22:25.497411 139724881797504 executor.py:115] TRAIN Batch 35/500 loss 22.683460 loss_att 17.803108 loss_ctc 34.070950 lr 0.00066440 rank 0
I0626 02:22:36.382379 139724881797504 executor.py:115] TRAIN Batch 35/600 loss 57.944958 loss_att 50.413200 loss_ctc 75.519066 lr 0.00066560 rank 0
I0626 02:22:47.849261 139724881797504 executor.py:115] TRAIN Batch 35/700 loss 45.837204 loss_att 37.148365 loss_ctc 66.111153 lr 0.00066680 rank 0
I0626 02:22:59.558217 139724881797504 executor.py:115] TRAIN Batch 35/800 loss 36.405731 loss_att 27.871733 loss_ctc 56.318398 lr 0.00066820 rank 0
I0626 02:23:11.045502 139724881797504 executor.py:115] TRAIN Batch 35/900 loss 32.792408 loss_att 26.839190 loss_ctc 46.683250 lr 0.00066940 rank 0
I0626 02:23:22.419914 139724881797504 executor.py:115] TRAIN Batch 35/1000 loss 22.347857 loss_att 17.757114 loss_ctc 33.059586 lr 0.00067060 rank 0
I0626 02:23:33.466145 139724881797504 executor.py:115] TRAIN Batch 35/1100 loss 58.347084 loss_att 51.248405 loss_ctc 74.910675 lr 0.00067180 rank 0
I0626 02:23:44.914434 139724881797504 executor.py:115] TRAIN Batch 35/1200 loss 52.270012 loss_att 43.194080 loss_ctc 73.447189 lr 0.00067320 rank 0
I0626 02:23:56.408740 139724881797504 executor.py:115] TRAIN Batch 35/1300 loss 44.867619 loss_att 34.689266 loss_ctc 68.617104 lr 0.00067440 rank 0
I0626 02:24:08.105125 139724881797504 executor.py:115] TRAIN Batch 35/1400 loss 36.265633 loss_att 26.783958 loss_ctc 58.389542 lr 0.00067560 rank 0
I0626 02:24:19.403133 139724881797504 executor.py:115] TRAIN Batch 35/1500 loss 49.031349 loss_att 40.444176 loss_ctc 69.068085 lr 0.00067680 rank 0
I0626 02:24:24.988104 139724881797504 executor.py:152] CV Batch 35/0 loss 31.424421 loss_att 32.004898 loss_ctc 30.069973 history loss 27.932819 rank 0
I0626 02:24:28.979015 139724881797504 executor.py:152] CV Batch 35/100 loss 80.835770 loss_att 82.207077 loss_ctc 77.636063 history loss 59.338047 rank 0
I0626 02:24:31.970616 139724881797504 train.py:288] Epoch 35 CV info cv_loss 67.38960898612982
I0626 02:24:31.970762 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/35.pt
I0626 02:24:32.229593 139724881797504 train.py:274] Epoch 36 TRAIN info lr 0.0006767999999999999
I0626 02:24:32.232666 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:24:32.232739 139724881797504 executor.py:46] total epoch is 36.
I0626 02:24:39.948872 139724881797504 executor.py:115] TRAIN Batch 36/0 loss 23.277086 loss_att 18.904100 loss_ctc 33.480721 lr 0.00067700 rank 0
I0626 02:24:51.103989 139724881797504 executor.py:115] TRAIN Batch 36/100 loss 59.180626 loss_att 46.831306 loss_ctc 87.995697 lr 0.00067820 rank 0
I0626 02:25:02.259966 139724881797504 executor.py:115] TRAIN Batch 36/200 loss 44.183941 loss_att 37.521530 loss_ctc 59.729568 lr 0.00067940 rank 0
I0626 02:25:13.866418 139724881797504 executor.py:115] TRAIN Batch 36/300 loss 34.746334 loss_att 26.058365 loss_ctc 55.018265 lr 0.00068060 rank 0
I0626 02:25:25.667982 139724881797504 executor.py:115] TRAIN Batch 36/400 loss 28.156054 loss_att 21.455738 loss_ctc 43.790123 lr 0.00068200 rank 0
I0626 02:25:37.203936 139724881797504 executor.py:115] TRAIN Batch 36/500 loss 22.898779 loss_att 19.590149 loss_ctc 30.618916 lr 0.00068320 rank 0
I0626 02:25:47.642374 139724881797504 executor.py:115] TRAIN Batch 36/600 loss 58.916698 loss_att 49.121998 loss_ctc 81.770996 lr 0.00068440 rank 0
I0626 02:25:58.756430 139724881797504 executor.py:115] TRAIN Batch 36/700 loss 50.389652 loss_att 42.336296 loss_ctc 69.180817 lr 0.00068560 rank 0
I0626 02:26:10.356350 139724881797504 executor.py:115] TRAIN Batch 36/800 loss 32.822502 loss_att 25.564117 loss_ctc 49.758724 lr 0.00068700 rank 0
I0626 02:26:21.848745 139724881797504 executor.py:115] TRAIN Batch 36/900 loss 32.892609 loss_att 24.180445 loss_ctc 53.220985 lr 0.00068820 rank 0
I0626 02:26:33.327909 139724881797504 executor.py:115] TRAIN Batch 36/1000 loss 19.952663 loss_att 15.201561 loss_ctc 31.038567 lr 0.00068940 rank 0
I0626 02:26:44.005401 139724881797504 executor.py:115] TRAIN Batch 36/1100 loss 55.745941 loss_att 47.613785 loss_ctc 74.720985 lr 0.00069060 rank 0
I0626 02:26:55.392966 139724881797504 executor.py:115] TRAIN Batch 36/1200 loss 48.047840 loss_att 39.772743 loss_ctc 67.356400 lr 0.00069200 rank 0
I0626 02:27:06.919414 139724881797504 executor.py:115] TRAIN Batch 36/1300 loss 37.941566 loss_att 30.449738 loss_ctc 55.422501 lr 0.00069320 rank 0
I0626 02:27:18.164072 139724881797504 executor.py:115] TRAIN Batch 36/1400 loss 31.727192 loss_att 27.167753 loss_ctc 42.365879 lr 0.00069440 rank 0
I0626 02:27:29.197481 139724881797504 executor.py:115] TRAIN Batch 36/1500 loss 40.554192 loss_att 34.206745 loss_ctc 55.364891 lr 0.00069560 rank 0
I0626 02:27:34.773751 139724881797504 executor.py:152] CV Batch 36/0 loss 28.103840 loss_att 28.280025 loss_ctc 27.692741 history loss 24.981191 rank 0
I0626 02:27:38.807290 139724881797504 executor.py:152] CV Batch 36/100 loss 79.765198 loss_att 82.434570 loss_ctc 73.536652 history loss 56.874724 rank 0
I0626 02:27:41.805589 139724881797504 train.py:288] Epoch 36 CV info cv_loss 65.05881930961529
I0626 02:27:41.805809 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/36.pt
I0626 02:27:42.063775 139724881797504 train.py:274] Epoch 37 TRAIN info lr 0.0006956
I0626 02:27:42.066868 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:27:42.066929 139724881797504 executor.py:46] total epoch is 37.
I0626 02:27:49.868941 139724881797504 executor.py:115] TRAIN Batch 37/0 loss 18.638119 loss_att 15.060500 loss_ctc 26.985897 lr 0.00069580 rank 0
I0626 02:28:00.717711 139724881797504 executor.py:115] TRAIN Batch 37/100 loss 50.269566 loss_att 43.155849 loss_ctc 66.868233 lr 0.00069700 rank 0
I0626 02:28:11.792797 139724881797504 executor.py:115] TRAIN Batch 37/200 loss 41.663677 loss_att 31.022478 loss_ctc 66.493141 lr 0.00069820 rank 0
I0626 02:28:23.252951 139724881797504 executor.py:115] TRAIN Batch 37/300 loss 37.389709 loss_att 27.310345 loss_ctc 60.908234 lr 0.00069940 rank 0
I0626 02:28:34.728172 139724881797504 executor.py:115] TRAIN Batch 37/400 loss 37.035847 loss_att 29.068851 loss_ctc 55.625500 lr 0.00070080 rank 0
I0626 02:28:46.128580 139724881797504 executor.py:115] TRAIN Batch 37/500 loss 18.811275 loss_att 14.418023 loss_ctc 29.062195 lr 0.00070200 rank 0
I0626 02:28:56.906521 139724881797504 executor.py:115] TRAIN Batch 37/600 loss 55.877262 loss_att 47.568390 loss_ctc 75.264633 lr 0.00070320 rank 0
I0626 02:29:08.303766 139724881797504 executor.py:115] TRAIN Batch 37/700 loss 42.226562 loss_att 34.798782 loss_ctc 59.558044 lr 0.00070440 rank 0
I0626 02:29:20.342096 139724881797504 executor.py:115] TRAIN Batch 37/800 loss 40.063950 loss_att 30.636641 loss_ctc 62.061008 lr 0.00070580 rank 0
I0626 02:29:32.083646 139724881797504 executor.py:115] TRAIN Batch 37/900 loss 29.903706 loss_att 23.029533 loss_ctc 45.943439 lr 0.00070700 rank 0
I0626 02:29:42.963909 139724881797504 executor.py:115] TRAIN Batch 37/1000 loss 14.961686 loss_att 11.388824 loss_ctc 23.298363 lr 0.00070820 rank 0
I0626 02:29:53.582519 139724881797504 executor.py:115] TRAIN Batch 37/1100 loss 50.213287 loss_att 38.545975 loss_ctc 77.437019 lr 0.00070940 rank 0
I0626 02:30:04.942252 139724881797504 executor.py:115] TRAIN Batch 37/1200 loss 39.161175 loss_att 29.660332 loss_ctc 61.329807 lr 0.00071080 rank 0
I0626 02:30:16.379312 139724881797504 executor.py:115] TRAIN Batch 37/1300 loss 32.113586 loss_att 25.367218 loss_ctc 47.855114 lr 0.00071200 rank 0
I0626 02:30:27.690326 139724881797504 executor.py:115] TRAIN Batch 37/1400 loss 27.212639 loss_att 21.098141 loss_ctc 41.479801 lr 0.00071320 rank 0
I0626 02:30:38.884462 139724881797504 executor.py:115] TRAIN Batch 37/1500 loss 33.240002 loss_att 26.191153 loss_ctc 49.687309 lr 0.00071440 rank 0
I0626 02:30:44.484563 139724881797504 executor.py:152] CV Batch 37/0 loss 29.837450 loss_att 30.971756 loss_ctc 27.190735 history loss 26.522178 rank 0
I0626 02:30:48.476078 139724881797504 executor.py:152] CV Batch 37/100 loss 79.842026 loss_att 82.963669 loss_ctc 72.558182 history loss 56.101870 rank 0
I0626 02:30:51.455234 139724881797504 train.py:288] Epoch 37 CV info cv_loss 63.736839289109525
I0626 02:30:51.455510 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/37.pt
I0626 02:30:51.719441 139724881797504 train.py:274] Epoch 38 TRAIN info lr 0.0007143999999999999
I0626 02:30:51.722429 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:30:51.722490 139724881797504 executor.py:46] total epoch is 38.
I0626 02:30:59.683013 139724881797504 executor.py:115] TRAIN Batch 38/0 loss 20.293449 loss_att 15.150677 loss_ctc 32.293251 lr 0.00071460 rank 0
I0626 02:31:10.540538 139724881797504 executor.py:115] TRAIN Batch 38/100 loss 53.344307 loss_att 41.093529 loss_ctc 81.929459 lr 0.00071580 rank 0
I0626 02:31:21.590477 139724881797504 executor.py:115] TRAIN Batch 38/200 loss 40.092728 loss_att 33.465645 loss_ctc 55.555916 lr 0.00071700 rank 0
I0626 02:31:32.756253 139724881797504 executor.py:115] TRAIN Batch 38/300 loss 33.194885 loss_att 24.035507 loss_ctc 54.566765 lr 0.00071820 rank 0
I0626 02:31:44.139014 139724881797504 executor.py:115] TRAIN Batch 38/400 loss 30.946102 loss_att 23.125124 loss_ctc 49.195053 lr 0.00071960 rank 0
I0626 02:31:55.592129 139724881797504 executor.py:115] TRAIN Batch 38/500 loss 22.830776 loss_att 19.113144 loss_ctc 31.505245 lr 0.00072080 rank 0
I0626 02:32:06.017927 139724881797504 executor.py:115] TRAIN Batch 38/600 loss 54.260227 loss_att 42.768242 loss_ctc 81.074860 lr 0.00072200 rank 0
I0626 02:32:17.221237 139724881797504 executor.py:115] TRAIN Batch 38/700 loss 41.081356 loss_att 32.365059 loss_ctc 61.419384 lr 0.00072320 rank 0
I0626 02:32:29.063245 139724881797504 executor.py:115] TRAIN Batch 38/800 loss 36.261330 loss_att 27.710514 loss_ctc 56.213238 lr 0.00072460 rank 0
I0626 02:32:40.746333 139724881797504 executor.py:115] TRAIN Batch 38/900 loss 33.585209 loss_att 25.451830 loss_ctc 52.563095 lr 0.00072580 rank 0
I0626 02:32:51.839829 139724881797504 executor.py:115] TRAIN Batch 38/1000 loss 19.458641 loss_att 17.674801 loss_ctc 23.620937 lr 0.00072700 rank 0
I0626 02:33:02.845908 139724881797504 executor.py:115] TRAIN Batch 38/1100 loss 57.062855 loss_att 49.362465 loss_ctc 75.030426 lr 0.00072820 rank 0
I0626 02:33:13.959677 139724881797504 executor.py:115] TRAIN Batch 38/1200 loss 40.605171 loss_att 33.722717 loss_ctc 56.664223 lr 0.00072960 rank 0
I0626 02:33:25.379500 139724881797504 executor.py:115] TRAIN Batch 38/1300 loss 33.015930 loss_att 26.783932 loss_ctc 47.557262 lr 0.00073080 rank 0
I0626 02:33:36.400581 139724881797504 executor.py:115] TRAIN Batch 38/1400 loss 32.278587 loss_att 25.646322 loss_ctc 47.753876 lr 0.00073200 rank 0
I0626 02:33:47.342118 139724881797504 executor.py:115] TRAIN Batch 38/1500 loss 38.623550 loss_att 32.103394 loss_ctc 53.837250 lr 0.00073320 rank 0
I0626 02:33:52.912072 139724881797504 executor.py:152] CV Batch 38/0 loss 26.637646 loss_att 26.889412 loss_ctc 26.050190 history loss 23.677907 rank 0
I0626 02:33:56.870465 139724881797504 executor.py:152] CV Batch 38/100 loss 74.705719 loss_att 78.747368 loss_ctc 65.275208 history loss 54.342835 rank 0
I0626 02:33:59.851644 139724881797504 train.py:288] Epoch 38 CV info cv_loss 62.0010322735847
I0626 02:33:59.851898 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/38.pt
I0626 02:34:00.118481 139724881797504 train.py:274] Epoch 39 TRAIN info lr 0.0007331999999999999
I0626 02:34:00.121690 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:34:00.121754 139724881797504 executor.py:46] total epoch is 39.
I0626 02:34:07.874037 139724881797504 executor.py:115] TRAIN Batch 39/0 loss 20.765564 loss_att 16.090069 loss_ctc 31.675047 lr 0.00073340 rank 0
I0626 02:34:18.873279 139724881797504 executor.py:115] TRAIN Batch 39/100 loss 40.202644 loss_att 33.095867 loss_ctc 56.785126 lr 0.00073460 rank 0
I0626 02:34:29.818104 139724881797504 executor.py:115] TRAIN Batch 39/200 loss 45.206497 loss_att 36.839828 loss_ctc 64.728722 lr 0.00073580 rank 0
I0626 02:34:40.920529 139724881797504 executor.py:115] TRAIN Batch 39/300 loss 35.897560 loss_att 27.279705 loss_ctc 56.005894 lr 0.00073700 rank 0
I0626 02:34:52.327704 139724881797504 executor.py:115] TRAIN Batch 39/400 loss 25.737713 loss_att 19.516508 loss_ctc 40.253860 lr 0.00073840 rank 0
I0626 02:35:03.792820 139724881797504 executor.py:115] TRAIN Batch 39/500 loss 18.047789 loss_att 15.221953 loss_ctc 24.641403 lr 0.00073960 rank 0
I0626 02:35:14.389889 139724881797504 executor.py:115] TRAIN Batch 39/600 loss 42.703743 loss_att 33.698376 loss_ctc 63.716263 lr 0.00074080 rank 0
I0626 02:35:25.885230 139724881797504 executor.py:115] TRAIN Batch 39/700 loss 39.514130 loss_att 30.456715 loss_ctc 60.648087 lr 0.00074200 rank 0
I0626 02:35:37.574021 139724881797504 executor.py:115] TRAIN Batch 39/800 loss 36.219910 loss_att 27.331467 loss_ctc 56.959610 lr 0.00074340 rank 0
I0626 02:35:49.158868 139724881797504 executor.py:115] TRAIN Batch 39/900 loss 29.208424 loss_att 21.383266 loss_ctc 47.467125 lr 0.00074460 rank 0
I0626 02:36:00.350281 139724881797504 executor.py:115] TRAIN Batch 39/1000 loss 17.274876 loss_att 12.854877 loss_ctc 27.588200 lr 0.00074580 rank 0
I0626 02:36:11.265773 139724881797504 executor.py:115] TRAIN Batch 39/1100 loss 46.881714 loss_att 37.105309 loss_ctc 69.693336 lr 0.00074700 rank 0
I0626 02:36:22.767168 139724881797504 executor.py:115] TRAIN Batch 39/1200 loss 33.452736 loss_att 28.047085 loss_ctc 46.065926 lr 0.00074840 rank 0
I0626 02:36:34.094514 139724881797504 executor.py:115] TRAIN Batch 39/1300 loss 26.904533 loss_att 20.150293 loss_ctc 42.664421 lr 0.00074960 rank 0
I0626 02:36:45.589545 139724881797504 executor.py:115] TRAIN Batch 39/1400 loss 27.794231 loss_att 21.174740 loss_ctc 43.239712 lr 0.00075080 rank 0
I0626 02:36:56.683628 139724881797504 executor.py:115] TRAIN Batch 39/1500 loss 36.166714 loss_att 31.649343 loss_ctc 46.707245 lr 0.00075200 rank 0
I0626 02:37:02.265036 139724881797504 executor.py:152] CV Batch 39/0 loss 25.865101 loss_att 26.801273 loss_ctc 23.680698 history loss 22.991201 rank 0
I0626 02:37:06.242163 139724881797504 executor.py:152] CV Batch 39/100 loss 72.177612 loss_att 76.673500 loss_ctc 61.687218 history loss 51.765456 rank 0
I0626 02:37:09.214758 139724881797504 train.py:288] Epoch 39 CV info cv_loss 59.58942959047412
I0626 02:37:09.214905 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/39.pt
I0626 02:37:09.473524 139724881797504 train.py:274] Epoch 40 TRAIN info lr 0.000752
I0626 02:37:09.476604 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:37:09.476675 139724881797504 executor.py:46] total epoch is 40.
I0626 02:37:17.258706 139724881797504 executor.py:115] TRAIN Batch 40/0 loss 19.824467 loss_att 14.203791 loss_ctc 32.939377 lr 0.00075220 rank 0
I0626 02:37:28.484211 139724881797504 executor.py:115] TRAIN Batch 40/100 loss 34.772278 loss_att 26.576591 loss_ctc 53.895546 lr 0.00075340 rank 0
I0626 02:37:39.310480 139724881797504 executor.py:115] TRAIN Batch 40/200 loss 43.926506 loss_att 34.467422 loss_ctc 65.997696 lr 0.00075460 rank 0
I0626 02:37:50.279636 139724881797504 executor.py:115] TRAIN Batch 40/300 loss 28.171352 loss_att 19.966099 loss_ctc 47.316940 lr 0.00075580 rank 0
I0626 02:38:01.634886 139724881797504 executor.py:115] TRAIN Batch 40/400 loss 28.439005 loss_att 23.983959 loss_ctc 38.834114 lr 0.00075720 rank 0
I0626 02:38:13.092509 139724881797504 executor.py:115] TRAIN Batch 40/500 loss 16.748562 loss_att 14.330273 loss_ctc 22.391235 lr 0.00075840 rank 0
I0626 02:38:23.560807 139724881797504 executor.py:115] TRAIN Batch 40/600 loss 46.603931 loss_att 38.124809 loss_ctc 66.388550 lr 0.00075960 rank 0
I0626 02:38:34.759771 139724881797504 executor.py:115] TRAIN Batch 40/700 loss 36.048157 loss_att 28.302034 loss_ctc 54.122437 lr 0.00076080 rank 0
I0626 02:38:46.469338 139724881797504 executor.py:115] TRAIN Batch 40/800 loss 31.401474 loss_att 24.464071 loss_ctc 47.588745 lr 0.00076220 rank 0
I0626 02:38:57.929337 139724881797504 executor.py:115] TRAIN Batch 40/900 loss 26.995216 loss_att 20.070852 loss_ctc 43.152065 lr 0.00076340 rank 0
I0626 02:39:09.041702 139724881797504 executor.py:115] TRAIN Batch 40/1000 loss 17.514185 loss_att 15.701448 loss_ctc 21.743904 lr 0.00076460 rank 0
I0626 02:39:19.690585 139724881797504 executor.py:115] TRAIN Batch 40/1100 loss 46.332481 loss_att 38.509758 loss_ctc 64.585503 lr 0.00076580 rank 0
I0626 02:39:31.234719 139724881797504 executor.py:115] TRAIN Batch 40/1200 loss 35.171093 loss_att 30.130623 loss_ctc 46.932190 lr 0.00076720 rank 0
I0626 02:39:42.735197 139724881797504 executor.py:115] TRAIN Batch 40/1300 loss 35.183273 loss_att 27.325941 loss_ctc 53.517040 lr 0.00076840 rank 0
I0626 02:39:54.389580 139724881797504 executor.py:115] TRAIN Batch 40/1400 loss 22.723652 loss_att 17.061893 loss_ctc 35.934425 lr 0.00076960 rank 0
I0626 02:40:05.765024 139724881797504 executor.py:115] TRAIN Batch 40/1500 loss 36.278622 loss_att 28.390881 loss_ctc 54.683350 lr 0.00077080 rank 0
I0626 02:40:11.363400 139724881797504 executor.py:152] CV Batch 40/0 loss 28.161789 loss_att 28.863344 loss_ctc 26.524830 history loss 25.032701 rank 0
I0626 02:40:15.320676 139724881797504 executor.py:152] CV Batch 40/100 loss 71.745155 loss_att 75.926857 loss_ctc 61.987846 history loss 52.160440 rank 0
I0626 02:40:18.310374 139724881797504 train.py:288] Epoch 40 CV info cv_loss 59.32350398274626
I0626 02:40:18.310524 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/40.pt
I0626 02:40:18.569950 139724881797504 train.py:274] Epoch 41 TRAIN info lr 0.0007707999999999999
I0626 02:40:18.573064 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:40:18.573142 139724881797504 executor.py:46] total epoch is 41.
I0626 02:40:26.411120 139724881797504 executor.py:115] TRAIN Batch 41/0 loss 14.980912 loss_att 10.982579 loss_ctc 24.310356 lr 0.00077100 rank 0
I0626 02:40:37.461274 139724881797504 executor.py:115] TRAIN Batch 41/100 loss 42.787846 loss_att 34.454735 loss_ctc 62.231766 lr 0.00077220 rank 0
I0626 02:40:48.627247 139724881797504 executor.py:115] TRAIN Batch 41/200 loss 33.188873 loss_att 27.049822 loss_ctc 47.513329 lr 0.00077340 rank 0
I0626 02:41:00.258829 139724881797504 executor.py:115] TRAIN Batch 41/300 loss 33.950172 loss_att 25.539490 loss_ctc 53.575104 lr 0.00077460 rank 0
I0626 02:41:12.219046 139724881797504 executor.py:115] TRAIN Batch 41/400 loss 21.451935 loss_att 16.004398 loss_ctc 34.162849 lr 0.00077600 rank 0
I0626 02:41:24.338480 139724881797504 executor.py:115] TRAIN Batch 41/500 loss 21.451363 loss_att 17.894411 loss_ctc 29.750916 lr 0.00077720 rank 0
I0626 02:41:34.817385 139724881797504 executor.py:115] TRAIN Batch 41/600 loss 48.703835 loss_att 35.339836 loss_ctc 79.886505 lr 0.00077840 rank 0
I0626 02:41:46.224675 139724881797504 executor.py:115] TRAIN Batch 41/700 loss 30.727285 loss_att 25.241226 loss_ctc 43.528091 lr 0.00077960 rank 0
I0626 02:41:58.107188 139724881797504 executor.py:115] TRAIN Batch 41/800 loss 29.248650 loss_att 23.728033 loss_ctc 42.130089 lr 0.00078100 rank 0
I0626 02:42:09.699057 139724881797504 executor.py:115] TRAIN Batch 41/900 loss 28.016756 loss_att 22.161184 loss_ctc 41.679756 lr 0.00078220 rank 0
I0626 02:42:21.081462 139724881797504 executor.py:115] TRAIN Batch 41/1000 loss 17.898235 loss_att 13.024892 loss_ctc 29.269365 lr 0.00078340 rank 0
I0626 02:42:31.627071 139724881797504 executor.py:115] TRAIN Batch 41/1100 loss 41.397884 loss_att 33.930519 loss_ctc 58.821732 lr 0.00078460 rank 0
I0626 02:42:42.725815 139724881797504 executor.py:115] TRAIN Batch 41/1200 loss 29.101952 loss_att 21.268589 loss_ctc 47.379795 lr 0.00078600 rank 0
I0626 02:42:53.991513 139724881797504 executor.py:115] TRAIN Batch 41/1300 loss 32.823154 loss_att 24.841923 loss_ctc 51.446030 lr 0.00078720 rank 0
I0626 02:43:05.652287 139724881797504 executor.py:115] TRAIN Batch 41/1400 loss 23.438797 loss_att 16.757730 loss_ctc 39.027950 lr 0.00078840 rank 0
I0626 02:43:16.904231 139724881797504 executor.py:115] TRAIN Batch 41/1500 loss 35.609924 loss_att 23.472128 loss_ctc 63.931458 lr 0.00078960 rank 0
I0626 02:43:22.473700 139724881797504 executor.py:152] CV Batch 41/0 loss 24.999077 loss_att 26.076408 loss_ctc 22.485310 history loss 22.221402 rank 0
I0626 02:43:26.579652 139724881797504 executor.py:152] CV Batch 41/100 loss 68.574448 loss_att 72.567566 loss_ctc 59.257187 history loss 49.814436 rank 0
I0626 02:43:29.558542 139724881797504 train.py:288] Epoch 41 CV info cv_loss 57.018970178043766
I0626 02:43:29.558784 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/41.pt
I0626 02:43:29.824589 139724881797504 train.py:274] Epoch 42 TRAIN info lr 0.0007896
I0626 02:43:29.827467 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:43:29.827534 139724881797504 executor.py:46] total epoch is 42.
I0626 02:43:37.717853 139724881797504 executor.py:115] TRAIN Batch 42/0 loss 15.891219 loss_att 11.944199 loss_ctc 25.100933 lr 0.00078980 rank 0
I0626 02:43:48.837126 139724881797504 executor.py:115] TRAIN Batch 42/100 loss 37.235199 loss_att 29.686636 loss_ctc 54.848503 lr 0.00079100 rank 0
I0626 02:43:59.852370 139724881797504 executor.py:115] TRAIN Batch 42/200 loss 26.852043 loss_att 19.395279 loss_ctc 44.251160 lr 0.00079220 rank 0
I0626 02:44:11.037938 139724881797504 executor.py:115] TRAIN Batch 42/300 loss 27.577137 loss_att 21.538864 loss_ctc 41.666439 lr 0.00079340 rank 0
I0626 02:44:22.693593 139724881797504 executor.py:115] TRAIN Batch 42/400 loss 18.953564 loss_att 13.651478 loss_ctc 31.325098 lr 0.00079480 rank 0
I0626 02:44:34.481024 139724881797504 executor.py:115] TRAIN Batch 42/500 loss 18.484154 loss_att 14.010069 loss_ctc 28.923689 lr 0.00079600 rank 0
I0626 02:44:45.441844 139724881797504 executor.py:115] TRAIN Batch 42/600 loss 43.786140 loss_att 33.549995 loss_ctc 67.670471 lr 0.00079720 rank 0
I0626 02:44:57.163973 139724881797504 executor.py:115] TRAIN Batch 42/700 loss 33.368572 loss_att 25.803862 loss_ctc 51.019562 lr 0.00079840 rank 0
I0626 02:45:08.941711 139724881797504 executor.py:115] TRAIN Batch 42/800 loss 31.646282 loss_att 23.610371 loss_ctc 50.396736 lr 0.00079980 rank 0
I0626 02:45:20.355118 139724881797504 executor.py:115] TRAIN Batch 42/900 loss 22.354977 loss_att 16.961414 loss_ctc 34.939953 lr 0.00080100 rank 0
I0626 02:45:31.722592 139724881797504 executor.py:115] TRAIN Batch 42/1000 loss 18.703686 loss_att 13.926886 loss_ctc 29.849552 lr 0.00080220 rank 0
I0626 02:45:42.688629 139724881797504 executor.py:115] TRAIN Batch 42/1100 loss 42.328835 loss_att 33.823303 loss_ctc 62.175072 lr 0.00080340 rank 0
I0626 02:45:53.965630 139724881797504 executor.py:115] TRAIN Batch 42/1200 loss 34.216812 loss_att 28.705345 loss_ctc 47.076897 lr 0.00080480 rank 0
I0626 02:46:05.498345 139724881797504 executor.py:115] TRAIN Batch 42/1300 loss 25.551073 loss_att 19.989830 loss_ctc 38.527306 lr 0.00080600 rank 0
I0626 02:46:17.180485 139724881797504 executor.py:115] TRAIN Batch 42/1400 loss 24.092670 loss_att 17.004717 loss_ctc 40.631226 lr 0.00080720 rank 0
I0626 02:46:28.322016 139724881797504 executor.py:115] TRAIN Batch 42/1500 loss 29.156124 loss_att 23.488577 loss_ctc 42.380398 lr 0.00080840 rank 0
I0626 02:46:33.921522 139724881797504 executor.py:152] CV Batch 42/0 loss 24.056786 loss_att 24.504791 loss_ctc 23.011440 history loss 21.383809 rank 0
I0626 02:46:37.916735 139724881797504 executor.py:152] CV Batch 42/100 loss 65.430275 loss_att 69.358841 loss_ctc 56.263618 history loss 47.900371 rank 0
I0626 02:46:40.891190 139724881797504 train.py:288] Epoch 42 CV info cv_loss 54.698798915013846
I0626 02:46:40.891453 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/42.pt
I0626 02:46:41.152287 139724881797504 train.py:274] Epoch 43 TRAIN info lr 0.0008083999999999999
I0626 02:46:41.155339 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:46:41.155402 139724881797504 executor.py:46] total epoch is 43.
I0626 02:46:48.970572 139724881797504 executor.py:115] TRAIN Batch 43/0 loss 17.822163 loss_att 13.695799 loss_ctc 27.450346 lr 0.00080860 rank 0
I0626 02:46:59.821376 139724881797504 executor.py:115] TRAIN Batch 43/100 loss 39.986996 loss_att 32.024902 loss_ctc 58.565216 lr 0.00080980 rank 0
I0626 02:47:11.113957 139724881797504 executor.py:115] TRAIN Batch 43/200 loss 28.897057 loss_att 21.346039 loss_ctc 46.516094 lr 0.00081100 rank 0
I0626 02:47:22.587666 139724881797504 executor.py:115] TRAIN Batch 43/300 loss 22.282768 loss_att 16.414146 loss_ctc 35.976219 lr 0.00081220 rank 0
I0626 02:47:33.972696 139724881797504 executor.py:115] TRAIN Batch 43/400 loss 22.976053 loss_att 15.931870 loss_ctc 39.412479 lr 0.00081360 rank 0
I0626 02:47:45.327302 139724881797504 executor.py:115] TRAIN Batch 43/500 loss 16.688339 loss_att 13.957964 loss_ctc 23.059212 lr 0.00081480 rank 0
I0626 02:47:55.620672 139724881797504 executor.py:115] TRAIN Batch 43/600 loss 35.657166 loss_att 27.944750 loss_ctc 53.652805 lr 0.00081600 rank 0
I0626 02:48:06.697323 139724881797504 executor.py:115] TRAIN Batch 43/700 loss 33.672794 loss_att 25.652386 loss_ctc 52.387085 lr 0.00081720 rank 0
I0626 02:48:18.265597 139724881797504 executor.py:115] TRAIN Batch 43/800 loss 27.054394 loss_att 20.389290 loss_ctc 42.606297 lr 0.00081860 rank 0
I0626 02:48:29.650715 139724881797504 executor.py:115] TRAIN Batch 43/900 loss 20.413094 loss_att 14.497320 loss_ctc 34.216568 lr 0.00081980 rank 0
I0626 02:48:40.924840 139724881797504 executor.py:115] TRAIN Batch 43/1000 loss 16.680319 loss_att 12.532383 loss_ctc 26.358837 lr 0.00082100 rank 0
I0626 02:48:51.527876 139724881797504 executor.py:115] TRAIN Batch 43/1100 loss 37.350861 loss_att 29.920055 loss_ctc 54.689407 lr 0.00082220 rank 0
I0626 02:49:02.916478 139724881797504 executor.py:115] TRAIN Batch 43/1200 loss 35.848789 loss_att 26.612196 loss_ctc 57.400841 lr 0.00082360 rank 0
I0626 02:49:14.118353 139724881797504 executor.py:115] TRAIN Batch 43/1300 loss 24.034134 loss_att 17.449303 loss_ctc 39.398743 lr 0.00082480 rank 0
I0626 02:49:25.410565 139724881797504 executor.py:115] TRAIN Batch 43/1400 loss 21.295383 loss_att 15.556979 loss_ctc 34.684998 lr 0.00082600 rank 0
I0626 02:49:36.509884 139724881797504 executor.py:115] TRAIN Batch 43/1500 loss 33.627052 loss_att 26.002777 loss_ctc 51.417023 lr 0.00082720 rank 0
I0626 02:49:42.066696 139724881797504 executor.py:152] CV Batch 43/0 loss 22.805801 loss_att 23.726723 loss_ctc 20.656982 history loss 20.271823 rank 0
I0626 02:49:46.188851 139724881797504 executor.py:152] CV Batch 43/100 loss 64.790237 loss_att 69.200554 loss_ctc 54.499485 history loss 47.135939 rank 0
I0626 02:49:49.172436 139724881797504 train.py:288] Epoch 43 CV info cv_loss 53.89443766347029
I0626 02:49:49.172793 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/43.pt
I0626 02:49:49.435199 139724881797504 train.py:274] Epoch 44 TRAIN info lr 0.0008271999999999999
I0626 02:49:49.438324 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:49:49.438385 139724881797504 executor.py:46] total epoch is 44.
I0626 02:49:57.346560 139724881797504 executor.py:115] TRAIN Batch 44/0 loss 15.124714 loss_att 11.282477 loss_ctc 24.089930 lr 0.00082740 rank 0
I0626 02:50:08.269459 139724881797504 executor.py:115] TRAIN Batch 44/100 loss 33.656620 loss_att 27.738178 loss_ctc 47.466316 lr 0.00082860 rank 0
I0626 02:50:19.633758 139724881797504 executor.py:115] TRAIN Batch 44/200 loss 25.162365 loss_att 19.472763 loss_ctc 38.438099 lr 0.00082980 rank 0
I0626 02:50:30.961333 139724881797504 executor.py:115] TRAIN Batch 44/300 loss 25.253735 loss_att 18.050632 loss_ctc 42.060974 lr 0.00083100 rank 0
I0626 02:50:42.418625 139724881797504 executor.py:115] TRAIN Batch 44/400 loss 22.797947 loss_att 18.020067 loss_ctc 33.946331 lr 0.00083240 rank 0
I0626 02:50:53.992752 139724881797504 executor.py:115] TRAIN Batch 44/500 loss 14.431646 loss_att 10.936433 loss_ctc 22.587143 lr 0.00083360 rank 0
I0626 02:51:04.469779 139724881797504 executor.py:115] TRAIN Batch 44/600 loss 35.988960 loss_att 27.692677 loss_ctc 55.346958 lr 0.00083480 rank 0
I0626 02:51:15.668006 139724881797504 executor.py:115] TRAIN Batch 44/700 loss 31.869219 loss_att 22.104893 loss_ctc 54.652641 lr 0.00083600 rank 0
I0626 02:51:27.440947 139724881797504 executor.py:115] TRAIN Batch 44/800 loss 27.301081 loss_att 20.531204 loss_ctc 43.097458 lr 0.00083740 rank 0
I0626 02:51:38.842071 139724881797504 executor.py:115] TRAIN Batch 44/900 loss 25.607914 loss_att 16.723232 loss_ctc 46.338837 lr 0.00083860 rank 0
I0626 02:51:50.346955 139724881797504 executor.py:115] TRAIN Batch 44/1000 loss 15.471794 loss_att 11.045557 loss_ctc 25.799683 lr 0.00083980 rank 0
I0626 02:52:01.479413 139724881797504 executor.py:115] TRAIN Batch 44/1100 loss 35.539974 loss_att 28.364773 loss_ctc 52.282108 lr 0.00084100 rank 0
I0626 02:52:12.920685 139724881797504 executor.py:115] TRAIN Batch 44/1200 loss 29.413414 loss_att 22.147333 loss_ctc 46.367599 lr 0.00084240 rank 0
I0626 02:52:24.614267 139724881797504 executor.py:115] TRAIN Batch 44/1300 loss 27.832466 loss_att 20.530163 loss_ctc 44.871170 lr 0.00084360 rank 0
I0626 02:52:36.280772 139724881797504 executor.py:115] TRAIN Batch 44/1400 loss 20.268415 loss_att 15.430799 loss_ctc 31.556187 lr 0.00084480 rank 0
I0626 02:52:47.214616 139724881797504 executor.py:115] TRAIN Batch 44/1500 loss 23.708740 loss_att 17.851635 loss_ctc 37.375317 lr 0.00084600 rank 0
I0626 02:52:52.809354 139724881797504 executor.py:152] CV Batch 44/0 loss 21.986799 loss_att 23.524185 loss_ctc 18.399570 history loss 19.543822 rank 0
I0626 02:52:56.844772 139724881797504 executor.py:152] CV Batch 44/100 loss 62.168953 loss_att 67.737991 loss_ctc 49.174534 history loss 44.958598 rank 0
I0626 02:52:59.813907 139724881797504 train.py:288] Epoch 44 CV info cv_loss 51.82586924412495
I0626 02:52:59.814083 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/44.pt
I0626 02:53:00.075370 139724881797504 train.py:274] Epoch 45 TRAIN info lr 0.000846
I0626 02:53:00.078329 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:53:00.078390 139724881797504 executor.py:46] total epoch is 45.
I0626 02:53:07.972612 139724881797504 executor.py:115] TRAIN Batch 45/0 loss 15.080608 loss_att 10.836760 loss_ctc 24.982922 lr 0.00084620 rank 0
I0626 02:53:19.005458 139724881797504 executor.py:115] TRAIN Batch 45/100 loss 34.479610 loss_att 27.364275 loss_ctc 51.082054 lr 0.00084740 rank 0
I0626 02:53:30.123458 139724881797504 executor.py:115] TRAIN Batch 45/200 loss 29.017094 loss_att 21.007481 loss_ctc 47.706192 lr 0.00084860 rank 0
I0626 02:53:41.485473 139724881797504 executor.py:115] TRAIN Batch 45/300 loss 22.113314 loss_att 14.847645 loss_ctc 39.066540 lr 0.00084980 rank 0
I0626 02:53:52.920927 139724881797504 executor.py:115] TRAIN Batch 45/400 loss 19.115211 loss_att 12.029711 loss_ctc 35.648048 lr 0.00085120 rank 0
I0626 02:54:04.121475 139724881797504 executor.py:115] TRAIN Batch 45/500 loss 13.475302 loss_att 9.821712 loss_ctc 22.000343 lr 0.00085240 rank 0
I0626 02:54:14.526570 139724881797504 executor.py:115] TRAIN Batch 45/600 loss 33.110054 loss_att 27.352688 loss_ctc 46.543907 lr 0.00085360 rank 0
I0626 02:54:25.586192 139724881797504 executor.py:115] TRAIN Batch 45/700 loss 31.066143 loss_att 23.445208 loss_ctc 48.848324 lr 0.00085480 rank 0
I0626 02:54:37.101727 139724881797504 executor.py:115] TRAIN Batch 45/800 loss 25.758617 loss_att 16.000725 loss_ctc 48.527035 lr 0.00085620 rank 0
I0626 02:54:48.696732 139724881797504 executor.py:115] TRAIN Batch 45/900 loss 21.533833 loss_att 14.315319 loss_ctc 38.377026 lr 0.00085740 rank 0
I0626 02:54:59.971096 139724881797504 executor.py:115] TRAIN Batch 45/1000 loss 13.673502 loss_att 10.034182 loss_ctc 22.165249 lr 0.00085860 rank 0
I0626 02:55:10.788919 139724881797504 executor.py:115] TRAIN Batch 45/1100 loss 34.040405 loss_att 30.339718 loss_ctc 42.675346 lr 0.00085980 rank 0
I0626 02:55:22.204723 139724881797504 executor.py:115] TRAIN Batch 45/1200 loss 25.755844 loss_att 21.518827 loss_ctc 35.642212 lr 0.00086120 rank 0
I0626 02:55:33.548644 139724881797504 executor.py:115] TRAIN Batch 45/1300 loss 26.145966 loss_att 19.030424 loss_ctc 42.748890 lr 0.00086240 rank 0
I0626 02:55:44.855654 139724881797504 executor.py:115] TRAIN Batch 45/1400 loss 16.279713 loss_att 11.031734 loss_ctc 28.524992 lr 0.00086360 rank 0
I0626 02:55:55.893002 139724881797504 executor.py:115] TRAIN Batch 45/1500 loss 28.391529 loss_att 21.912657 loss_ctc 43.508892 lr 0.00086480 rank 0
I0626 02:56:01.462431 139724881797504 executor.py:152] CV Batch 45/0 loss 25.022181 loss_att 27.220289 loss_ctc 19.893257 history loss 22.241938 rank 0
I0626 02:56:05.469393 139724881797504 executor.py:152] CV Batch 45/100 loss 63.270294 loss_att 67.919586 loss_ctc 52.421951 history loss 46.277836 rank 0
I0626 02:56:08.468728 139724881797504 train.py:288] Epoch 45 CV info cv_loss 52.41005605097343
I0626 02:56:08.468889 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/45.pt
I0626 02:56:08.726954 139724881797504 train.py:274] Epoch 46 TRAIN info lr 0.0008647999999999999
I0626 02:56:08.730017 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:56:08.730077 139724881797504 executor.py:46] total epoch is 46.
I0626 02:56:16.562961 139724881797504 executor.py:115] TRAIN Batch 46/0 loss 15.618304 loss_att 12.862801 loss_ctc 22.047812 lr 0.00086500 rank 0
I0626 02:56:28.030909 139724881797504 executor.py:115] TRAIN Batch 46/100 loss 30.114155 loss_att 24.940386 loss_ctc 42.186283 lr 0.00086620 rank 0
I0626 02:56:39.040472 139724881797504 executor.py:115] TRAIN Batch 46/200 loss 27.118956 loss_att 19.326933 loss_ctc 45.300343 lr 0.00086740 rank 0
I0626 02:56:50.625846 139724881797504 executor.py:115] TRAIN Batch 46/300 loss 19.769669 loss_att 15.018293 loss_ctc 30.856213 lr 0.00086860 rank 0
I0626 02:57:02.354605 139724881797504 executor.py:115] TRAIN Batch 46/400 loss 19.018692 loss_att 12.317707 loss_ctc 34.654327 lr 0.00087000 rank 0
I0626 02:57:13.856571 139724881797504 executor.py:115] TRAIN Batch 46/500 loss 16.419891 loss_att 10.787523 loss_ctc 29.562084 lr 0.00087120 rank 0
I0626 02:57:24.686498 139724881797504 executor.py:115] TRAIN Batch 46/600 loss 31.634834 loss_att 23.411076 loss_ctc 50.823601 lr 0.00087240 rank 0
I0626 02:57:35.920538 139724881797504 executor.py:115] TRAIN Batch 46/700 loss 30.007341 loss_att 20.906048 loss_ctc 51.243690 lr 0.00087360 rank 0
I0626 02:57:47.704474 139724881797504 executor.py:115] TRAIN Batch 46/800 loss 24.604193 loss_att 17.406635 loss_ctc 41.398491 lr 0.00087500 rank 0
I0626 02:57:59.418976 139724881797504 executor.py:115] TRAIN Batch 46/900 loss 19.043503 loss_att 14.517427 loss_ctc 29.604343 lr 0.00087620 rank 0
I0626 02:58:10.524019 139724881797504 executor.py:115] TRAIN Batch 46/1000 loss 13.693117 loss_att 10.917971 loss_ctc 20.168459 lr 0.00087740 rank 0
I0626 02:58:21.312892 139724881797504 executor.py:115] TRAIN Batch 46/1100 loss 31.895943 loss_att 23.538189 loss_ctc 51.397362 lr 0.00087860 rank 0
I0626 02:58:32.720285 139724881797504 executor.py:115] TRAIN Batch 46/1200 loss 25.744057 loss_att 19.256355 loss_ctc 40.882030 lr 0.00088000 rank 0
I0626 02:58:43.946325 139724881797504 executor.py:115] TRAIN Batch 46/1300 loss 23.580349 loss_att 17.466328 loss_ctc 37.846397 lr 0.00088120 rank 0
I0626 02:58:54.931196 139724881797504 executor.py:115] TRAIN Batch 46/1400 loss 20.700394 loss_att 15.156142 loss_ctc 33.636978 lr 0.00088240 rank 0
I0626 02:59:06.143355 139724881797504 executor.py:115] TRAIN Batch 46/1500 loss 26.286345 loss_att 20.695498 loss_ctc 39.331654 lr 0.00088360 rank 0
I0626 02:59:11.758421 139724881797504 executor.py:152] CV Batch 46/0 loss 21.365635 loss_att 21.927523 loss_ctc 20.054560 history loss 18.991675 rank 0
I0626 02:59:15.789924 139724881797504 executor.py:152] CV Batch 46/100 loss 61.899323 loss_att 66.401291 loss_ctc 51.394730 history loss 43.641391 rank 0
I0626 02:59:18.818098 139724881797504 train.py:288] Epoch 46 CV info cv_loss 50.36242180642596
I0626 02:59:18.818309 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/46.pt
I0626 02:59:19.076890 139724881797504 train.py:274] Epoch 47 TRAIN info lr 0.0008835999999999999
I0626 02:59:19.079664 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 02:59:19.079723 139724881797504 executor.py:46] total epoch is 47.
I0626 02:59:26.839933 139724881797504 executor.py:115] TRAIN Batch 47/0 loss 14.088631 loss_att 9.556481 loss_ctc 24.663647 lr 0.00088380 rank 0
I0626 02:59:38.075467 139724881797504 executor.py:115] TRAIN Batch 47/100 loss 33.788490 loss_att 25.483345 loss_ctc 53.167164 lr 0.00088500 rank 0
I0626 02:59:49.315217 139724881797504 executor.py:115] TRAIN Batch 47/200 loss 27.640320 loss_att 21.013962 loss_ctc 43.101822 lr 0.00088620 rank 0
I0626 03:00:00.679629 139724881797504 executor.py:115] TRAIN Batch 47/300 loss 28.016710 loss_att 20.334099 loss_ctc 45.942802 lr 0.00088740 rank 0
I0626 03:00:12.403106 139724881797504 executor.py:115] TRAIN Batch 47/400 loss 20.115026 loss_att 14.302279 loss_ctc 33.678104 lr 0.00088880 rank 0
I0626 03:00:23.885776 139724881797504 executor.py:115] TRAIN Batch 47/500 loss 12.069005 loss_att 8.335888 loss_ctc 20.779610 lr 0.00089000 rank 0
I0626 03:00:34.515843 139724881797504 executor.py:115] TRAIN Batch 47/600 loss 31.825939 loss_att 24.490280 loss_ctc 48.942474 lr 0.00089120 rank 0
I0626 03:00:46.016125 139724881797504 executor.py:115] TRAIN Batch 47/700 loss 25.883980 loss_att 18.772943 loss_ctc 42.476395 lr 0.00089240 rank 0
I0626 03:00:57.661827 139724881797504 executor.py:115] TRAIN Batch 47/800 loss 24.834778 loss_att 17.330616 loss_ctc 42.344490 lr 0.00089380 rank 0
I0626 03:01:09.270586 139724881797504 executor.py:115] TRAIN Batch 47/900 loss 20.745199 loss_att 15.405687 loss_ctc 33.204060 lr 0.00089500 rank 0
I0626 03:01:20.338128 139724881797504 executor.py:115] TRAIN Batch 47/1000 loss 14.865425 loss_att 11.396748 loss_ctc 22.959007 lr 0.00089620 rank 0
I0626 03:01:31.192717 139724881797504 executor.py:115] TRAIN Batch 47/1100 loss 28.757349 loss_att 21.623510 loss_ctc 45.402969 lr 0.00089740 rank 0
I0626 03:01:42.629970 139724881797504 executor.py:115] TRAIN Batch 47/1200 loss 23.134678 loss_att 15.701052 loss_ctc 40.479805 lr 0.00089880 rank 0
I0626 03:01:53.851200 139724881797504 executor.py:115] TRAIN Batch 47/1300 loss 23.392624 loss_att 17.439445 loss_ctc 37.283367 lr 0.00090000 rank 0
I0626 03:02:04.975455 139724881797504 executor.py:115] TRAIN Batch 47/1400 loss 19.422184 loss_att 13.448106 loss_ctc 33.361702 lr 0.00090120 rank 0
I0626 03:02:16.095341 139724881797504 executor.py:115] TRAIN Batch 47/1500 loss 23.067646 loss_att 17.660139 loss_ctc 35.685162 lr 0.00090240 rank 0
I0626 03:02:21.673590 139724881797504 executor.py:152] CV Batch 47/0 loss 21.763760 loss_att 23.337852 loss_ctc 18.090878 history loss 19.345564 rank 0
I0626 03:02:25.814301 139724881797504 executor.py:152] CV Batch 47/100 loss 61.731159 loss_att 66.576416 loss_ctc 50.425568 history loss 43.581327 rank 0
I0626 03:02:28.796442 139724881797504 train.py:288] Epoch 47 CV info cv_loss 49.901419970345344
I0626 03:02:28.796602 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/47.pt
I0626 03:02:29.055792 139724881797504 train.py:274] Epoch 48 TRAIN info lr 0.0009023999999999999
I0626 03:02:29.058725 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:02:29.058784 139724881797504 executor.py:46] total epoch is 48.
I0626 03:02:36.951799 139724881797504 executor.py:115] TRAIN Batch 48/0 loss 14.496958 loss_att 11.327536 loss_ctc 21.892277 lr 0.00090260 rank 0
I0626 03:02:47.554535 139724881797504 executor.py:115] TRAIN Batch 48/100 loss 27.324146 loss_att 21.071953 loss_ctc 41.912594 lr 0.00090380 rank 0
I0626 03:02:58.798702 139724881797504 executor.py:115] TRAIN Batch 48/200 loss 22.030056 loss_att 14.396519 loss_ctc 39.841637 lr 0.00090500 rank 0
I0626 03:03:09.938164 139724881797504 executor.py:115] TRAIN Batch 48/300 loss 21.151756 loss_att 15.147356 loss_ctc 35.162025 lr 0.00090620 rank 0
I0626 03:03:21.351919 139724881797504 executor.py:115] TRAIN Batch 48/400 loss 17.514301 loss_att 11.006033 loss_ctc 32.700260 lr 0.00090760 rank 0
I0626 03:03:32.890942 139724881797504 executor.py:115] TRAIN Batch 48/500 loss 12.738358 loss_att 8.803547 loss_ctc 21.919584 lr 0.00090880 rank 0
I0626 03:03:43.291841 139724881797504 executor.py:115] TRAIN Batch 48/600 loss 33.142464 loss_att 25.635948 loss_ctc 50.657669 lr 0.00091000 rank 0
I0626 03:03:54.312492 139724881797504 executor.py:115] TRAIN Batch 48/700 loss 24.032234 loss_att 17.903341 loss_ctc 38.332981 lr 0.00091120 rank 0
I0626 03:04:05.735440 139724881797504 executor.py:115] TRAIN Batch 48/800 loss 18.652061 loss_att 13.335148 loss_ctc 31.058193 lr 0.00091260 rank 0
I0626 03:04:16.983493 139724881797504 executor.py:115] TRAIN Batch 48/900 loss 17.850018 loss_att 11.708742 loss_ctc 32.179661 lr 0.00091380 rank 0
I0626 03:04:27.996623 139724881797504 executor.py:115] TRAIN Batch 48/1000 loss 15.285053 loss_att 10.356002 loss_ctc 26.786171 lr 0.00091500 rank 0
I0626 03:04:38.866758 139724881797504 executor.py:115] TRAIN Batch 48/1100 loss 29.977163 loss_att 22.754501 loss_ctc 46.830040 lr 0.00091620 rank 0
I0626 03:04:50.187955 139724881797504 executor.py:115] TRAIN Batch 48/1200 loss 23.144646 loss_att 14.842908 loss_ctc 42.515366 lr 0.00091760 rank 0
I0626 03:05:01.887218 139724881797504 executor.py:115] TRAIN Batch 48/1300 loss 18.884617 loss_att 13.903704 loss_ctc 30.506748 lr 0.00091880 rank 0
I0626 03:05:13.352792 139724881797504 executor.py:115] TRAIN Batch 48/1400 loss 18.199299 loss_att 13.450279 loss_ctc 29.280342 lr 0.00092000 rank 0
I0626 03:05:24.201834 139724881797504 executor.py:115] TRAIN Batch 48/1500 loss 18.891220 loss_att 12.360104 loss_ctc 34.130493 lr 0.00092120 rank 0
I0626 03:05:29.802965 139724881797504 executor.py:152] CV Batch 48/0 loss 19.569977 loss_att 20.685339 loss_ctc 16.967461 history loss 17.395535 rank 0
I0626 03:05:33.722712 139724881797504 executor.py:152] CV Batch 48/100 loss 59.349373 loss_att 64.349060 loss_ctc 47.683437 history loss 42.215296 rank 0
I0626 03:05:36.717091 139724881797504 train.py:288] Epoch 48 CV info cv_loss 47.983266283395984
I0626 03:05:36.717333 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/48.pt
I0626 03:05:36.976227 139724881797504 train.py:274] Epoch 49 TRAIN info lr 0.0009212
I0626 03:05:36.979144 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:05:36.979202 139724881797504 executor.py:46] total epoch is 49.
I0626 03:05:44.722263 139724881797504 executor.py:115] TRAIN Batch 49/0 loss 14.492615 loss_att 9.843102 loss_ctc 25.341476 lr 0.00092140 rank 0
I0626 03:05:55.579133 139724881797504 executor.py:115] TRAIN Batch 49/100 loss 21.256008 loss_att 15.604898 loss_ctc 34.441929 lr 0.00092260 rank 0
I0626 03:06:06.774498 139724881797504 executor.py:115] TRAIN Batch 49/200 loss 23.025394 loss_att 16.361118 loss_ctc 38.575371 lr 0.00092380 rank 0
I0626 03:06:18.037390 139724881797504 executor.py:115] TRAIN Batch 49/300 loss 21.659584 loss_att 15.665578 loss_ctc 35.645596 lr 0.00092500 rank 0
I0626 03:06:29.678204 139724881797504 executor.py:115] TRAIN Batch 49/400 loss 22.639416 loss_att 14.523807 loss_ctc 41.575836 lr 0.00092640 rank 0
I0626 03:06:41.238595 139724881797504 executor.py:115] TRAIN Batch 49/500 loss 12.233230 loss_att 9.365734 loss_ctc 18.924053 lr 0.00092760 rank 0
I0626 03:06:51.757750 139724881797504 executor.py:115] TRAIN Batch 49/600 loss 34.002312 loss_att 25.067539 loss_ctc 54.850121 lr 0.00092880 rank 0
I0626 03:07:03.077914 139724881797504 executor.py:115] TRAIN Batch 49/700 loss 22.809738 loss_att 17.714668 loss_ctc 34.698235 lr 0.00093000 rank 0
I0626 03:07:14.942815 139724881797504 executor.py:115] TRAIN Batch 49/800 loss 20.565266 loss_att 13.759994 loss_ctc 36.444237 lr 0.00093140 rank 0
I0626 03:07:26.333605 139724881797504 executor.py:115] TRAIN Batch 49/900 loss 13.492132 loss_att 9.568354 loss_ctc 22.647617 lr 0.00093260 rank 0
I0626 03:07:37.439601 139724881797504 executor.py:115] TRAIN Batch 49/1000 loss 12.476419 loss_att 8.967979 loss_ctc 20.662777 lr 0.00093380 rank 0
I0626 03:07:48.256963 139724881797504 executor.py:115] TRAIN Batch 49/1100 loss 33.450310 loss_att 24.575352 loss_ctc 54.158539 lr 0.00093500 rank 0
I0626 03:07:59.465638 139724881797504 executor.py:115] TRAIN Batch 49/1200 loss 25.941553 loss_att 20.278076 loss_ctc 39.156330 lr 0.00093640 rank 0
I0626 03:08:10.784438 139724881797504 executor.py:115] TRAIN Batch 49/1300 loss 25.447571 loss_att 17.979399 loss_ctc 42.873306 lr 0.00093760 rank 0
I0626 03:08:21.964244 139724881797504 executor.py:115] TRAIN Batch 49/1400 loss 20.038242 loss_att 14.244167 loss_ctc 33.557755 lr 0.00093880 rank 0
I0626 03:08:33.226700 139724881797504 executor.py:115] TRAIN Batch 49/1500 loss 25.332262 loss_att 16.885670 loss_ctc 45.040977 lr 0.00094000 rank 0
I0626 03:08:38.792874 139724881797504 executor.py:152] CV Batch 49/0 loss 20.523258 loss_att 21.190323 loss_ctc 18.966772 history loss 18.242896 rank 0
I0626 03:08:42.726177 139724881797504 executor.py:152] CV Batch 49/100 loss 60.590603 loss_att 65.633934 loss_ctc 48.822838 history loss 43.100126 rank 0
I0626 03:08:45.714195 139724881797504 train.py:288] Epoch 49 CV info cv_loss 48.78798893824043
I0626 03:08:45.714470 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/49.pt
I0626 03:08:45.979284 139724881797504 train.py:274] Epoch 50 TRAIN info lr 0.00094
I0626 03:08:45.982337 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:08:45.982398 139724881797504 executor.py:46] total epoch is 50.
I0626 03:08:53.871507 139724881797504 executor.py:115] TRAIN Batch 50/0 loss 14.297871 loss_att 10.113150 loss_ctc 24.062218 lr 0.00094020 rank 0
I0626 03:09:04.685184 139724881797504 executor.py:115] TRAIN Batch 50/100 loss 32.067509 loss_att 22.341814 loss_ctc 54.760799 lr 0.00094140 rank 0
I0626 03:09:15.783276 139724881797504 executor.py:115] TRAIN Batch 50/200 loss 19.106030 loss_att 12.950542 loss_ctc 33.468834 lr 0.00094260 rank 0
I0626 03:09:27.020005 139724881797504 executor.py:115] TRAIN Batch 50/300 loss 20.402905 loss_att 11.710636 loss_ctc 40.684860 lr 0.00094380 rank 0
I0626 03:09:38.726853 139724881797504 executor.py:115] TRAIN Batch 50/400 loss 17.199352 loss_att 12.500463 loss_ctc 28.163422 lr 0.00094520 rank 0
I0626 03:09:50.347678 139724881797504 executor.py:115] TRAIN Batch 50/500 loss 14.379975 loss_att 11.888629 loss_ctc 20.193115 lr 0.00094640 rank 0
I0626 03:10:01.253463 139724881797504 executor.py:115] TRAIN Batch 50/600 loss 31.564381 loss_att 23.905920 loss_ctc 49.434124 lr 0.00094760 rank 0
I0626 03:10:13.078203 139724881797504 executor.py:115] TRAIN Batch 50/700 loss 28.748535 loss_att 20.181173 loss_ctc 48.739040 lr 0.00094880 rank 0
I0626 03:10:24.563797 139724881797504 executor.py:115] TRAIN Batch 50/800 loss 19.652626 loss_att 14.122656 loss_ctc 32.555893 lr 0.00095020 rank 0
I0626 03:10:36.043831 139724881797504 executor.py:115] TRAIN Batch 50/900 loss 17.670444 loss_att 12.325912 loss_ctc 30.141024 lr 0.00095140 rank 0
I0626 03:10:47.323551 139724881797504 executor.py:115] TRAIN Batch 50/1000 loss 11.970804 loss_att 8.142060 loss_ctc 20.904539 lr 0.00095260 rank 0
I0626 03:10:58.298768 139724881797504 executor.py:115] TRAIN Batch 50/1100 loss 23.693312 loss_att 18.500284 loss_ctc 35.810375 lr 0.00095380 rank 0
I0626 03:11:09.876155 139724881797504 executor.py:115] TRAIN Batch 50/1200 loss 28.872240 loss_att 20.351286 loss_ctc 48.754463 lr 0.00095520 rank 0
I0626 03:11:21.204387 139724881797504 executor.py:115] TRAIN Batch 50/1300 loss 20.299479 loss_att 13.576318 loss_ctc 35.986851 lr 0.00095640 rank 0
I0626 03:11:32.123475 139724881797504 executor.py:115] TRAIN Batch 50/1400 loss 22.233410 loss_att 15.392382 loss_ctc 38.195805 lr 0.00095760 rank 0
I0626 03:11:43.087911 139724881797504 executor.py:115] TRAIN Batch 50/1500 loss 26.120817 loss_att 18.957220 loss_ctc 42.835876 lr 0.00095880 rank 0
I0626 03:11:48.650702 139724881797504 executor.py:152] CV Batch 50/0 loss 20.665863 loss_att 21.976305 loss_ctc 17.608168 history loss 18.369656 rank 0
I0626 03:11:52.719233 139724881797504 executor.py:152] CV Batch 50/100 loss 57.478001 loss_att 63.542953 loss_ctc 43.326439 history loss 39.369139 rank 0
I0626 03:11:55.714557 139724881797504 train.py:288] Epoch 50 CV info cv_loss 45.15792551726123
I0626 03:11:55.714703 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/50.pt
I0626 03:11:56.005877 139724881797504 train.py:274] Epoch 51 TRAIN info lr 0.0009587999999999999
I0626 03:11:56.008929 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:11:56.009001 139724881797504 executor.py:46] total epoch is 51.
I0626 03:12:03.748553 139724881797504 executor.py:115] TRAIN Batch 51/0 loss 14.931125 loss_att 9.748048 loss_ctc 27.024971 lr 0.00095900 rank 0
I0626 03:12:14.632678 139724881797504 executor.py:115] TRAIN Batch 51/100 loss 24.489510 loss_att 16.001583 loss_ctc 44.294670 lr 0.00096020 rank 0
I0626 03:12:25.783575 139724881797504 executor.py:115] TRAIN Batch 51/200 loss 25.577206 loss_att 16.391012 loss_ctc 47.011658 lr 0.00096140 rank 0
I0626 03:12:36.899459 139724881797504 executor.py:115] TRAIN Batch 51/300 loss 15.999901 loss_att 10.477356 loss_ctc 28.885841 lr 0.00096260 rank 0
I0626 03:12:48.250001 139724881797504 executor.py:115] TRAIN Batch 51/400 loss 16.593260 loss_att 9.576778 loss_ctc 32.965050 lr 0.00096400 rank 0
I0626 03:12:59.901342 139724881797504 executor.py:115] TRAIN Batch 51/500 loss 13.666643 loss_att 9.609597 loss_ctc 23.133083 lr 0.00096520 rank 0
I0626 03:13:10.144254 139724881797504 executor.py:115] TRAIN Batch 51/600 loss 26.214382 loss_att 19.016466 loss_ctc 43.009521 lr 0.00096640 rank 0
I0626 03:13:21.608751 139724881797504 executor.py:115] TRAIN Batch 51/700 loss 20.005020 loss_att 14.219179 loss_ctc 33.505318 lr 0.00096760 rank 0
I0626 03:13:33.433661 139724881797504 executor.py:115] TRAIN Batch 51/800 loss 17.067196 loss_att 11.917172 loss_ctc 29.083916 lr 0.00096900 rank 0
I0626 03:13:44.850085 139724881797504 executor.py:115] TRAIN Batch 51/900 loss 18.817356 loss_att 13.333200 loss_ctc 31.613718 lr 0.00097020 rank 0
I0626 03:13:56.225649 139724881797504 executor.py:115] TRAIN Batch 51/1000 loss 11.428927 loss_att 8.353428 loss_ctc 18.605093 lr 0.00097140 rank 0
I0626 03:14:06.945263 139724881797504 executor.py:115] TRAIN Batch 51/1100 loss 24.529413 loss_att 18.907942 loss_ctc 37.646179 lr 0.00097260 rank 0
I0626 03:14:18.201230 139724881797504 executor.py:115] TRAIN Batch 51/1200 loss 23.427311 loss_att 16.774996 loss_ctc 38.949379 lr 0.00097400 rank 0
I0626 03:14:29.362240 139724881797504 executor.py:115] TRAIN Batch 51/1300 loss 21.007725 loss_att 14.938901 loss_ctc 35.168316 lr 0.00097520 rank 0
I0626 03:14:40.365722 139724881797504 executor.py:115] TRAIN Batch 51/1400 loss 14.303175 loss_att 9.029378 loss_ctc 26.608700 lr 0.00097640 rank 0
I0626 03:14:51.387460 139724881797504 executor.py:115] TRAIN Batch 51/1500 loss 26.636860 loss_att 19.838179 loss_ctc 42.500446 lr 0.00097760 rank 0
I0626 03:14:56.972260 139724881797504 executor.py:152] CV Batch 51/0 loss 20.226994 loss_att 20.927109 loss_ctc 18.593388 history loss 17.979550 rank 0
I0626 03:15:00.929910 139724881797504 executor.py:152] CV Batch 51/100 loss 54.203644 loss_att 58.802727 loss_ctc 43.472450 history loss 38.832097 rank 0
I0626 03:15:03.906182 139724881797504 train.py:288] Epoch 51 CV info cv_loss 44.94709640470985
I0626 03:15:03.906431 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/51.pt
I0626 03:15:04.165735 139724881797504 train.py:274] Epoch 52 TRAIN info lr 0.0009776
I0626 03:15:04.168836 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:15:04.168907 139724881797504 executor.py:46] total epoch is 52.
I0626 03:15:12.004323 139724881797504 executor.py:115] TRAIN Batch 52/0 loss 12.328241 loss_att 9.064581 loss_ctc 19.943447 lr 0.00097780 rank 0
I0626 03:15:22.932883 139724881797504 executor.py:115] TRAIN Batch 52/100 loss 24.680992 loss_att 17.306961 loss_ctc 41.887066 lr 0.00097900 rank 0
I0626 03:15:33.908639 139724881797504 executor.py:115] TRAIN Batch 52/200 loss 20.582088 loss_att 14.254115 loss_ctc 35.347359 lr 0.00098020 rank 0
I0626 03:15:44.986403 139724881797504 executor.py:115] TRAIN Batch 52/300 loss 17.967506 loss_att 11.633039 loss_ctc 32.747932 lr 0.00098140 rank 0
I0626 03:15:56.643108 139724881797504 executor.py:115] TRAIN Batch 52/400 loss 15.129417 loss_att 10.489607 loss_ctc 25.955643 lr 0.00098280 rank 0
I0626 03:16:08.076819 139724881797504 executor.py:115] TRAIN Batch 52/500 loss 10.722296 loss_att 7.624200 loss_ctc 17.951185 lr 0.00098400 rank 0
I0626 03:16:18.727818 139724881797504 executor.py:115] TRAIN Batch 52/600 loss 23.603165 loss_att 17.069918 loss_ctc 38.847404 lr 0.00098520 rank 0
I0626 03:16:30.008230 139724881797504 executor.py:115] TRAIN Batch 52/700 loss 25.303501 loss_att 17.552700 loss_ctc 43.388702 lr 0.00098640 rank 0
I0626 03:16:41.893069 139724881797504 executor.py:115] TRAIN Batch 52/800 loss 16.427019 loss_att 11.212141 loss_ctc 28.595066 lr 0.00098780 rank 0
I0626 03:16:53.684036 139724881797504 executor.py:115] TRAIN Batch 52/900 loss 13.859981 loss_att 8.787702 loss_ctc 25.695297 lr 0.00098900 rank 0
I0626 03:17:04.866243 139724881797504 executor.py:115] TRAIN Batch 52/1000 loss 12.047781 loss_att 8.496370 loss_ctc 20.334406 lr 0.00099020 rank 0
I0626 03:17:15.796477 139724881797504 executor.py:115] TRAIN Batch 52/1100 loss 27.347042 loss_att 19.954504 loss_ctc 44.596291 lr 0.00099140 rank 0
I0626 03:17:27.001618 139724881797504 executor.py:115] TRAIN Batch 52/1200 loss 20.921038 loss_att 13.621935 loss_ctc 37.952274 lr 0.00099280 rank 0
I0626 03:17:38.357386 139724881797504 executor.py:115] TRAIN Batch 52/1300 loss 16.520010 loss_att 10.893358 loss_ctc 29.648861 lr 0.00099400 rank 0
I0626 03:17:49.531342 139724881797504 executor.py:115] TRAIN Batch 52/1400 loss 17.454094 loss_att 11.357763 loss_ctc 31.678862 lr 0.00099520 rank 0
I0626 03:18:00.692825 139724881797504 executor.py:115] TRAIN Batch 52/1500 loss 19.709438 loss_att 13.868182 loss_ctc 33.339039 lr 0.00099640 rank 0
I0626 03:18:06.298045 139724881797504 executor.py:152] CV Batch 52/0 loss 21.286570 loss_att 22.351387 loss_ctc 18.801998 history loss 18.921395 rank 0
I0626 03:18:10.596676 139724881797504 executor.py:152] CV Batch 52/100 loss 54.783386 loss_att 59.529449 loss_ctc 43.709251 history loss 40.563647 rank 0
I0626 03:18:13.569057 139724881797504 train.py:288] Epoch 52 CV info cv_loss 45.57122825627883
I0626 03:18:13.569198 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/52.pt
I0626 03:18:13.825523 139724881797504 train.py:274] Epoch 53 TRAIN info lr 0.0009964
I0626 03:18:13.828541 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:18:13.828613 139724881797504 executor.py:46] total epoch is 53.
I0626 03:18:21.635894 139724881797504 executor.py:115] TRAIN Batch 53/0 loss 11.591009 loss_att 7.541120 loss_ctc 21.040749 lr 0.00099660 rank 0
I0626 03:18:32.703202 139724881797504 executor.py:115] TRAIN Batch 53/100 loss 32.617916 loss_att 21.337608 loss_ctc 58.938625 lr 0.00099780 rank 0
I0626 03:18:43.768477 139724881797504 executor.py:115] TRAIN Batch 53/200 loss 22.123096 loss_att 14.536741 loss_ctc 39.824593 lr 0.00099900 rank 0
I0626 03:18:54.958770 139724881797504 executor.py:115] TRAIN Batch 53/300 loss 21.706097 loss_att 14.972670 loss_ctc 37.417427 lr 0.00099990 rank 0
I0626 03:19:06.741655 139724881797504 executor.py:115] TRAIN Batch 53/400 loss 15.679085 loss_att 10.126585 loss_ctc 28.634918 lr 0.00099920 rank 0
I0626 03:19:18.798824 139724881797504 executor.py:115] TRAIN Batch 53/500 loss 10.965570 loss_att 8.469339 loss_ctc 16.790108 lr 0.00099860 rank 0
I0626 03:19:29.644917 139724881797504 executor.py:115] TRAIN Batch 53/600 loss 27.514318 loss_att 18.835106 loss_ctc 47.765812 lr 0.00099801 rank 0
I0626 03:19:40.972338 139724881797504 executor.py:115] TRAIN Batch 53/700 loss 19.969479 loss_att 13.712301 loss_ctc 34.569557 lr 0.00099741 rank 0
I0626 03:19:52.446693 139724881797504 executor.py:115] TRAIN Batch 53/800 loss 17.764400 loss_att 11.614334 loss_ctc 32.114555 lr 0.00099672 rank 0
I0626 03:20:04.065929 139724881797504 executor.py:115] TRAIN Batch 53/900 loss 15.365846 loss_att 8.963747 loss_ctc 30.304075 lr 0.00099612 rank 0
I0626 03:20:15.211921 139724881797504 executor.py:115] TRAIN Batch 53/1000 loss 12.332412 loss_att 8.794099 loss_ctc 20.588474 lr 0.00099553 rank 0
I0626 03:20:25.694263 139724881797504 executor.py:115] TRAIN Batch 53/1100 loss 27.044661 loss_att 20.039280 loss_ctc 43.390549 lr 0.00099494 rank 0
I0626 03:20:37.054716 139724881797504 executor.py:115] TRAIN Batch 53/1200 loss 20.558266 loss_att 15.257687 loss_ctc 32.926277 lr 0.00099425 rank 0
I0626 03:20:48.568197 139724881797504 executor.py:115] TRAIN Batch 53/1300 loss 18.186012 loss_att 13.194866 loss_ctc 29.832016 lr 0.00099366 rank 0
I0626 03:21:00.115252 139724881797504 executor.py:115] TRAIN Batch 53/1400 loss 14.616341 loss_att 10.907125 loss_ctc 23.271179 lr 0.00099307 rank 0
I0626 03:21:11.103277 139724881797504 executor.py:115] TRAIN Batch 53/1500 loss 16.263222 loss_att 12.078945 loss_ctc 26.026531 lr 0.00099249 rank 0
I0626 03:21:16.717421 139724881797504 executor.py:152] CV Batch 53/0 loss 18.540791 loss_att 19.142376 loss_ctc 17.137089 history loss 16.480703 rank 0
I0626 03:21:20.768681 139724881797504 executor.py:152] CV Batch 53/100 loss 52.418499 loss_att 57.122284 loss_ctc 41.442993 history loss 38.089807 rank 0
I0626 03:21:23.746374 139724881797504 train.py:288] Epoch 53 CV info cv_loss 43.66844525440373
I0626 03:21:23.746541 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/53.pt
I0626 03:21:24.006452 139724881797504 train.py:274] Epoch 54 TRAIN info lr 0.0009924855569590227
I0626 03:21:24.009426 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:21:24.009681 139724881797504 executor.py:46] total epoch is 54.
I0626 03:21:31.734092 139724881797504 executor.py:115] TRAIN Batch 54/0 loss 14.131194 loss_att 9.544004 loss_ctc 24.834637 lr 0.00099239 rank 0
I0626 03:21:42.569891 139724881797504 executor.py:115] TRAIN Batch 54/100 loss 22.913025 loss_att 16.269489 loss_ctc 38.414612 lr 0.00099180 rank 0
I0626 03:21:53.469683 139724881797504 executor.py:115] TRAIN Batch 54/200 loss 19.302195 loss_att 12.901843 loss_ctc 34.236347 lr 0.00099122 rank 0
I0626 03:22:04.619890 139724881797504 executor.py:115] TRAIN Batch 54/300 loss 16.214182 loss_att 9.656869 loss_ctc 31.514576 lr 0.00099063 rank 0
I0626 03:22:16.217968 139724881797504 executor.py:115] TRAIN Batch 54/400 loss 15.398012 loss_att 9.861150 loss_ctc 28.317358 lr 0.00098995 rank 0
I0626 03:22:28.022136 139724881797504 executor.py:115] TRAIN Batch 54/500 loss 12.724134 loss_att 9.206606 loss_ctc 20.931702 lr 0.00098937 rank 0
I0626 03:22:38.578649 139724881797504 executor.py:115] TRAIN Batch 54/600 loss 26.947239 loss_att 18.771408 loss_ctc 46.024178 lr 0.00098879 rank 0
I0626 03:22:49.773762 139724881797504 executor.py:115] TRAIN Batch 54/700 loss 22.586567 loss_att 14.799762 loss_ctc 40.755775 lr 0.00098821 rank 0
I0626 03:23:01.277401 139724881797504 executor.py:115] TRAIN Batch 54/800 loss 15.060888 loss_att 9.652845 loss_ctc 27.679653 lr 0.00098754 rank 0
I0626 03:23:12.721607 139724881797504 executor.py:115] TRAIN Batch 54/900 loss 17.356155 loss_att 12.306374 loss_ctc 29.138981 lr 0.00098696 rank 0
I0626 03:23:23.809758 139724881797504 executor.py:115] TRAIN Batch 54/1000 loss 13.728674 loss_att 9.468803 loss_ctc 23.668369 lr 0.00098638 rank 0
I0626 03:23:34.398532 139724881797504 executor.py:115] TRAIN Batch 54/1100 loss 26.981773 loss_att 20.299944 loss_ctc 42.572708 lr 0.00098581 rank 0
I0626 03:23:46.032452 139724881797504 executor.py:115] TRAIN Batch 54/1200 loss 22.978645 loss_att 16.081295 loss_ctc 39.072464 lr 0.00098514 rank 0
I0626 03:23:57.171328 139724881797504 executor.py:115] TRAIN Batch 54/1300 loss 17.079063 loss_att 12.098614 loss_ctc 28.700113 lr 0.00098456 rank 0
I0626 03:24:08.125714 139724881797504 executor.py:115] TRAIN Batch 54/1400 loss 16.290749 loss_att 10.024755 loss_ctc 30.911400 lr 0.00098399 rank 0
I0626 03:24:18.975754 139724881797504 executor.py:115] TRAIN Batch 54/1500 loss 17.877789 loss_att 12.485927 loss_ctc 30.458797 lr 0.00098342 rank 0
I0626 03:24:24.572362 139724881797504 executor.py:152] CV Batch 54/0 loss 18.174404 loss_att 19.010464 loss_ctc 16.223595 history loss 16.155026 rank 0
I0626 03:24:28.516818 139724881797504 executor.py:152] CV Batch 54/100 loss 53.060291 loss_att 57.251663 loss_ctc 43.280426 history loss 37.834200 rank 0
I0626 03:24:31.498478 139724881797504 train.py:288] Epoch 54 CV info cv_loss 43.68192380704168
I0626 03:24:31.498692 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/54.pt
I0626 03:24:31.763720 139724881797504 train.py:274] Epoch 55 TRAIN info lr 0.0009834215720608249
I0626 03:24:31.766473 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:24:31.766530 139724881797504 executor.py:46] total epoch is 55.
I0626 03:24:39.605291 139724881797504 executor.py:115] TRAIN Batch 55/0 loss 9.828325 loss_att 6.449433 loss_ctc 17.712406 lr 0.00098333 rank 0
I0626 03:24:50.368668 139724881797504 executor.py:115] TRAIN Batch 55/100 loss 21.794315 loss_att 14.347397 loss_ctc 39.170460 lr 0.00098276 rank 0
I0626 03:25:01.377328 139724881797504 executor.py:115] TRAIN Batch 55/200 loss 18.674690 loss_att 11.599070 loss_ctc 35.184475 lr 0.00098219 rank 0
I0626 03:25:12.391858 139724881797504 executor.py:115] TRAIN Batch 55/300 loss 18.735287 loss_att 12.579185 loss_ctc 33.099525 lr 0.00098162 rank 0
I0626 03:25:23.579120 139724881797504 executor.py:115] TRAIN Batch 55/400 loss 15.363198 loss_att 10.522091 loss_ctc 26.659115 lr 0.00098096 rank 0
I0626 03:25:35.159747 139724881797504 executor.py:115] TRAIN Batch 55/500 loss 10.480024 loss_att 8.640461 loss_ctc 14.772340 lr 0.00098039 rank 0
I0626 03:25:45.612625 139724881797504 executor.py:115] TRAIN Batch 55/600 loss 26.190969 loss_att 19.540943 loss_ctc 41.707695 lr 0.00097983 rank 0
I0626 03:25:56.471408 139724881797504 executor.py:115] TRAIN Batch 55/700 loss 15.453798 loss_att 9.830862 loss_ctc 28.573982 lr 0.00097926 rank 0
I0626 03:26:07.574082 139724881797504 executor.py:115] TRAIN Batch 55/800 loss 18.830578 loss_att 12.711300 loss_ctc 33.108891 lr 0.00097861 rank 0
I0626 03:26:19.153429 139724881797504 executor.py:115] TRAIN Batch 55/900 loss 13.604975 loss_att 8.804413 loss_ctc 24.806288 lr 0.00097804 rank 0
I0626 03:26:30.268689 139724881797504 executor.py:115] TRAIN Batch 55/1000 loss 12.687050 loss_att 9.580231 loss_ctc 19.936295 lr 0.00097748 rank 0
I0626 03:26:41.156911 139724881797504 executor.py:115] TRAIN Batch 55/1100 loss 19.960758 loss_att 14.570901 loss_ctc 32.537094 lr 0.00097692 rank 0
I0626 03:26:52.656224 139724881797504 executor.py:115] TRAIN Batch 55/1200 loss 17.228504 loss_att 12.509208 loss_ctc 28.240194 lr 0.00097627 rank 0
I0626 03:27:04.021148 139724881797504 executor.py:115] TRAIN Batch 55/1300 loss 16.692154 loss_att 11.407431 loss_ctc 29.023174 lr 0.00097571 rank 0
I0626 03:27:15.452716 139724881797504 executor.py:115] TRAIN Batch 55/1400 loss 14.331182 loss_att 9.172865 loss_ctc 26.367258 lr 0.00097516 rank 0
I0626 03:27:26.289944 139724881797504 executor.py:115] TRAIN Batch 55/1500 loss 15.453724 loss_att 9.739924 loss_ctc 28.785925 lr 0.00097460 rank 0
I0626 03:27:31.846258 139724881797504 executor.py:152] CV Batch 55/0 loss 18.067932 loss_att 18.952578 loss_ctc 16.003763 history loss 16.060384 rank 0
I0626 03:27:35.839138 139724881797504 executor.py:152] CV Batch 55/100 loss 51.596798 loss_att 55.871819 loss_ctc 41.621750 history loss 36.453193 rank 0
I0626 03:27:38.838607 139724881797504 train.py:288] Epoch 55 CV info cv_loss 41.58281955332999
I0626 03:27:38.838779 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/55.pt
I0626 03:27:39.100508 139724881797504 train.py:274] Epoch 56 TRAIN info lr 0.0009746014694818131
I0626 03:27:39.103501 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:27:39.103573 139724881797504 executor.py:46] total epoch is 56.
I0626 03:27:46.936830 139724881797504 executor.py:115] TRAIN Batch 56/0 loss 11.549281 loss_att 8.054495 loss_ctc 19.703781 lr 0.00097451 rank 0
I0626 03:27:57.786244 139724881797504 executor.py:115] TRAIN Batch 56/100 loss 22.715866 loss_att 15.924868 loss_ctc 38.561531 lr 0.00097395 rank 0
I0626 03:28:08.720160 139724881797504 executor.py:115] TRAIN Batch 56/200 loss 20.647984 loss_att 13.141661 loss_ctc 38.162731 lr 0.00097340 rank 0
I0626 03:28:19.627462 139724881797504 executor.py:115] TRAIN Batch 56/300 loss 16.276588 loss_att 10.099913 loss_ctc 30.688829 lr 0.00097285 rank 0
I0626 03:28:30.764172 139724881797504 executor.py:115] TRAIN Batch 56/400 loss 15.310298 loss_att 10.053297 loss_ctc 27.576633 lr 0.00097220 rank 0
I0626 03:28:42.742810 139724881797504 executor.py:115] TRAIN Batch 56/500 loss 9.840270 loss_att 6.827466 loss_ctc 16.870146 lr 0.00097165 rank 0
I0626 03:28:53.053717 139724881797504 executor.py:115] TRAIN Batch 56/600 loss 21.975048 loss_att 15.242702 loss_ctc 37.683857 lr 0.00097110 rank 0
I0626 03:29:04.250453 139724881797504 executor.py:115] TRAIN Batch 56/700 loss 20.876770 loss_att 15.139803 loss_ctc 34.263023 lr 0.00097055 rank 0
I0626 03:29:15.899744 139724881797504 executor.py:115] TRAIN Batch 56/800 loss 16.784468 loss_att 10.027582 loss_ctc 32.550529 lr 0.00096991 rank 0
I0626 03:29:27.429331 139724881797504 executor.py:115] TRAIN Batch 56/900 loss 18.216007 loss_att 11.266131 loss_ctc 34.432381 lr 0.00096937 rank 0
I0626 03:29:38.499907 139724881797504 executor.py:115] TRAIN Batch 56/1000 loss 13.892334 loss_att 8.967329 loss_ctc 25.384014 lr 0.00096882 rank 0
I0626 03:29:49.180197 139724881797504 executor.py:115] TRAIN Batch 56/1100 loss 22.468422 loss_att 15.725407 loss_ctc 38.202118 lr 0.00096828 rank 0
I0626 03:30:00.178978 139724881797504 executor.py:115] TRAIN Batch 56/1200 loss 18.229021 loss_att 13.072031 loss_ctc 30.261997 lr 0.00096764 rank 0
I0626 03:30:11.453058 139724881797504 executor.py:115] TRAIN Batch 56/1300 loss 17.600670 loss_att 12.265978 loss_ctc 30.048279 lr 0.00096710 rank 0
I0626 03:30:22.483000 139724881797504 executor.py:115] TRAIN Batch 56/1400 loss 12.397451 loss_att 8.271336 loss_ctc 22.025057 lr 0.00096656 rank 0
I0626 03:30:33.266886 139724881797504 executor.py:115] TRAIN Batch 56/1500 loss 20.499245 loss_att 13.196266 loss_ctc 37.539528 lr 0.00096601 rank 0
I0626 03:30:38.843740 139724881797504 executor.py:152] CV Batch 56/0 loss 18.962666 loss_att 19.670132 loss_ctc 17.311916 history loss 16.855703 rank 0
I0626 03:30:42.802318 139724881797504 executor.py:152] CV Batch 56/100 loss 51.359837 loss_att 55.680756 loss_ctc 41.277695 history loss 36.621429 rank 0
I0626 03:30:45.782844 139724881797504 train.py:288] Epoch 56 CV info cv_loss 42.26728838370217
I0626 03:30:45.783061 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/56.pt
I0626 03:30:46.039372 139724881797504 train.py:274] Epoch 57 TRAIN info lr 0.0009660145050098943
I0626 03:30:46.042377 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:30:46.042438 139724881797504 executor.py:46] total epoch is 57.
I0626 03:30:53.753915 139724881797504 executor.py:115] TRAIN Batch 57/0 loss 10.454561 loss_att 7.085542 loss_ctc 18.315609 lr 0.00096592 rank 0
I0626 03:31:04.963371 139724881797504 executor.py:115] TRAIN Batch 57/100 loss 18.560894 loss_att 13.615408 loss_ctc 30.100365 lr 0.00096538 rank 0
I0626 03:31:16.201473 139724881797504 executor.py:115] TRAIN Batch 57/200 loss 20.545086 loss_att 13.714479 loss_ctc 36.483166 lr 0.00096484 rank 0
I0626 03:31:27.504293 139724881797504 executor.py:115] TRAIN Batch 57/300 loss 13.792303 loss_att 9.605625 loss_ctc 23.561216 lr 0.00096431 rank 0
I0626 03:31:38.792185 139724881797504 executor.py:115] TRAIN Batch 57/400 loss 13.420637 loss_att 8.545597 loss_ctc 24.795731 lr 0.00096368 rank 0
I0626 03:31:49.945622 139724881797504 executor.py:115] TRAIN Batch 57/500 loss 11.571297 loss_att 8.277848 loss_ctc 19.256008 lr 0.00096314 rank 0
I0626 03:32:00.133507 139724881797504 executor.py:115] TRAIN Batch 57/600 loss 24.346081 loss_att 16.475363 loss_ctc 42.711090 lr 0.00096261 rank 0
I0626 03:32:10.977679 139724881797504 executor.py:115] TRAIN Batch 57/700 loss 17.626940 loss_att 12.239609 loss_ctc 30.197378 lr 0.00096207 rank 0
I0626 03:32:22.681389 139724881797504 executor.py:115] TRAIN Batch 57/800 loss 19.244574 loss_att 12.563128 loss_ctc 34.834614 lr 0.00096145 rank 0
I0626 03:32:34.413423 139724881797504 executor.py:115] TRAIN Batch 57/900 loss 12.851775 loss_att 8.341282 loss_ctc 23.376259 lr 0.00096092 rank 0
I0626 03:32:45.528823 139724881797504 executor.py:115] TRAIN Batch 57/1000 loss 15.810929 loss_att 9.800803 loss_ctc 29.834557 lr 0.00096038 rank 0
I0626 03:32:56.502166 139724881797504 executor.py:115] TRAIN Batch 57/1100 loss 21.965740 loss_att 16.069473 loss_ctc 35.723690 lr 0.00095985 rank 0
I0626 03:33:07.574334 139724881797504 executor.py:115] TRAIN Batch 57/1200 loss 19.382753 loss_att 13.037768 loss_ctc 34.187717 lr 0.00095924 rank 0
I0626 03:33:18.460829 139724881797504 executor.py:115] TRAIN Batch 57/1300 loss 17.882946 loss_att 11.828083 loss_ctc 32.010956 lr 0.00095871 rank 0
I0626 03:33:29.523193 139724881797504 executor.py:115] TRAIN Batch 57/1400 loss 11.028899 loss_att 7.679919 loss_ctc 18.843185 lr 0.00095818 rank 0
I0626 03:33:40.458396 139724881797504 executor.py:115] TRAIN Batch 57/1500 loss 15.175564 loss_att 9.694115 loss_ctc 27.965612 lr 0.00095765 rank 0
I0626 03:33:46.013045 139724881797504 executor.py:152] CV Batch 57/0 loss 16.648573 loss_att 17.541054 loss_ctc 14.566117 history loss 14.798731 rank 0
I0626 03:33:50.114395 139724881797504 executor.py:152] CV Batch 57/100 loss 50.441830 loss_att 54.178902 loss_ctc 41.721992 history loss 35.589650 rank 0
I0626 03:33:53.091216 139724881797504 train.py:288] Epoch 57 CV info cv_loss 40.68137566725524
I0626 03:33:53.091364 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/57.pt
I0626 03:33:53.349244 139724881797504 train.py:274] Epoch 58 TRAIN info lr 0.0009576505856351146
I0626 03:33:53.352367 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:33:53.352437 139724881797504 executor.py:46] total epoch is 58.
I0626 03:34:01.167493 139724881797504 executor.py:115] TRAIN Batch 58/0 loss 8.902134 loss_att 6.072941 loss_ctc 15.503582 lr 0.00095756 rank 0
I0626 03:34:11.927604 139724881797504 executor.py:115] TRAIN Batch 58/100 loss 14.955305 loss_att 10.282343 loss_ctc 25.858885 lr 0.00095704 rank 0
I0626 03:34:23.084646 139724881797504 executor.py:115] TRAIN Batch 58/200 loss 15.700056 loss_att 11.694670 loss_ctc 25.045956 lr 0.00095651 rank 0
I0626 03:34:34.239357 139724881797504 executor.py:115] TRAIN Batch 58/300 loss 17.608915 loss_att 11.401455 loss_ctc 32.092991 lr 0.00095599 rank 0
I0626 03:34:45.677324 139724881797504 executor.py:115] TRAIN Batch 58/400 loss 11.789579 loss_att 8.308471 loss_ctc 19.912165 lr 0.00095538 rank 0
I0626 03:34:57.232919 139724881797504 executor.py:115] TRAIN Batch 58/500 loss 10.254356 loss_att 6.817057 loss_ctc 18.274719 lr 0.00095485 rank 0
I0626 03:35:07.397439 139724881797504 executor.py:115] TRAIN Batch 58/600 loss 18.511068 loss_att 13.646190 loss_ctc 29.862452 lr 0.00095433 rank 0
I0626 03:35:18.360801 139724881797504 executor.py:115] TRAIN Batch 58/700 loss 20.126869 loss_att 13.041008 loss_ctc 36.660545 lr 0.00095381 rank 0
I0626 03:35:29.775355 139724881797504 executor.py:115] TRAIN Batch 58/800 loss 18.972624 loss_att 13.196410 loss_ctc 32.450455 lr 0.00095320 rank 0
I0626 03:35:40.894597 139724881797504 executor.py:115] TRAIN Batch 58/900 loss 12.839458 loss_att 7.525240 loss_ctc 25.239304 lr 0.00095268 rank 0
I0626 03:35:51.829632 139724881797504 executor.py:115] TRAIN Batch 58/1000 loss 10.872717 loss_att 6.930682 loss_ctc 20.070795 lr 0.00095217 rank 0
I0626 03:36:02.239767 139724881797504 executor.py:115] TRAIN Batch 58/1100 loss 19.084427 loss_att 14.039951 loss_ctc 30.854874 lr 0.00095165 rank 0
I0626 03:36:13.241751 139724881797504 executor.py:115] TRAIN Batch 58/1200 loss 15.338440 loss_att 11.611776 loss_ctc 24.033989 lr 0.00095104 rank 0
I0626 03:36:24.400060 139724881797504 executor.py:115] TRAIN Batch 58/1300 loss 15.360248 loss_att 10.194159 loss_ctc 27.414454 lr 0.00095053 rank 0
I0626 03:36:35.287089 139724881797504 executor.py:115] TRAIN Batch 58/1400 loss 14.640782 loss_att 9.492004 loss_ctc 26.654596 lr 0.00095001 rank 0
I0626 03:36:45.955722 139724881797504 executor.py:115] TRAIN Batch 58/1500 loss 19.947386 loss_att 15.518009 loss_ctc 30.282598 lr 0.00094950 rank 0
I0626 03:36:51.509593 139724881797504 executor.py:152] CV Batch 58/0 loss 17.843977 loss_att 18.341331 loss_ctc 16.683483 history loss 15.861313 rank 0
I0626 03:36:55.512689 139724881797504 executor.py:152] CV Batch 58/100 loss 52.875546 loss_att 57.343735 loss_ctc 42.449776 history loss 35.993393 rank 0
I0626 03:36:58.489152 139724881797504 train.py:288] Epoch 58 CV info cv_loss 41.03976752952688
I0626 03:36:58.489333 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/58.pt
I0626 03:36:58.747401 139724881797504 train.py:274] Epoch 59 TRAIN info lr 0.0009495002196669012
I0626 03:36:58.750377 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:36:58.750437 139724881797504 executor.py:46] total epoch is 59.
I0626 03:37:06.477085 139724881797504 executor.py:115] TRAIN Batch 59/0 loss 8.300944 loss_att 5.118820 loss_ctc 15.725902 lr 0.00094941 rank 0
I0626 03:37:17.201821 139724881797504 executor.py:115] TRAIN Batch 59/100 loss 19.833454 loss_att 14.037501 loss_ctc 33.357342 lr 0.00094890 rank 0
I0626 03:37:28.653366 139724881797504 executor.py:115] TRAIN Batch 59/200 loss 15.582014 loss_att 9.988325 loss_ctc 28.633955 lr 0.00094839 rank 0
I0626 03:37:40.188807 139724881797504 executor.py:115] TRAIN Batch 59/300 loss 13.265932 loss_att 9.665779 loss_ctc 21.666290 lr 0.00094788 rank 0
I0626 03:37:51.883036 139724881797504 executor.py:115] TRAIN Batch 59/400 loss 12.183100 loss_att 8.097665 loss_ctc 21.715782 lr 0.00094728 rank 0
I0626 03:38:03.509799 139724881797504 executor.py:115] TRAIN Batch 59/500 loss 7.508065 loss_att 5.150312 loss_ctc 13.009488 lr 0.00094677 rank 0
I0626 03:38:14.445000 139724881797504 executor.py:115] TRAIN Batch 59/600 loss 19.924002 loss_att 14.207716 loss_ctc 33.262001 lr 0.00094626 rank 0
I0626 03:38:25.865540 139724881797504 executor.py:115] TRAIN Batch 59/700 loss 19.349398 loss_att 12.604515 loss_ctc 35.087456 lr 0.00094576 rank 0
I0626 03:38:37.393364 139724881797504 executor.py:115] TRAIN Batch 59/800 loss 14.085415 loss_att 8.747615 loss_ctc 26.540283 lr 0.00094516 rank 0
I0626 03:38:48.910773 139724881797504 executor.py:115] TRAIN Batch 59/900 loss 13.464895 loss_att 8.716253 loss_ctc 24.545061 lr 0.00094466 rank 0
I0626 03:39:00.064696 139724881797504 executor.py:115] TRAIN Batch 59/1000 loss 11.236570 loss_att 7.731109 loss_ctc 19.415981 lr 0.00094415 rank 0
I0626 03:39:10.951480 139724881797504 executor.py:115] TRAIN Batch 59/1100 loss 23.040663 loss_att 16.093725 loss_ctc 39.250183 lr 0.00094365 rank 0
I0626 03:39:22.459323 139724881797504 executor.py:115] TRAIN Batch 59/1200 loss 17.394424 loss_att 11.918236 loss_ctc 30.172201 lr 0.00094306 rank 0
I0626 03:39:33.424570 139724881797504 executor.py:115] TRAIN Batch 59/1300 loss 14.259087 loss_att 9.207801 loss_ctc 26.045422 lr 0.00094256 rank 0
I0626 03:39:44.480485 139724881797504 executor.py:115] TRAIN Batch 59/1400 loss 7.944138 loss_att 5.844382 loss_ctc 12.843567 lr 0.00094206 rank 0
I0626 03:39:55.291493 139724881797504 executor.py:115] TRAIN Batch 59/1500 loss 24.365047 loss_att 15.763160 loss_ctc 44.436119 lr 0.00094155 rank 0
I0626 03:40:00.827916 139724881797504 executor.py:152] CV Batch 59/0 loss 16.245111 loss_att 17.418060 loss_ctc 13.508229 history loss 14.440099 rank 0
I0626 03:40:04.818981 139724881797504 executor.py:152] CV Batch 59/100 loss 47.632801 loss_att 51.965446 loss_ctc 37.523293 history loss 33.554416 rank 0
I0626 03:40:07.805778 139724881797504 train.py:288] Epoch 59 CV info cv_loss 38.898012127979435
I0626 03:40:07.805985 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/59.pt
I0626 03:40:08.065222 139724881797504 train.py:274] Epoch 60 TRAIN info lr 0.0009415544714433869
I0626 03:40:08.068247 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:40:08.068315 139724881797504 executor.py:46] total epoch is 60.
I0626 03:40:15.962023 139724881797504 executor.py:115] TRAIN Batch 60/0 loss 9.656076 loss_att 6.328369 loss_ctc 17.420727 lr 0.00094147 rank 0
I0626 03:40:26.568351 139724881797504 executor.py:115] TRAIN Batch 60/100 loss 20.314373 loss_att 14.283889 loss_ctc 34.385506 lr 0.00094097 rank 0
I0626 03:40:37.419316 139724881797504 executor.py:115] TRAIN Batch 60/200 loss 18.182438 loss_att 11.442545 loss_ctc 33.908852 lr 0.00094047 rank 0
I0626 03:40:48.535811 139724881797504 executor.py:115] TRAIN Batch 60/300 loss 11.395394 loss_att 7.110476 loss_ctc 21.393536 lr 0.00093997 rank 0
I0626 03:41:00.167794 139724881797504 executor.py:115] TRAIN Batch 60/400 loss 16.910931 loss_att 10.303427 loss_ctc 32.328438 lr 0.00093939 rank 0
I0626 03:41:11.864750 139724881797504 executor.py:115] TRAIN Batch 60/500 loss 10.639447 loss_att 7.687961 loss_ctc 17.526249 lr 0.00093889 rank 0
I0626 03:41:22.238234 139724881797504 executor.py:115] TRAIN Batch 60/600 loss 17.657404 loss_att 13.290854 loss_ctc 27.846018 lr 0.00093840 rank 0
I0626 03:41:33.497279 139724881797504 executor.py:115] TRAIN Batch 60/700 loss 14.237652 loss_att 9.350718 loss_ctc 25.640497 lr 0.00093790 rank 0
I0626 03:41:45.027030 139724881797504 executor.py:115] TRAIN Batch 60/800 loss 16.517878 loss_att 11.994551 loss_ctc 27.072308 lr 0.00093733 rank 0
I0626 03:41:56.580090 139724881797504 executor.py:115] TRAIN Batch 60/900 loss 14.156862 loss_att 8.401897 loss_ctc 27.585110 lr 0.00093683 rank 0
I0626 03:42:07.698659 139724881797504 executor.py:115] TRAIN Batch 60/1000 loss 10.537085 loss_att 6.867064 loss_ctc 19.100466 lr 0.00093634 rank 0
I0626 03:42:18.533088 139724881797504 executor.py:115] TRAIN Batch 60/1100 loss 18.574312 loss_att 12.757465 loss_ctc 32.146954 lr 0.00093585 rank 0
I0626 03:42:29.998782 139724881797504 executor.py:115] TRAIN Batch 60/1200 loss 11.932384 loss_att 9.774195 loss_ctc 16.968161 lr 0.00093527 rank 0
I0626 03:42:41.025322 139724881797504 executor.py:115] TRAIN Batch 60/1300 loss 13.213266 loss_att 9.040436 loss_ctc 22.949873 lr 0.00093478 rank 0
I0626 03:42:52.023993 139724881797504 executor.py:115] TRAIN Batch 60/1400 loss 12.294298 loss_att 9.107293 loss_ctc 19.730642 lr 0.00093429 rank 0
I0626 03:43:03.069579 139724881797504 executor.py:115] TRAIN Batch 60/1500 loss 11.030027 loss_att 7.856882 loss_ctc 18.434032 lr 0.00093380 rank 0
I0626 03:43:08.658219 139724881797504 executor.py:152] CV Batch 60/0 loss 16.643312 loss_att 17.244360 loss_ctc 15.240870 history loss 14.794056 rank 0
I0626 03:43:12.652986 139724881797504 executor.py:152] CV Batch 60/100 loss 49.050457 loss_att 53.499420 loss_ctc 38.669544 history loss 34.327343 rank 0
I0626 03:43:15.641013 139724881797504 train.py:288] Epoch 60 CV info cv_loss 39.361627683885764
I0626 03:43:15.641247 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/60.pt
I0626 03:43:15.904321 139724881797504 train.py:274] Epoch 61 TRAIN info lr 0.0009338049201414408
I0626 03:43:15.907358 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:43:15.907423 139724881797504 executor.py:46] total epoch is 61.
I0626 03:43:23.759835 139724881797504 executor.py:115] TRAIN Batch 61/0 loss 8.120728 loss_att 5.799095 loss_ctc 13.537871 lr 0.00093372 rank 0
I0626 03:43:34.812701 139724881797504 executor.py:115] TRAIN Batch 61/100 loss 17.589874 loss_att 12.478752 loss_ctc 29.515827 lr 0.00093324 rank 0
I0626 03:43:46.213987 139724881797504 executor.py:115] TRAIN Batch 61/200 loss 12.881278 loss_att 8.972453 loss_ctc 22.001869 lr 0.00093275 rank 0
I0626 03:43:57.771053 139724881797504 executor.py:115] TRAIN Batch 61/300 loss 14.434107 loss_att 10.148047 loss_ctc 24.434914 lr 0.00093226 rank 0
I0626 03:44:09.396975 139724881797504 executor.py:115] TRAIN Batch 61/400 loss 14.485990 loss_att 9.180962 loss_ctc 26.864388 lr 0.00093169 rank 0
I0626 03:44:21.022537 139724881797504 executor.py:115] TRAIN Batch 61/500 loss 13.566347 loss_att 8.170189 loss_ctc 26.157385 lr 0.00093121 rank 0
I0626 03:44:31.850519 139724881797504 executor.py:115] TRAIN Batch 61/600 loss 17.895807 loss_att 10.933337 loss_ctc 34.141571 lr 0.00093073 rank 0
I0626 03:44:43.005816 139724881797504 executor.py:115] TRAIN Batch 61/700 loss 16.133635 loss_att 9.123280 loss_ctc 32.491127 lr 0.00093024 rank 0
I0626 03:44:54.758253 139724881797504 executor.py:115] TRAIN Batch 61/800 loss 18.019346 loss_att 11.420624 loss_ctc 33.416363 lr 0.00092968 rank 0
I0626 03:45:06.387176 139724881797504 executor.py:115] TRAIN Batch 61/900 loss 13.717441 loss_att 9.314938 loss_ctc 23.989946 lr 0.00092920 rank 0
I0626 03:45:17.340630 139724881797504 executor.py:115] TRAIN Batch 61/1000 loss 7.964784 loss_att 5.216089 loss_ctc 14.378403 lr 0.00092872 rank 0
I0626 03:45:27.858645 139724881797504 executor.py:115] TRAIN Batch 61/1100 loss 23.488277 loss_att 15.753212 loss_ctc 41.536758 lr 0.00092824 rank 0
I0626 03:45:39.318012 139724881797504 executor.py:115] TRAIN Batch 61/1200 loss 18.674368 loss_att 12.847124 loss_ctc 32.271271 lr 0.00092768 rank 0
I0626 03:45:50.599213 139724881797504 executor.py:115] TRAIN Batch 61/1300 loss 13.910101 loss_att 8.675228 loss_ctc 26.124804 lr 0.00092720 rank 0
I0626 03:46:01.785293 139724881797504 executor.py:115] TRAIN Batch 61/1400 loss 10.475008 loss_att 6.984739 loss_ctc 18.618969 lr 0.00092672 rank 0
I0626 03:46:12.726419 139724881797504 executor.py:115] TRAIN Batch 61/1500 loss 17.114832 loss_att 11.119463 loss_ctc 31.104027 lr 0.00092624 rank 0
I0626 03:46:18.308228 139724881797504 executor.py:152] CV Batch 61/0 loss 16.809456 loss_att 16.886656 loss_ctc 16.629320 history loss 14.941739 rank 0
I0626 03:46:22.378483 139724881797504 executor.py:152] CV Batch 61/100 loss 48.677250 loss_att 53.098801 loss_ctc 38.360298 history loss 33.320171 rank 0
I0626 03:46:25.348200 139724881797504 train.py:288] Epoch 61 CV info cv_loss 38.4244409842002
I0626 03:46:25.348374 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/61.pt
I0626 03:46:25.607789 139724881797504 train.py:274] Epoch 62 TRAIN info lr 0.0009262436222557158
I0626 03:46:25.610763 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:46:25.610826 139724881797504 executor.py:46] total epoch is 62.
I0626 03:46:33.434647 139724881797504 executor.py:115] TRAIN Batch 62/0 loss 11.807410 loss_att 7.757403 loss_ctc 21.257425 lr 0.00092616 rank 0
I0626 03:46:44.434745 139724881797504 executor.py:115] TRAIN Batch 62/100 loss 20.185764 loss_att 12.722063 loss_ctc 37.601067 lr 0.00092569 rank 0
I0626 03:46:55.277107 139724881797504 executor.py:115] TRAIN Batch 62/200 loss 13.218956 loss_att 9.857489 loss_ctc 21.062378 lr 0.00092521 rank 0
I0626 03:47:06.717633 139724881797504 executor.py:115] TRAIN Batch 62/300 loss 14.714653 loss_att 9.778175 loss_ctc 26.233101 lr 0.00092474 rank 0
I0626 03:47:18.298858 139724881797504 executor.py:115] TRAIN Batch 62/400 loss 10.328100 loss_att 6.755480 loss_ctc 18.664215 lr 0.00092418 rank 0
I0626 03:47:29.804041 139724881797504 executor.py:115] TRAIN Batch 62/500 loss 8.306646 loss_att 6.154889 loss_ctc 13.327415 lr 0.00092371 rank 0
I0626 03:47:40.196964 139724881797504 executor.py:115] TRAIN Batch 62/600 loss 19.846336 loss_att 14.313725 loss_ctc 32.755768 lr 0.00092324 rank 0
I0626 03:47:51.318744 139724881797504 executor.py:115] TRAIN Batch 62/700 loss 18.884296 loss_att 12.390233 loss_ctc 34.037113 lr 0.00092277 rank 0
I0626 03:48:02.794865 139724881797504 executor.py:115] TRAIN Batch 62/800 loss 18.219378 loss_att 11.578414 loss_ctc 33.714958 lr 0.00092222 rank 0
I0626 03:48:14.193157 139724881797504 executor.py:115] TRAIN Batch 62/900 loss 9.790639 loss_att 6.883843 loss_ctc 16.573162 lr 0.00092175 rank 0
I0626 03:48:25.128187 139724881797504 executor.py:115] TRAIN Batch 62/1000 loss 12.164970 loss_att 7.463210 loss_ctc 23.135746 lr 0.00092128 rank 0
I0626 03:48:35.606659 139724881797504 executor.py:115] TRAIN Batch 62/1100 loss 17.936272 loss_att 12.200794 loss_ctc 31.319054 lr 0.00092081 rank 0
I0626 03:48:47.020269 139724881797504 executor.py:115] TRAIN Batch 62/1200 loss 15.791075 loss_att 11.770184 loss_ctc 25.173153 lr 0.00092026 rank 0
I0626 03:48:58.423162 139724881797504 executor.py:115] TRAIN Batch 62/1300 loss 14.010277 loss_att 8.588823 loss_ctc 26.660336 lr 0.00091980 rank 0
I0626 03:49:09.679149 139724881797504 executor.py:115] TRAIN Batch 62/1400 loss 12.306496 loss_att 8.062490 loss_ctc 22.209175 lr 0.00091933 rank 0
I0626 03:49:20.566570 139724881797504 executor.py:115] TRAIN Batch 62/1500 loss 14.844178 loss_att 10.047081 loss_ctc 26.037405 lr 0.00091886 rank 0
I0626 03:49:26.157525 139724881797504 executor.py:152] CV Batch 62/0 loss 16.331131 loss_att 17.185005 loss_ctc 14.338760 history loss 14.516561 rank 0
I0626 03:49:30.093905 139724881797504 executor.py:152] CV Batch 62/100 loss 48.422642 loss_att 52.931812 loss_ctc 37.901237 history loss 33.374920 rank 0
I0626 03:49:33.083008 139724881797504 train.py:288] Epoch 62 CV info cv_loss 38.28389454087592
I0626 03:49:33.083131 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/62.pt
I0626 03:49:33.339800 139724881797504 train.py:274] Epoch 63 TRAIN info lr 0.0009188630773666188
I0626 03:49:33.342819 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:49:33.342879 139724881797504 executor.py:46] total epoch is 63.
I0626 03:49:41.117993 139724881797504 executor.py:115] TRAIN Batch 63/0 loss 12.445456 loss_att 8.142569 loss_ctc 22.485525 lr 0.00091879 rank 0
I0626 03:49:51.954462 139724881797504 executor.py:115] TRAIN Batch 63/100 loss 18.123035 loss_att 13.208759 loss_ctc 29.589680 lr 0.00091832 rank 0
I0626 03:50:02.858648 139724881797504 executor.py:115] TRAIN Batch 63/200 loss 9.454352 loss_att 6.313044 loss_ctc 16.784071 lr 0.00091786 rank 0
I0626 03:50:14.086992 139724881797504 executor.py:115] TRAIN Batch 63/300 loss 10.463194 loss_att 7.765990 loss_ctc 16.756668 lr 0.00091739 rank 0
I0626 03:50:25.627479 139724881797504 executor.py:115] TRAIN Batch 63/400 loss 13.135271 loss_att 7.180624 loss_ctc 27.029446 lr 0.00091685 rank 0
I0626 03:50:37.489878 139724881797504 executor.py:115] TRAIN Batch 63/500 loss 13.844437 loss_att 9.243030 loss_ctc 24.581055 lr 0.00091639 rank 0
I0626 03:50:47.849499 139724881797504 executor.py:115] TRAIN Batch 63/600 loss 19.027374 loss_att 13.407273 loss_ctc 32.140945 lr 0.00091593 rank 0
I0626 03:50:59.275866 139724881797504 executor.py:115] TRAIN Batch 63/700 loss 17.753105 loss_att 12.240746 loss_ctc 30.615280 lr 0.00091547 rank 0
I0626 03:51:10.533312 139724881797504 executor.py:115] TRAIN Batch 63/800 loss 15.095635 loss_att 10.332296 loss_ctc 26.210093 lr 0.00091493 rank 0
I0626 03:51:22.194418 139724881797504 executor.py:115] TRAIN Batch 63/900 loss 15.392159 loss_att 10.787417 loss_ctc 26.136555 lr 0.00091447 rank 0
I0626 03:51:33.654795 139724881797504 executor.py:115] TRAIN Batch 63/1000 loss 10.135754 loss_att 6.266563 loss_ctc 19.163862 lr 0.00091401 rank 0
I0626 03:51:44.603178 139724881797504 executor.py:115] TRAIN Batch 63/1100 loss 19.056450 loss_att 11.510477 loss_ctc 36.663723 lr 0.00091356 rank 0
I0626 03:51:55.926820 139724881797504 executor.py:115] TRAIN Batch 63/1200 loss 21.475040 loss_att 13.868169 loss_ctc 39.224411 lr 0.00091302 rank 0
I0626 03:52:07.234824 139724881797504 executor.py:115] TRAIN Batch 63/1300 loss 14.973850 loss_att 10.154542 loss_ctc 26.218903 lr 0.00091257 rank 0
I0626 03:52:18.557406 139724881797504 executor.py:115] TRAIN Batch 63/1400 loss 11.858881 loss_att 7.500730 loss_ctc 22.027901 lr 0.00091211 rank 0
I0626 03:52:29.659076 139724881797504 executor.py:115] TRAIN Batch 63/1500 loss 12.460298 loss_att 7.796592 loss_ctc 23.342278 lr 0.00091166 rank 0
I0626 03:52:35.233851 139724881797504 executor.py:152] CV Batch 63/0 loss 16.213581 loss_att 17.101028 loss_ctc 14.142868 history loss 14.412072 rank 0
I0626 03:52:39.210311 139724881797504 executor.py:152] CV Batch 63/100 loss 48.547905 loss_att 52.557564 loss_ctc 39.192036 history loss 33.102497 rank 0
I0626 03:52:42.185307 139724881797504 train.py:288] Epoch 63 CV info cv_loss 38.257006616585755
I0626 03:52:42.185560 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/63.pt
I0626 03:52:42.445125 139724881797504 train.py:274] Epoch 64 TRAIN info lr 0.0009116561968618409
I0626 03:52:42.447944 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:52:42.448012 139724881797504 executor.py:46] total epoch is 64.
I0626 03:52:50.213143 139724881797504 executor.py:115] TRAIN Batch 64/0 loss 8.090707 loss_att 5.319838 loss_ctc 14.556067 lr 0.00091158 rank 0
I0626 03:53:01.297959 139724881797504 executor.py:115] TRAIN Batch 64/100 loss 14.567554 loss_att 9.897975 loss_ctc 25.463242 lr 0.00091113 rank 0
I0626 03:53:12.120766 139724881797504 executor.py:115] TRAIN Batch 64/200 loss 10.028435 loss_att 6.841702 loss_ctc 17.464142 lr 0.00091067 rank 0
I0626 03:53:23.148202 139724881797504 executor.py:115] TRAIN Batch 64/300 loss 13.665936 loss_att 8.674969 loss_ctc 25.311523 lr 0.00091022 rank 0
I0626 03:53:34.682401 139724881797504 executor.py:115] TRAIN Batch 64/400 loss 16.879009 loss_att 10.914724 loss_ctc 30.795673 lr 0.00090969 rank 0
I0626 03:53:46.379963 139724881797504 executor.py:115] TRAIN Batch 64/500 loss 8.467449 loss_att 5.157767 loss_ctc 16.190041 lr 0.00090924 rank 0
I0626 03:53:56.569540 139724881797504 executor.py:115] TRAIN Batch 64/600 loss 16.237282 loss_att 11.639582 loss_ctc 26.965250 lr 0.00090879 rank 0
I0626 03:54:07.551439 139724881797504 executor.py:115] TRAIN Batch 64/700 loss 15.055608 loss_att 8.741906 loss_ctc 29.787579 lr 0.00090834 rank 0
I0626 03:54:19.004846 139724881797504 executor.py:115] TRAIN Batch 64/800 loss 14.135389 loss_att 9.317669 loss_ctc 25.376736 lr 0.00090782 rank 0
I0626 03:54:30.383933 139724881797504 executor.py:115] TRAIN Batch 64/900 loss 11.944288 loss_att 7.136980 loss_ctc 23.161341 lr 0.00090737 rank 0
I0626 03:54:41.325715 139724881797504 executor.py:115] TRAIN Batch 64/1000 loss 11.087761 loss_att 7.775282 loss_ctc 18.816877 lr 0.00090692 rank 0
I0626 03:54:52.171458 139724881797504 executor.py:115] TRAIN Batch 64/1100 loss 16.609940 loss_att 11.577509 loss_ctc 28.352282 lr 0.00090647 rank 0
I0626 03:55:03.659481 139724881797504 executor.py:115] TRAIN Batch 64/1200 loss 18.973602 loss_att 12.512375 loss_ctc 34.049797 lr 0.00090595 rank 0
I0626 03:55:15.354159 139724881797504 executor.py:115] TRAIN Batch 64/1300 loss 11.254348 loss_att 7.343818 loss_ctc 20.378916 lr 0.00090551 rank 0
I0626 03:55:26.669260 139724881797504 executor.py:115] TRAIN Batch 64/1400 loss 10.472942 loss_att 7.126214 loss_ctc 18.281975 lr 0.00090506 rank 0
I0626 03:55:37.694701 139724881797504 executor.py:115] TRAIN Batch 64/1500 loss 14.912675 loss_att 9.956288 loss_ctc 26.477575 lr 0.00090462 rank 0
I0626 03:55:43.255629 139724881797504 executor.py:152] CV Batch 64/0 loss 15.944939 loss_att 16.571476 loss_ctc 14.483017 history loss 14.173279 rank 0
I0626 03:55:47.195506 139724881797504 executor.py:152] CV Batch 64/100 loss 46.811478 loss_att 50.489788 loss_ctc 38.228756 history loss 32.753868 rank 0
I0626 03:55:50.157714 139724881797504 train.py:288] Epoch 64 CV info cv_loss 37.954956448668796
I0626 03:55:50.157885 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/64.pt
I0626 03:55:50.415309 139724881797504 train.py:274] Epoch 65 TRAIN info lr 0.000904616275314925
I0626 03:55:50.418355 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:55:50.418425 139724881797504 executor.py:46] total epoch is 65.
I0626 03:55:58.114350 139724881797504 executor.py:115] TRAIN Batch 65/0 loss 6.499405 loss_att 4.701906 loss_ctc 10.693569 lr 0.00090454 rank 0
I0626 03:56:08.963080 139724881797504 executor.py:115] TRAIN Batch 65/100 loss 15.575220 loss_att 10.786451 loss_ctc 26.749014 lr 0.00090410 rank 0
I0626 03:56:20.249303 139724881797504 executor.py:115] TRAIN Batch 65/200 loss 12.357920 loss_att 7.879413 loss_ctc 22.807766 lr 0.00090366 rank 0
I0626 03:56:32.039020 139724881797504 executor.py:115] TRAIN Batch 65/300 loss 13.233854 loss_att 8.489807 loss_ctc 24.303295 lr 0.00090321 rank 0
I0626 03:56:43.341387 139724881797504 executor.py:115] TRAIN Batch 65/400 loss 10.490162 loss_att 7.336932 loss_ctc 17.847698 lr 0.00090270 rank 0
I0626 03:56:54.860053 139724881797504 executor.py:115] TRAIN Batch 65/500 loss 14.823453 loss_att 10.013359 loss_ctc 26.047005 lr 0.00090226 rank 0
I0626 03:57:05.885973 139724881797504 executor.py:115] TRAIN Batch 65/600 loss 15.471850 loss_att 11.254362 loss_ctc 25.312656 lr 0.00090182 rank 0
I0626 03:57:17.419437 139724881797504 executor.py:115] TRAIN Batch 65/700 loss 13.179472 loss_att 8.537075 loss_ctc 24.011730 lr 0.00090138 rank 0
I0626 03:57:28.853852 139724881797504 executor.py:115] TRAIN Batch 65/800 loss 12.333757 loss_att 7.248290 loss_ctc 24.199846 lr 0.00090086 rank 0
I0626 03:57:40.196132 139724881797504 executor.py:115] TRAIN Batch 65/900 loss 13.698009 loss_att 9.554182 loss_ctc 23.366940 lr 0.00090043 rank 0
I0626 03:57:51.229748 139724881797504 executor.py:115] TRAIN Batch 65/1000 loss 9.248277 loss_att 6.142667 loss_ctc 16.494699 lr 0.00089999 rank 0
I0626 03:58:01.760492 139724881797504 executor.py:115] TRAIN Batch 65/1100 loss 18.202488 loss_att 12.111513 loss_ctc 32.414764 lr 0.00089955 rank 0
I0626 03:58:13.090079 139724881797504 executor.py:115] TRAIN Batch 65/1200 loss 17.365162 loss_att 11.086569 loss_ctc 32.015209 lr 0.00089904 rank 0
I0626 03:58:24.463729 139724881797504 executor.py:115] TRAIN Batch 65/1300 loss 10.174262 loss_att 6.790787 loss_ctc 18.069036 lr 0.00089861 rank 0
I0626 03:58:35.469779 139724881797504 executor.py:115] TRAIN Batch 65/1400 loss 11.262430 loss_att 7.144933 loss_ctc 20.869923 lr 0.00089817 rank 0
I0626 03:58:46.252078 139724881797504 executor.py:115] TRAIN Batch 65/1500 loss 17.510296 loss_att 12.250091 loss_ctc 29.784109 lr 0.00089774 rank 0
I0626 03:58:51.808678 139724881797504 executor.py:152] CV Batch 65/0 loss 16.410574 loss_att 16.673183 loss_ctc 15.797818 history loss 14.587177 rank 0
I0626 03:58:55.817811 139724881797504 executor.py:152] CV Batch 65/100 loss 48.481030 loss_att 52.311096 loss_ctc 39.544209 history loss 32.289735 rank 0
I0626 03:58:58.784022 139724881797504 train.py:288] Epoch 65 CV info cv_loss 37.467126397760445
I0626 03:58:58.784200 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/65.pt
I0626 03:58:59.041981 139724881797504 train.py:274] Epoch 66 TRAIN info lr 0.0008977369642581767
I0626 03:58:59.045140 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 03:58:59.045213 139724881797504 executor.py:46] total epoch is 66.
I0626 03:59:06.769598 139724881797504 executor.py:115] TRAIN Batch 66/0 loss 8.840967 loss_att 5.861864 loss_ctc 15.792206 lr 0.00089766 rank 0
I0626 03:59:18.312869 139724881797504 executor.py:115] TRAIN Batch 66/100 loss 12.366896 loss_att 8.787878 loss_ctc 20.717936 lr 0.00089723 rank 0
I0626 03:59:29.367302 139724881797504 executor.py:115] TRAIN Batch 66/200 loss 16.025837 loss_att 9.903716 loss_ctc 30.310787 lr 0.00089680 rank 0
I0626 03:59:40.654501 139724881797504 executor.py:115] TRAIN Batch 66/300 loss 10.165341 loss_att 7.229716 loss_ctc 17.015133 lr 0.00089637 rank 0
I0626 03:59:52.120898 139724881797504 executor.py:115] TRAIN Batch 66/400 loss 13.413325 loss_att 8.441137 loss_ctc 25.015097 lr 0.00089586 rank 0
I0626 04:00:03.759691 139724881797504 executor.py:115] TRAIN Batch 66/500 loss 7.350537 loss_att 4.804950 loss_ctc 13.290241 lr 0.00089543 rank 0
I0626 04:00:14.111459 139724881797504 executor.py:115] TRAIN Batch 66/600 loss 16.368210 loss_att 10.747912 loss_ctc 29.482239 lr 0.00089500 rank 0
I0626 04:00:25.053835 139724881797504 executor.py:115] TRAIN Batch 66/700 loss 14.925382 loss_att 10.680377 loss_ctc 24.830391 lr 0.00089457 rank 0
I0626 04:00:36.201250 139724881797504 executor.py:115] TRAIN Batch 66/800 loss 12.104662 loss_att 8.336483 loss_ctc 20.897078 lr 0.00089407 rank 0
I0626 04:00:47.519832 139724881797504 executor.py:115] TRAIN Batch 66/900 loss 11.594893 loss_att 6.943604 loss_ctc 22.447895 lr 0.00089364 rank 0
I0626 04:00:58.553191 139724881797504 executor.py:115] TRAIN Batch 66/1000 loss 6.402748 loss_att 4.211735 loss_ctc 11.515112 lr 0.00089321 rank 0
I0626 04:01:09.440624 139724881797504 executor.py:115] TRAIN Batch 66/1100 loss 16.860270 loss_att 11.183607 loss_ctc 30.105814 lr 0.00089279 rank 0
I0626 04:01:20.935060 139724881797504 executor.py:115] TRAIN Batch 66/1200 loss 16.385876 loss_att 10.695902 loss_ctc 29.662477 lr 0.00089229 rank 0
I0626 04:01:32.230655 139724881797504 executor.py:115] TRAIN Batch 66/1300 loss 18.017727 loss_att 10.910930 loss_ctc 34.600250 lr 0.00089186 rank 0
I0626 04:01:43.577602 139724881797504 executor.py:115] TRAIN Batch 66/1400 loss 10.726706 loss_att 6.648648 loss_ctc 20.242172 lr 0.00089144 rank 0
I0626 04:01:54.420893 139724881797504 executor.py:115] TRAIN Batch 66/1500 loss 16.012840 loss_att 10.869484 loss_ctc 28.014008 lr 0.00089101 rank 0
I0626 04:01:59.968657 139724881797504 executor.py:152] CV Batch 66/0 loss 15.693716 loss_att 16.327354 loss_ctc 14.215225 history loss 13.949970 rank 0
I0626 04:02:04.144143 139724881797504 executor.py:152] CV Batch 66/100 loss 47.681217 loss_att 51.660927 loss_ctc 38.395229 history loss 31.864582 rank 0
I0626 04:02:07.130108 139724881797504 train.py:288] Epoch 66 CV info cv_loss 36.83513363973294
I0626 04:02:07.130232 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/66.pt
I0626 04:02:07.389695 139724881797504 train.py:274] Epoch 67 TRAIN info lr 0.0008910122481167481
I0626 04:02:07.392802 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:02:07.392873 139724881797504 executor.py:46] total epoch is 67.
I0626 04:02:15.177551 139724881797504 executor.py:115] TRAIN Batch 67/0 loss 8.327545 loss_att 5.544401 loss_ctc 14.821548 lr 0.00089094 rank 0
I0626 04:02:25.755339 139724881797504 executor.py:115] TRAIN Batch 67/100 loss 14.509322 loss_att 9.995705 loss_ctc 25.041096 lr 0.00089052 rank 0
I0626 04:02:36.551407 139724881797504 executor.py:115] TRAIN Batch 67/200 loss 12.690088 loss_att 7.386057 loss_ctc 25.066158 lr 0.00089009 rank 0
I0626 04:02:47.827328 139724881797504 executor.py:115] TRAIN Batch 67/300 loss 10.221370 loss_att 7.096495 loss_ctc 17.512745 lr 0.00088967 rank 0
I0626 04:02:59.247629 139724881797504 executor.py:115] TRAIN Batch 67/400 loss 8.711298 loss_att 5.620837 loss_ctc 15.922371 lr 0.00088918 rank 0
I0626 04:03:10.761694 139724881797504 executor.py:115] TRAIN Batch 67/500 loss 8.000443 loss_att 4.504862 loss_ctc 16.156799 lr 0.00088876 rank 0
I0626 04:03:21.664269 139724881797504 executor.py:115] TRAIN Batch 67/600 loss 15.789252 loss_att 11.341590 loss_ctc 26.167131 lr 0.00088834 rank 0
I0626 04:03:33.144933 139724881797504 executor.py:115] TRAIN Batch 67/700 loss 13.511646 loss_att 9.247288 loss_ctc 23.461815 lr 0.00088792 rank 0
I0626 04:03:44.545915 139724881797504 executor.py:115] TRAIN Batch 67/800 loss 8.834311 loss_att 6.097654 loss_ctc 15.219841 lr 0.00088743 rank 0
I0626 04:03:56.156411 139724881797504 executor.py:115] TRAIN Batch 67/900 loss 9.835099 loss_att 4.969518 loss_ctc 21.188122 lr 0.00088701 rank 0
I0626 04:04:07.350791 139724881797504 executor.py:115] TRAIN Batch 67/1000 loss 6.586596 loss_att 4.653393 loss_ctc 11.097402 lr 0.00088659 rank 0
I0626 04:04:18.206233 139724881797504 executor.py:115] TRAIN Batch 67/1100 loss 15.547974 loss_att 10.469976 loss_ctc 27.396633 lr 0.00088617 rank 0
I0626 04:04:29.402081 139724881797504 executor.py:115] TRAIN Batch 67/1200 loss 13.339439 loss_att 8.192440 loss_ctc 25.349106 lr 0.00088568 rank 0
I0626 04:04:40.744765 139724881797504 executor.py:115] TRAIN Batch 67/1300 loss 13.264229 loss_att 9.377395 loss_ctc 22.333508 lr 0.00088527 rank 0
I0626 04:04:52.093633 139724881797504 executor.py:115] TRAIN Batch 67/1400 loss 8.669328 loss_att 6.420670 loss_ctc 13.916197 lr 0.00088485 rank 0
I0626 04:05:03.215407 139724881797504 executor.py:115] TRAIN Batch 67/1500 loss 14.099020 loss_att 10.196333 loss_ctc 23.205288 lr 0.00088444 rank 0
I0626 04:05:08.794782 139724881797504 executor.py:152] CV Batch 67/0 loss 14.644924 loss_att 14.772475 loss_ctc 14.347303 history loss 13.017710 rank 0
I0626 04:05:12.843230 139724881797504 executor.py:152] CV Batch 67/100 loss 48.205658 loss_att 51.947426 loss_ctc 39.474869 history loss 32.263215 rank 0
I0626 04:05:15.822363 139724881797504 train.py:288] Epoch 67 CV info cv_loss 37.3323290253218
I0626 04:05:15.822522 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/67.pt
I0626 04:05:16.083417 139724881797504 train.py:274] Epoch 68 TRAIN info lr 0.0008844364220965313
I0626 04:05:16.086434 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:05:16.086493 139724881797504 executor.py:46] total epoch is 68.
I0626 04:05:23.883134 139724881797504 executor.py:115] TRAIN Batch 68/0 loss 6.666151 loss_att 4.414332 loss_ctc 11.920394 lr 0.00088437 rank 0
I0626 04:05:35.412147 139724881797504 executor.py:115] TRAIN Batch 68/100 loss 12.805834 loss_att 9.125965 loss_ctc 21.392193 lr 0.00088395 rank 0
I0626 04:05:46.542224 139724881797504 executor.py:115] TRAIN Batch 68/200 loss 10.978737 loss_att 6.858229 loss_ctc 20.593254 lr 0.00088354 rank 0
I0626 04:05:58.103642 139724881797504 executor.py:115] TRAIN Batch 68/300 loss 11.378643 loss_att 8.351534 loss_ctc 18.441898 lr 0.00088312 rank 0
I0626 04:06:09.796631 139724881797504 executor.py:115] TRAIN Batch 68/400 loss 8.849573 loss_att 5.532836 loss_ctc 16.588625 lr 0.00088264 rank 0
I0626 04:06:21.427722 139724881797504 executor.py:115] TRAIN Batch 68/500 loss 7.616935 loss_att 5.316510 loss_ctc 12.984592 lr 0.00088223 rank 0
I0626 04:06:32.236888 139724881797504 executor.py:115] TRAIN Batch 68/600 loss 14.916698 loss_att 10.951019 loss_ctc 24.169947 lr 0.00088182 rank 0
I0626 04:06:43.348574 139724881797504 executor.py:115] TRAIN Batch 68/700 loss 12.907479 loss_att 9.604686 loss_ctc 20.613997 lr 0.00088141 rank 0
I0626 04:06:55.005943 139724881797504 executor.py:115] TRAIN Batch 68/800 loss 10.482983 loss_att 7.873878 loss_ctc 16.570890 lr 0.00088093 rank 0
I0626 04:07:06.313195 139724881797504 executor.py:115] TRAIN Batch 68/900 loss 9.383023 loss_att 5.469960 loss_ctc 18.513504 lr 0.00088052 rank 0
I0626 04:07:17.679693 139724881797504 executor.py:115] TRAIN Batch 68/1000 loss 10.080435 loss_att 6.610282 loss_ctc 18.177454 lr 0.00088011 rank 0
I0626 04:07:28.424303 139724881797504 executor.py:115] TRAIN Batch 68/1100 loss 19.288694 loss_att 13.415354 loss_ctc 32.993156 lr 0.00087970 rank 0
I0626 04:07:40.054454 139724881797504 executor.py:115] TRAIN Batch 68/1200 loss 11.362226 loss_att 7.680017 loss_ctc 19.954050 lr 0.00087922 rank 0
I0626 04:07:51.397794 139724881797504 executor.py:115] TRAIN Batch 68/1300 loss 14.565147 loss_att 9.537522 loss_ctc 26.296272 lr 0.00087882 rank 0
I0626 04:08:02.534081 139724881797504 executor.py:115] TRAIN Batch 68/1400 loss 8.441550 loss_att 5.725678 loss_ctc 14.778584 lr 0.00087841 rank 0
I0626 04:08:13.263110 139724881797504 executor.py:115] TRAIN Batch 68/1500 loss 16.771013 loss_att 12.263632 loss_ctc 27.288235 lr 0.00087800 rank 0
I0626 04:08:18.836373 139724881797504 executor.py:152] CV Batch 68/0 loss 14.600435 loss_att 14.655768 loss_ctc 14.471323 history loss 12.978165 rank 0
I0626 04:08:22.821902 139724881797504 executor.py:152] CV Batch 68/100 loss 46.766201 loss_att 50.405872 loss_ctc 38.273628 history loss 31.893820 rank 0
I0626 04:08:25.804970 139724881797504 train.py:288] Epoch 68 CV info cv_loss 36.86102075570127
I0626 04:08:25.805144 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/68.pt
I0626 04:08:26.068548 139724881797504 train.py:274] Epoch 69 TRAIN info lr 0.0008780040718411253
I0626 04:08:26.071556 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:08:26.071625 139724881797504 executor.py:46] total epoch is 69.
I0626 04:08:33.803213 139724881797504 executor.py:115] TRAIN Batch 69/0 loss 9.306490 loss_att 5.714321 loss_ctc 17.688215 lr 0.00087794 rank 0
I0626 04:08:44.964947 139724881797504 executor.py:115] TRAIN Batch 69/100 loss 10.564288 loss_att 8.380162 loss_ctc 15.660582 lr 0.00087753 rank 0
I0626 04:08:55.751433 139724881797504 executor.py:115] TRAIN Batch 69/200 loss 12.749008 loss_att 8.652939 loss_ctc 22.306501 lr 0.00087713 rank 0
I0626 04:09:06.909607 139724881797504 executor.py:115] TRAIN Batch 69/300 loss 11.610476 loss_att 7.954965 loss_ctc 20.139999 lr 0.00087672 rank 0
I0626 04:09:18.533086 139724881797504 executor.py:115] TRAIN Batch 69/400 loss 12.099650 loss_att 8.628326 loss_ctc 20.199406 lr 0.00087625 rank 0
I0626 04:09:30.196412 139724881797504 executor.py:115] TRAIN Batch 69/500 loss 5.909551 loss_att 4.671696 loss_ctc 8.797877 lr 0.00087585 rank 0
I0626 04:09:41.111006 139724881797504 executor.py:115] TRAIN Batch 69/600 loss 16.771545 loss_att 11.913898 loss_ctc 28.106058 lr 0.00087544 rank 0
I0626 04:09:52.601262 139724881797504 executor.py:115] TRAIN Batch 69/700 loss 11.453730 loss_att 7.240921 loss_ctc 21.283615 lr 0.00087504 rank 0
I0626 04:10:04.424409 139724881797504 executor.py:115] TRAIN Batch 69/800 loss 10.506391 loss_att 7.326237 loss_ctc 17.926750 lr 0.00087457 rank 0
I0626 04:10:16.106178 139724881797504 executor.py:115] TRAIN Batch 69/900 loss 10.883394 loss_att 7.259092 loss_ctc 19.340099 lr 0.00087417 rank 0
I0626 04:10:27.175497 139724881797504 executor.py:115] TRAIN Batch 69/1000 loss 6.679933 loss_att 5.383960 loss_ctc 9.703870 lr 0.00087377 rank 0
I0626 04:10:37.556621 139724881797504 executor.py:115] TRAIN Batch 69/1100 loss 14.995989 loss_att 11.360085 loss_ctc 23.479763 lr 0.00087337 rank 0
I0626 04:10:48.909847 139724881797504 executor.py:115] TRAIN Batch 69/1200 loss 15.967775 loss_att 9.671909 loss_ctc 30.658131 lr 0.00087290 rank 0
I0626 04:11:00.006064 139724881797504 executor.py:115] TRAIN Batch 69/1300 loss 13.221121 loss_att 7.578046 loss_ctc 26.388294 lr 0.00087251 rank 0
I0626 04:11:11.254183 139724881797504 executor.py:115] TRAIN Batch 69/1400 loss 13.944471 loss_att 9.158886 loss_ctc 25.110836 lr 0.00087211 rank 0
I0626 04:11:22.220861 139724881797504 executor.py:115] TRAIN Batch 69/1500 loss 9.523098 loss_att 6.561649 loss_ctc 16.433144 lr 0.00087171 rank 0
I0626 04:11:27.782771 139724881797504 executor.py:152] CV Batch 69/0 loss 14.463230 loss_att 14.290580 loss_ctc 14.866083 history loss 12.856205 rank 0
I0626 04:11:31.714604 139724881797504 executor.py:152] CV Batch 69/100 loss 47.045818 loss_att 50.812584 loss_ctc 38.256699 history loss 31.033286 rank 0
I0626 04:11:34.706983 139724881797504 train.py:288] Epoch 69 CV info cv_loss 36.053540146608746
I0626 04:11:34.707169 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/69.pt
I0626 04:11:34.968097 139724881797504 train.py:274] Epoch 70 TRAIN info lr 0.0008717100546930083
I0626 04:11:34.971009 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:11:34.971068 139724881797504 executor.py:46] total epoch is 70.
I0626 04:11:42.679335 139724881797504 executor.py:115] TRAIN Batch 70/0 loss 9.380287 loss_att 5.955817 loss_ctc 17.370716 lr 0.00087164 rank 0
I0626 04:11:53.768675 139724881797504 executor.py:115] TRAIN Batch 70/100 loss 14.468801 loss_att 9.493200 loss_ctc 26.078533 lr 0.00087125 rank 0
I0626 04:12:04.445451 139724881797504 executor.py:115] TRAIN Batch 70/200 loss 13.382578 loss_att 7.907436 loss_ctc 26.157906 lr 0.00087085 rank 0
I0626 04:12:15.617595 139724881797504 executor.py:115] TRAIN Batch 70/300 loss 11.156670 loss_att 7.031827 loss_ctc 20.781300 lr 0.00087045 rank 0
I0626 04:12:27.130836 139724881797504 executor.py:115] TRAIN Batch 70/400 loss 13.121515 loss_att 8.148300 loss_ctc 24.725681 lr 0.00086999 rank 0
I0626 04:12:38.602474 139724881797504 executor.py:115] TRAIN Batch 70/500 loss 4.718790 loss_att 2.832677 loss_ctc 9.119720 lr 0.00086960 rank 0
I0626 04:12:49.238952 139724881797504 executor.py:115] TRAIN Batch 70/600 loss 11.607243 loss_att 9.038563 loss_ctc 17.600830 lr 0.00086920 rank 0
I0626 04:13:00.253117 139724881797504 executor.py:115] TRAIN Batch 70/700 loss 12.644245 loss_att 8.913532 loss_ctc 21.349243 lr 0.00086881 rank 0
I0626 04:13:11.557293 139724881797504 executor.py:115] TRAIN Batch 70/800 loss 13.492596 loss_att 8.515572 loss_ctc 25.105652 lr 0.00086835 rank 0
I0626 04:13:22.977437 139724881797504 executor.py:115] TRAIN Batch 70/900 loss 9.339091 loss_att 7.321449 loss_ctc 14.046923 lr 0.00086796 rank 0
I0626 04:13:34.045889 139724881797504 executor.py:115] TRAIN Batch 70/1000 loss 6.665375 loss_att 5.144897 loss_ctc 10.213159 lr 0.00086757 rank 0
I0626 04:13:45.003718 139724881797504 executor.py:115] TRAIN Batch 70/1100 loss 12.988600 loss_att 9.841099 loss_ctc 20.332767 lr 0.00086718 rank 0
I0626 04:13:56.241189 139724881797504 executor.py:115] TRAIN Batch 70/1200 loss 13.379942 loss_att 10.144597 loss_ctc 20.929079 lr 0.00086672 rank 0
I0626 04:14:07.680961 139724881797504 executor.py:115] TRAIN Batch 70/1300 loss 14.649639 loss_att 8.951246 loss_ctc 27.945885 lr 0.00086633 rank 0
I0626 04:14:19.447245 139724881797504 executor.py:115] TRAIN Batch 70/1400 loss 12.314243 loss_att 7.539120 loss_ctc 23.456200 lr 0.00086594 rank 0
I0626 04:14:30.553055 139724881797504 executor.py:115] TRAIN Batch 70/1500 loss 11.802080 loss_att 7.664878 loss_ctc 21.455549 lr 0.00086555 rank 0
I0626 04:14:36.125587 139724881797504 executor.py:152] CV Batch 70/0 loss 15.256085 loss_att 15.608100 loss_ctc 14.434719 history loss 13.560965 rank 0
I0626 04:14:40.069627 139724881797504 executor.py:152] CV Batch 70/100 loss 45.579582 loss_att 48.860909 loss_ctc 37.923149 history loss 32.003212 rank 0
I0626 04:14:43.041976 139724881797504 train.py:288] Epoch 70 CV info cv_loss 36.61697143852835
I0626 04:14:43.042229 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/70.pt
I0626 04:14:43.306853 139724881797504 train.py:274] Epoch 71 TRAIN info lr 0.0008655494824115181
I0626 04:14:43.309706 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:14:43.309777 139724881797504 executor.py:46] total epoch is 71.
I0626 04:14:51.081232 139724881797504 executor.py:115] TRAIN Batch 71/0 loss 4.695995 loss_att 3.562235 loss_ctc 7.341437 lr 0.00086548 rank 0
I0626 04:15:01.747182 139724881797504 executor.py:115] TRAIN Batch 71/100 loss 13.747123 loss_att 10.916169 loss_ctc 20.352680 lr 0.00086510 rank 0
I0626 04:15:12.400419 139724881797504 executor.py:115] TRAIN Batch 71/200 loss 11.538881 loss_att 7.432046 loss_ctc 21.121494 lr 0.00086471 rank 0
I0626 04:15:23.842806 139724881797504 executor.py:115] TRAIN Batch 71/300 loss 13.729452 loss_att 8.115528 loss_ctc 26.828608 lr 0.00086432 rank 0
I0626 04:15:35.207847 139724881797504 executor.py:115] TRAIN Batch 71/400 loss 9.780422 loss_att 6.759193 loss_ctc 16.829956 lr 0.00086387 rank 0
I0626 04:15:46.899204 139724881797504 executor.py:115] TRAIN Batch 71/500 loss 9.029099 loss_att 6.198585 loss_ctc 15.633628 lr 0.00086348 rank 0
I0626 04:15:57.495310 139724881797504 executor.py:115] TRAIN Batch 71/600 loss 15.015671 loss_att 11.031693 loss_ctc 24.311621 lr 0.00086310 rank 0
I0626 04:16:08.921945 139724881797504 executor.py:115] TRAIN Batch 71/700 loss 13.914045 loss_att 9.136987 loss_ctc 25.060513 lr 0.00086271 rank 0
I0626 04:16:20.459060 139724881797504 executor.py:115] TRAIN Batch 71/800 loss 10.094279 loss_att 7.283288 loss_ctc 16.653259 lr 0.00086226 rank 0
I0626 04:16:31.900382 139724881797504 executor.py:115] TRAIN Batch 71/900 loss 12.816577 loss_att 7.640701 loss_ctc 24.893620 lr 0.00086188 rank 0
I0626 04:16:43.191400 139724881797504 executor.py:115] TRAIN Batch 71/1000 loss 10.711519 loss_att 7.161259 loss_ctc 18.995459 lr 0.00086149 rank 0
I0626 04:16:53.867916 139724881797504 executor.py:115] TRAIN Batch 71/1100 loss 12.375593 loss_att 9.476500 loss_ctc 19.140146 lr 0.00086111 rank 0
I0626 04:17:05.082175 139724881797504 executor.py:115] TRAIN Batch 71/1200 loss 11.213869 loss_att 7.932679 loss_ctc 18.869980 lr 0.00086066 rank 0
I0626 04:17:16.030292 139724881797504 executor.py:115] TRAIN Batch 71/1300 loss 11.232721 loss_att 6.404778 loss_ctc 22.497923 lr 0.00086028 rank 0
I0626 04:17:27.112761 139724881797504 executor.py:115] TRAIN Batch 71/1400 loss 9.867145 loss_att 6.706986 loss_ctc 17.240849 lr 0.00085990 rank 0
I0626 04:17:38.189178 139724881797504 executor.py:115] TRAIN Batch 71/1500 loss 9.898365 loss_att 6.387859 loss_ctc 18.089546 lr 0.00085952 rank 0
I0626 04:17:43.732244 139724881797504 executor.py:152] CV Batch 71/0 loss 14.791842 loss_att 15.037913 loss_ctc 14.217674 history loss 13.148304 rank 0
I0626 04:17:47.761713 139724881797504 executor.py:152] CV Batch 71/100 loss 45.073048 loss_att 48.168304 loss_ctc 37.850788 history loss 30.697204 rank 0
I0626 04:17:50.726357 139724881797504 train.py:288] Epoch 71 CV info cv_loss 35.705900007249255
I0626 04:17:50.726546 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/71.pt
I0626 04:17:50.986852 139724881797504 train.py:274] Epoch 72 TRAIN info lr 0.0008595177052156612
I0626 04:17:50.989902 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:17:50.989963 139724881797504 executor.py:46] total epoch is 72.
I0626 04:17:58.875503 139724881797504 executor.py:115] TRAIN Batch 72/0 loss 4.244129 loss_att 3.030873 loss_ctc 7.075060 lr 0.00085945 rank 0
I0626 04:18:10.266304 139724881797504 executor.py:115] TRAIN Batch 72/100 loss 12.082278 loss_att 8.264551 loss_ctc 20.990307 lr 0.00085907 rank 0
I0626 04:18:20.985477 139724881797504 executor.py:115] TRAIN Batch 72/200 loss 13.683766 loss_att 8.428538 loss_ctc 25.945965 lr 0.00085869 rank 0
I0626 04:18:32.164757 139724881797504 executor.py:115] TRAIN Batch 72/300 loss 7.300604 loss_att 4.966613 loss_ctc 12.746582 lr 0.00085831 rank 0
I0626 04:18:43.555289 139724881797504 executor.py:115] TRAIN Batch 72/400 loss 8.155116 loss_att 5.901599 loss_ctc 13.413322 lr 0.00085787 rank 0
I0626 04:18:55.026112 139724881797504 executor.py:115] TRAIN Batch 72/500 loss 6.635810 loss_att 4.750201 loss_ctc 11.035563 lr 0.00085749 rank 0
I0626 04:19:05.254515 139724881797504 executor.py:115] TRAIN Batch 72/600 loss 11.187883 loss_att 7.835438 loss_ctc 19.010258 lr 0.00085711 rank 0
I0626 04:19:16.656440 139724881797504 executor.py:115] TRAIN Batch 72/700 loss 12.572304 loss_att 7.959039 loss_ctc 23.336590 lr 0.00085674 rank 0
I0626 04:19:28.271494 139724881797504 executor.py:115] TRAIN Batch 72/800 loss 12.964206 loss_att 8.550179 loss_ctc 23.263601 lr 0.00085630 rank 0
I0626 04:19:39.516868 139724881797504 executor.py:115] TRAIN Batch 72/900 loss 8.137661 loss_att 5.865061 loss_ctc 13.440393 lr 0.00085592 rank 0
I0626 04:19:50.860746 139724881797504 executor.py:115] TRAIN Batch 72/1000 loss 8.337405 loss_att 5.617311 loss_ctc 14.684290 lr 0.00085555 rank 0
I0626 04:20:01.584157 139724881797504 executor.py:115] TRAIN Batch 72/1100 loss 11.014757 loss_att 7.507545 loss_ctc 19.198254 lr 0.00085517 rank 0
I0626 04:20:12.663347 139724881797504 executor.py:115] TRAIN Batch 72/1200 loss 11.972950 loss_att 8.528917 loss_ctc 20.009027 lr 0.00085473 rank 0
I0626 04:20:23.774399 139724881797504 executor.py:115] TRAIN Batch 72/1300 loss 11.283345 loss_att 6.707983 loss_ctc 21.959190 lr 0.00085436 rank 0
I0626 04:20:34.768008 139724881797504 executor.py:115] TRAIN Batch 72/1400 loss 11.666388 loss_att 7.438641 loss_ctc 21.531130 lr 0.00085398 rank 0
I0626 04:20:45.647961 139724881797504 executor.py:115] TRAIN Batch 72/1500 loss 11.987329 loss_att 7.458172 loss_ctc 22.555365 lr 0.00085361 rank 0
I0626 04:20:51.198530 139724881797504 executor.py:152] CV Batch 72/0 loss 14.733438 loss_att 14.490185 loss_ctc 15.301029 history loss 13.096390 rank 0
I0626 04:20:55.152977 139724881797504 executor.py:152] CV Batch 72/100 loss 43.820847 loss_att 46.089260 loss_ctc 38.527882 history loss 30.052899 rank 0
I0626 04:20:58.128389 139724881797504 train.py:288] Epoch 72 CV info cv_loss 35.03534364667042
I0626 04:20:58.128606 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/72.pt
I0626 04:20:58.385962 139724881797504 train.py:274] Epoch 73 TRAIN info lr 0.0008536102970333705
I0626 04:20:58.388867 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:20:58.388939 139724881797504 executor.py:46] total epoch is 73.
I0626 04:21:06.162744 139724881797504 executor.py:115] TRAIN Batch 73/0 loss 6.817757 loss_att 3.917645 loss_ctc 13.584684 lr 0.00085355 rank 0
I0626 04:21:17.197142 139724881797504 executor.py:115] TRAIN Batch 73/100 loss 13.162477 loss_att 8.490889 loss_ctc 24.062849 lr 0.00085318 rank 0
I0626 04:21:28.011449 139724881797504 executor.py:115] TRAIN Batch 73/200 loss 11.410341 loss_att 7.681800 loss_ctc 20.110270 lr 0.00085280 rank 0
I0626 04:21:39.022042 139724881797504 executor.py:115] TRAIN Batch 73/300 loss 10.461927 loss_att 5.927485 loss_ctc 21.042294 lr 0.00085243 rank 0
I0626 04:21:50.319362 139724881797504 executor.py:115] TRAIN Batch 73/400 loss 7.933068 loss_att 5.353956 loss_ctc 13.950996 lr 0.00085200 rank 0
I0626 04:22:01.437453 139724881797504 executor.py:115] TRAIN Batch 73/500 loss 7.877052 loss_att 4.759877 loss_ctc 15.150458 lr 0.00085163 rank 0
I0626 04:22:12.072323 139724881797504 executor.py:115] TRAIN Batch 73/600 loss 12.359703 loss_att 9.253546 loss_ctc 19.607403 lr 0.00085126 rank 0
I0626 04:22:23.190172 139724881797504 executor.py:115] TRAIN Batch 73/700 loss 11.281659 loss_att 7.275855 loss_ctc 20.628534 lr 0.00085089 rank 0
I0626 04:22:34.486423 139724881797504 executor.py:115] TRAIN Batch 73/800 loss 9.136106 loss_att 5.837425 loss_ctc 16.833027 lr 0.00085046 rank 0
I0626 04:22:45.898544 139724881797504 executor.py:115] TRAIN Batch 73/900 loss 12.907276 loss_att 8.880171 loss_ctc 22.303854 lr 0.00085009 rank 0
I0626 04:22:56.878654 139724881797504 executor.py:115] TRAIN Batch 73/1000 loss 7.542042 loss_att 5.762893 loss_ctc 11.693389 lr 0.00084972 rank 0
I0626 04:23:07.804321 139724881797504 executor.py:115] TRAIN Batch 73/1100 loss 9.640858 loss_att 7.537238 loss_ctc 14.549305 lr 0.00084935 rank 0
I0626 04:23:19.132132 139724881797504 executor.py:115] TRAIN Batch 73/1200 loss 11.530798 loss_att 7.549523 loss_ctc 20.820440 lr 0.00084892 rank 0
I0626 04:23:30.620106 139724881797504 executor.py:115] TRAIN Batch 73/1300 loss 9.456221 loss_att 6.699934 loss_ctc 15.887556 lr 0.00084856 rank 0
I0626 04:23:41.721014 139724881797504 executor.py:115] TRAIN Batch 73/1400 loss 12.171832 loss_att 7.934838 loss_ctc 22.058151 lr 0.00084819 rank 0
I0626 04:23:52.713683 139724881797504 executor.py:115] TRAIN Batch 73/1500 loss 10.656969 loss_att 8.061643 loss_ctc 16.712729 lr 0.00084782 rank 0
I0626 04:23:58.304147 139724881797504 executor.py:152] CV Batch 73/0 loss 15.835996 loss_att 15.469880 loss_ctc 16.690266 history loss 14.076441 rank 0
I0626 04:24:02.498975 139724881797504 executor.py:152] CV Batch 73/100 loss 45.604477 loss_att 48.314911 loss_ctc 39.280125 history loss 30.349649 rank 0
I0626 04:24:05.478148 139724881797504 train.py:288] Epoch 73 CV info cv_loss 35.188435125251125
I0626 04:24:05.478397 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/73.pt
I0626 04:24:05.739908 139724881797504 train.py:274] Epoch 74 TRAIN info lr 0.0008478230418508758
I0626 04:24:05.742973 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:24:05.743032 139724881797504 executor.py:46] total epoch is 74.
I0626 04:24:13.533461 139724881797504 executor.py:115] TRAIN Batch 74/0 loss 5.934615 loss_att 4.262409 loss_ctc 9.836429 lr 0.00084776 rank 0
I0626 04:24:24.306391 139724881797504 executor.py:115] TRAIN Batch 74/100 loss 8.926583 loss_att 6.600488 loss_ctc 14.354139 lr 0.00084740 rank 0
I0626 04:24:35.306518 139724881797504 executor.py:115] TRAIN Batch 74/200 loss 7.637175 loss_att 4.928088 loss_ctc 13.958375 lr 0.00084703 rank 0
I0626 04:24:46.725993 139724881797504 executor.py:115] TRAIN Batch 74/300 loss 8.800017 loss_att 5.430106 loss_ctc 16.663143 lr 0.00084667 rank 0
I0626 04:24:58.515093 139724881797504 executor.py:115] TRAIN Batch 74/400 loss 8.414183 loss_att 4.950394 loss_ctc 16.496355 lr 0.00084624 rank 0
I0626 04:25:09.912346 139724881797504 executor.py:115] TRAIN Batch 74/500 loss 6.774793 loss_att 4.701881 loss_ctc 11.611587 lr 0.00084588 rank 0
I0626 04:25:20.085167 139724881797504 executor.py:115] TRAIN Batch 74/600 loss 12.497688 loss_att 8.804464 loss_ctc 21.115211 lr 0.00084552 rank 0
I0626 04:25:31.113057 139724881797504 executor.py:115] TRAIN Batch 74/700 loss 14.551075 loss_att 8.874699 loss_ctc 27.795952 lr 0.00084515 rank 0
I0626 04:25:42.487929 139724881797504 executor.py:115] TRAIN Batch 74/800 loss 9.472599 loss_att 7.286267 loss_ctc 14.574038 lr 0.00084473 rank 0
I0626 04:25:54.067889 139724881797504 executor.py:115] TRAIN Batch 74/900 loss 13.226069 loss_att 8.937078 loss_ctc 23.233715 lr 0.00084437 rank 0
I0626 04:26:05.539632 139724881797504 executor.py:115] TRAIN Batch 74/1000 loss 6.873169 loss_att 4.932022 loss_ctc 11.402514 lr 0.00084401 rank 0
I0626 04:26:16.363576 139724881797504 executor.py:115] TRAIN Batch 74/1100 loss 15.392206 loss_att 11.287269 loss_ctc 24.970396 lr 0.00084365 rank 0
I0626 04:26:27.701327 139724881797504 executor.py:115] TRAIN Batch 74/1200 loss 10.526316 loss_att 6.724898 loss_ctc 19.396288 lr 0.00084323 rank 0
I0626 04:26:39.191838 139724881797504 executor.py:115] TRAIN Batch 74/1300 loss 13.806431 loss_att 7.490355 loss_ctc 28.543940 lr 0.00084287 rank 0
I0626 04:26:50.804440 139724881797504 executor.py:115] TRAIN Batch 74/1400 loss 11.101820 loss_att 7.855234 loss_ctc 18.677185 lr 0.00084251 rank 0
I0626 04:27:01.693643 139724881797504 executor.py:115] TRAIN Batch 74/1500 loss 9.552137 loss_att 7.663235 loss_ctc 13.959579 lr 0.00084215 rank 0
I0626 04:27:07.283211 139724881797504 executor.py:152] CV Batch 74/0 loss 15.262829 loss_att 14.761115 loss_ctc 16.433493 history loss 13.566959 rank 0
I0626 04:27:11.255535 139724881797504 executor.py:152] CV Batch 74/100 loss 43.866951 loss_att 46.487885 loss_ctc 37.751431 history loss 30.122298 rank 0
I0626 04:27:14.240026 139724881797504 train.py:288] Epoch 74 CV info cv_loss 34.70884677437079
I0626 04:27:14.240447 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/74.pt
I0626 04:27:14.504482 139724881797504 train.py:274] Epoch 75 TRAIN info lr 0.0008421519210665189
I0626 04:27:14.507475 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:27:14.507543 139724881797504 executor.py:46] total epoch is 75.
I0626 04:27:22.266950 139724881797504 executor.py:115] TRAIN Batch 75/0 loss 7.540293 loss_att 4.752633 loss_ctc 14.044832 lr 0.00084209 rank 0
I0626 04:27:33.066866 139724881797504 executor.py:115] TRAIN Batch 75/100 loss 13.809479 loss_att 9.489044 loss_ctc 23.890491 lr 0.00084173 rank 0
I0626 04:27:43.965531 139724881797504 executor.py:115] TRAIN Batch 75/200 loss 11.147240 loss_att 6.928167 loss_ctc 20.991743 lr 0.00084138 rank 0
I0626 04:27:55.187844 139724881797504 executor.py:115] TRAIN Batch 75/300 loss 10.209658 loss_att 6.681717 loss_ctc 18.441519 lr 0.00084102 rank 0
I0626 04:28:06.843177 139724881797504 executor.py:115] TRAIN Batch 75/400 loss 9.470173 loss_att 6.036706 loss_ctc 17.481596 lr 0.00084060 rank 0
I0626 04:28:18.384228 139724881797504 executor.py:115] TRAIN Batch 75/500 loss 8.317698 loss_att 6.446025 loss_ctc 12.684935 lr 0.00084025 rank 0
I0626 04:28:28.665570 139724881797504 executor.py:115] TRAIN Batch 75/600 loss 15.908899 loss_att 11.428152 loss_ctc 26.363976 lr 0.00083989 rank 0
I0626 04:28:39.514817 139724881797504 executor.py:115] TRAIN Batch 75/700 loss 12.127821 loss_att 8.185869 loss_ctc 21.325710 lr 0.00083954 rank 0
I0626 04:28:51.214136 139724881797504 executor.py:115] TRAIN Batch 75/800 loss 12.626656 loss_att 6.817928 loss_ctc 26.180351 lr 0.00083912 rank 0
I0626 04:29:02.608966 139724881797504 executor.py:115] TRAIN Batch 75/900 loss 7.644081 loss_att 5.396443 loss_ctc 12.888569 lr 0.00083877 rank 0
I0626 04:29:13.565999 139724881797504 executor.py:115] TRAIN Batch 75/1000 loss 5.053023 loss_att 3.749380 loss_ctc 8.094855 lr 0.00083841 rank 0
I0626 04:29:24.328429 139724881797504 executor.py:115] TRAIN Batch 75/1100 loss 11.849463 loss_att 7.755889 loss_ctc 21.401131 lr 0.00083806 rank 0
I0626 04:29:35.575772 139724881797504 executor.py:115] TRAIN Batch 75/1200 loss 14.007397 loss_att 9.214238 loss_ctc 25.191433 lr 0.00083765 rank 0
I0626 04:29:46.645048 139724881797504 executor.py:115] TRAIN Batch 75/1300 loss 9.526073 loss_att 6.396495 loss_ctc 16.828423 lr 0.00083730 rank 0
I0626 04:29:57.693829 139724881797504 executor.py:115] TRAIN Batch 75/1400 loss 9.666836 loss_att 5.879073 loss_ctc 18.504951 lr 0.00083694 rank 0
I0626 04:30:08.791619 139724881797504 executor.py:115] TRAIN Batch 75/1500 loss 14.905380 loss_att 9.505126 loss_ctc 27.505972 lr 0.00083659 rank 0
I0626 04:30:14.368781 139724881797504 executor.py:152] CV Batch 75/0 loss 14.758088 loss_att 14.716471 loss_ctc 14.855196 history loss 13.118301 rank 0
I0626 04:30:18.418646 139724881797504 executor.py:152] CV Batch 75/100 loss 42.249920 loss_att 45.725426 loss_ctc 34.140411 history loss 29.944881 rank 0
I0626 04:30:21.424979 139724881797504 train.py:288] Epoch 75 CV info cv_loss 34.91732068271464
I0626 04:30:21.425092 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/75.pt
I0626 04:30:21.681297 139724881797504 train.py:274] Epoch 76 TRAIN info lr 0.0008365931017628183
I0626 04:30:21.684344 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:30:21.684414 139724881797504 executor.py:46] total epoch is 76.
I0626 04:30:29.527934 139724881797504 executor.py:115] TRAIN Batch 76/0 loss 4.870717 loss_att 3.929041 loss_ctc 7.067960 lr 0.00083653 rank 0
I0626 04:30:40.313467 139724881797504 executor.py:115] TRAIN Batch 76/100 loss 14.148076 loss_att 8.177331 loss_ctc 28.079813 lr 0.00083618 rank 0
I0626 04:30:51.428995 139724881797504 executor.py:115] TRAIN Batch 76/200 loss 10.564560 loss_att 6.801068 loss_ctc 19.346043 lr 0.00083583 rank 0
I0626 04:31:03.025312 139724881797504 executor.py:115] TRAIN Batch 76/300 loss 9.906756 loss_att 5.774737 loss_ctc 19.548134 lr 0.00083548 rank 0
I0626 04:31:14.578722 139724881797504 executor.py:115] TRAIN Batch 76/400 loss 8.638433 loss_att 6.088538 loss_ctc 14.588190 lr 0.00083507 rank 0
I0626 04:31:26.165825 139724881797504 executor.py:115] TRAIN Batch 76/500 loss 9.214321 loss_att 6.469761 loss_ctc 15.618293 lr 0.00083473 rank 0
I0626 04:31:36.444347 139724881797504 executor.py:115] TRAIN Batch 76/600 loss 17.859634 loss_att 11.809975 loss_ctc 31.975502 lr 0.00083438 rank 0
I0626 04:31:47.411706 139724881797504 executor.py:115] TRAIN Batch 76/700 loss 11.886168 loss_att 7.929757 loss_ctc 21.117794 lr 0.00083403 rank 0
I0626 04:31:59.006543 139724881797504 executor.py:115] TRAIN Batch 76/800 loss 9.098531 loss_att 7.228459 loss_ctc 13.462030 lr 0.00083362 rank 0
I0626 04:32:10.642465 139724881797504 executor.py:115] TRAIN Batch 76/900 loss 9.745444 loss_att 6.074843 loss_ctc 18.310179 lr 0.00083328 rank 0
I0626 04:32:22.306045 139724881797504 executor.py:115] TRAIN Batch 76/1000 loss 8.311554 loss_att 5.341406 loss_ctc 15.241900 lr 0.00083293 rank 0
I0626 04:32:33.323664 139724881797504 executor.py:115] TRAIN Batch 76/1100 loss 13.242144 loss_att 10.259741 loss_ctc 20.201084 lr 0.00083258 rank 0
I0626 04:32:44.861181 139724881797504 executor.py:115] TRAIN Batch 76/1200 loss 13.824867 loss_att 9.144302 loss_ctc 24.746185 lr 0.00083218 rank 0
I0626 04:32:55.905074 139724881797504 executor.py:115] TRAIN Batch 76/1300 loss 10.053549 loss_att 6.500528 loss_ctc 18.343931 lr 0.00083183 rank 0
I0626 04:33:07.192166 139724881797504 executor.py:115] TRAIN Batch 76/1400 loss 9.349047 loss_att 6.808019 loss_ctc 15.278112 lr 0.00083149 rank 0
I0626 04:33:18.490320 139724881797504 executor.py:115] TRAIN Batch 76/1500 loss 12.469895 loss_att 8.895447 loss_ctc 20.810274 lr 0.00083114 rank 0
I0626 04:33:24.060761 139724881797504 executor.py:152] CV Batch 76/0 loss 14.579547 loss_att 13.796146 loss_ctc 16.407482 history loss 12.959597 rank 0
I0626 04:33:28.152679 139724881797504 executor.py:152] CV Batch 76/100 loss 43.338352 loss_att 46.195282 loss_ctc 36.672184 history loss 29.978709 rank 0
I0626 04:33:31.146699 139724881797504 train.py:288] Epoch 76 CV info cv_loss 34.66620801032546
I0626 04:33:31.146842 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/76.pt
I0626 04:33:31.406231 139724881797504 train.py:274] Epoch 77 TRAIN info lr 0.0008311429258190126
I0626 04:33:31.409238 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:33:31.409298 139724881797504 executor.py:46] total epoch is 77.
I0626 04:33:39.183635 139724881797504 executor.py:115] TRAIN Batch 77/0 loss 5.257189 loss_att 3.473461 loss_ctc 9.419218 lr 0.00083109 rank 0
I0626 04:33:50.047752 139724881797504 executor.py:115] TRAIN Batch 77/100 loss 11.843053 loss_att 8.681064 loss_ctc 19.221027 lr 0.00083074 rank 0
I0626 04:34:00.755858 139724881797504 executor.py:115] TRAIN Batch 77/200 loss 11.062851 loss_att 8.367243 loss_ctc 17.352604 lr 0.00083040 rank 0
I0626 04:34:11.788739 139724881797504 executor.py:115] TRAIN Batch 77/300 loss 7.697486 loss_att 4.973413 loss_ctc 14.053654 lr 0.00083005 rank 0
I0626 04:34:23.448354 139724881797504 executor.py:115] TRAIN Batch 77/400 loss 11.878799 loss_att 7.552237 loss_ctc 21.974113 lr 0.00082965 rank 0
I0626 04:34:35.021302 139724881797504 executor.py:115] TRAIN Batch 77/500 loss 5.435878 loss_att 3.240231 loss_ctc 10.559053 lr 0.00082931 rank 0
I0626 04:34:45.555242 139724881797504 executor.py:115] TRAIN Batch 77/600 loss 12.072090 loss_att 8.091520 loss_ctc 21.360088 lr 0.00082897 rank 0
I0626 04:34:57.400394 139724881797504 executor.py:115] TRAIN Batch 77/700 loss 10.793879 loss_att 7.401865 loss_ctc 18.708578 lr 0.00082863 rank 0
I0626 04:35:08.985707 139724881797504 executor.py:115] TRAIN Batch 77/800 loss 10.000340 loss_att 6.507926 loss_ctc 18.149307 lr 0.00082823 rank 0
I0626 04:35:20.727535 139724881797504 executor.py:115] TRAIN Batch 77/900 loss 9.758529 loss_att 6.777468 loss_ctc 16.714336 lr 0.00082789 rank 0
I0626 04:35:32.312536 139724881797504 executor.py:115] TRAIN Batch 77/1000 loss 6.567030 loss_att 4.665849 loss_ctc 11.003120 lr 0.00082755 rank 0
I0626 04:35:43.000831 139724881797504 executor.py:115] TRAIN Batch 77/1100 loss 15.998196 loss_att 11.974602 loss_ctc 25.386581 lr 0.00082721 rank 0
I0626 04:35:54.428092 139724881797504 executor.py:115] TRAIN Batch 77/1200 loss 9.217993 loss_att 6.775028 loss_ctc 14.918243 lr 0.00082681 rank 0
I0626 04:36:05.623861 139724881797504 executor.py:115] TRAIN Batch 77/1300 loss 11.617999 loss_att 8.146191 loss_ctc 19.718885 lr 0.00082647 rank 0
I0626 04:36:16.625691 139724881797504 executor.py:115] TRAIN Batch 77/1400 loss 10.054082 loss_att 6.566136 loss_ctc 18.192623 lr 0.00082614 rank 0
I0626 04:36:27.639176 139724881797504 executor.py:115] TRAIN Batch 77/1500 loss 14.534239 loss_att 7.942471 loss_ctc 29.915031 lr 0.00082580 rank 0
I0626 04:36:33.198674 139724881797504 executor.py:152] CV Batch 77/0 loss 14.431808 loss_att 13.576838 loss_ctc 16.426737 history loss 12.828274 rank 0
I0626 04:36:37.188197 139724881797504 executor.py:152] CV Batch 77/100 loss 39.863064 loss_att 43.329956 loss_ctc 31.773653 history loss 29.174584 rank 0
I0626 04:36:40.165532 139724881797504 train.py:288] Epoch 77 CV info cv_loss 34.24016556669929
I0626 04:36:40.165771 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/77.pt
I0626 04:36:40.423050 139724881797504 train.py:274] Epoch 78 TRAIN info lr 0.0008257978997938135
I0626 04:36:40.426028 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:36:40.426282 139724881797504 executor.py:46] total epoch is 78.
I0626 04:36:48.255389 139724881797504 executor.py:115] TRAIN Batch 78/0 loss 4.318411 loss_att 2.508535 loss_ctc 8.541454 lr 0.00082574 rank 0
I0626 04:36:59.048929 139724881797504 executor.py:115] TRAIN Batch 78/100 loss 11.919704 loss_att 8.494259 loss_ctc 19.912411 lr 0.00082540 rank 0
I0626 04:37:10.198162 139724881797504 executor.py:115] TRAIN Batch 78/200 loss 10.572395 loss_att 8.204510 loss_ctc 16.097462 lr 0.00082507 rank 0
I0626 04:37:21.864963 139724881797504 executor.py:115] TRAIN Batch 78/300 loss 6.163717 loss_att 4.558125 loss_ctc 9.910097 lr 0.00082473 rank 0
I0626 04:37:33.452568 139724881797504 executor.py:115] TRAIN Batch 78/400 loss 7.643733 loss_att 5.067873 loss_ctc 13.654073 lr 0.00082434 rank 0
I0626 04:37:44.940763 139724881797504 executor.py:115] TRAIN Batch 78/500 loss 7.386569 loss_att 5.324578 loss_ctc 12.197882 lr 0.00082400 rank 0
I0626 04:37:55.485131 139724881797504 executor.py:115] TRAIN Batch 78/600 loss 12.652561 loss_att 8.476755 loss_ctc 22.396107 lr 0.00082367 rank 0
I0626 04:38:06.537774 139724881797504 executor.py:115] TRAIN Batch 78/700 loss 10.849296 loss_att 8.407015 loss_ctc 16.547951 lr 0.00082333 rank 0
I0626 04:38:18.062252 139724881797504 executor.py:115] TRAIN Batch 78/800 loss 9.075720 loss_att 6.002483 loss_ctc 16.246603 lr 0.00082294 rank 0
I0626 04:38:29.616119 139724881797504 executor.py:115] TRAIN Batch 78/900 loss 5.950180 loss_att 3.803983 loss_ctc 10.957973 lr 0.00082261 rank 0
I0626 04:38:40.832787 139724881797504 executor.py:115] TRAIN Batch 78/1000 loss 4.004198 loss_att 3.279515 loss_ctc 5.695124 lr 0.00082227 rank 0
I0626 04:38:51.547421 139724881797504 executor.py:115] TRAIN Batch 78/1100 loss 8.085682 loss_att 5.715835 loss_ctc 13.615324 lr 0.00082194 rank 0
I0626 04:39:02.409603 139724881797504 executor.py:115] TRAIN Batch 78/1200 loss 10.861904 loss_att 7.490413 loss_ctc 18.728716 lr 0.00082155 rank 0
I0626 04:39:13.705949 139724881797504 executor.py:115] TRAIN Batch 78/1300 loss 9.325162 loss_att 5.833971 loss_ctc 17.471275 lr 0.00082122 rank 0
I0626 04:39:25.237801 139724881797504 executor.py:115] TRAIN Batch 78/1400 loss 11.476892 loss_att 7.433678 loss_ctc 20.911060 lr 0.00082089 rank 0
I0626 04:39:36.270733 139724881797504 executor.py:115] TRAIN Batch 78/1500 loss 7.145390 loss_att 4.754155 loss_ctc 12.724936 lr 0.00082055 rank 0
I0626 04:39:41.842953 139724881797504 executor.py:152] CV Batch 78/0 loss 16.524460 loss_att 15.601502 loss_ctc 18.678026 history loss 14.688409 rank 0
I0626 04:39:45.806364 139724881797504 executor.py:152] CV Batch 78/100 loss 41.422810 loss_att 43.765640 loss_ctc 35.956200 history loss 30.113128 rank 0
I0626 04:39:48.791401 139724881797504 train.py:288] Epoch 78 CV info cv_loss 34.97809421408867
I0626 04:39:48.791648 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/78.pt
I0626 04:39:49.049870 139724881797504 train.py:274] Epoch 79 TRAIN info lr 0.0008205546855147925
I0626 04:39:49.052697 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:39:49.052763 139724881797504 executor.py:46] total epoch is 79.
I0626 04:39:56.756520 139724881797504 executor.py:115] TRAIN Batch 79/0 loss 5.725712 loss_att 4.238351 loss_ctc 9.196220 lr 0.00082050 rank 0
I0626 04:40:07.567264 139724881797504 executor.py:115] TRAIN Batch 79/100 loss 10.925913 loss_att 7.876264 loss_ctc 18.041759 lr 0.00082017 rank 0
I0626 04:40:18.649484 139724881797504 executor.py:115] TRAIN Batch 79/200 loss 9.784187 loss_att 6.428441 loss_ctc 17.614264 lr 0.00081984 rank 0
I0626 04:40:30.056308 139724881797504 executor.py:115] TRAIN Batch 79/300 loss 8.458170 loss_att 5.450242 loss_ctc 15.476666 lr 0.00081951 rank 0
I0626 04:40:41.774399 139724881797504 executor.py:115] TRAIN Batch 79/400 loss 8.775204 loss_att 6.469939 loss_ctc 14.154154 lr 0.00081912 rank 0
I0626 04:40:53.463352 139724881797504 executor.py:115] TRAIN Batch 79/500 loss 8.402452 loss_att 5.038438 loss_ctc 16.251822 lr 0.00081879 rank 0
I0626 04:41:04.295794 139724881797504 executor.py:115] TRAIN Batch 79/600 loss 12.225067 loss_att 8.829741 loss_ctc 20.147491 lr 0.00081846 rank 0
I0626 04:41:15.647065 139724881797504 executor.py:115] TRAIN Batch 79/700 loss 11.018372 loss_att 8.069550 loss_ctc 17.898958 lr 0.00081813 rank 0
I0626 04:41:27.038426 139724881797504 executor.py:115] TRAIN Batch 79/800 loss 8.264583 loss_att 5.129008 loss_ctc 15.580922 lr 0.00081775 rank 0
I0626 04:41:38.325817 139724881797504 executor.py:115] TRAIN Batch 79/900 loss 7.959131 loss_att 5.781277 loss_ctc 13.040792 lr 0.00081742 rank 0
I0626 04:41:49.512885 139724881797504 executor.py:115] TRAIN Batch 79/1000 loss 4.877175 loss_att 3.580059 loss_ctc 7.903779 lr 0.00081710 rank 0
I0626 04:42:00.363368 139724881797504 executor.py:115] TRAIN Batch 79/1100 loss 12.208261 loss_att 9.252956 loss_ctc 19.103970 lr 0.00081677 rank 0
I0626 04:42:11.874071 139724881797504 executor.py:115] TRAIN Batch 79/1200 loss 8.892288 loss_att 6.473946 loss_ctc 14.535086 lr 0.00081639 rank 0
I0626 04:42:23.458307 139724881797504 executor.py:115] TRAIN Batch 79/1300 loss 8.299698 loss_att 5.890864 loss_ctc 13.920312 lr 0.00081606 rank 0
I0626 04:42:34.916900 139724881797504 executor.py:115] TRAIN Batch 79/1400 loss 11.669899 loss_att 7.079877 loss_ctc 22.379948 lr 0.00081574 rank 0
I0626 04:42:46.091622 139724881797504 executor.py:115] TRAIN Batch 79/1500 loss 5.231973 loss_att 3.937117 loss_ctc 8.253303 lr 0.00081541 rank 0
I0626 04:42:51.685580 139724881797504 executor.py:152] CV Batch 79/0 loss 14.797371 loss_att 14.282948 loss_ctc 15.997692 history loss 13.153219 rank 0
I0626 04:42:55.672747 139724881797504 executor.py:152] CV Batch 79/100 loss 40.306026 loss_att 42.317802 loss_ctc 35.611885 history loss 29.061825 rank 0
I0626 04:42:58.661523 139724881797504 train.py:288] Epoch 79 CV info cv_loss 33.877164692955255
I0626 04:42:58.661699 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/79.pt
I0626 04:42:58.921141 139724881797504 train.py:274] Epoch 80 TRAIN info lr 0.0008154100913168028
I0626 04:42:58.924267 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:42:58.924335 139724881797504 executor.py:46] total epoch is 80.
I0626 04:43:06.672585 139724881797504 executor.py:115] TRAIN Batch 80/0 loss 5.411950 loss_att 4.515457 loss_ctc 7.503766 lr 0.00081536 rank 0
I0626 04:43:17.341554 139724881797504 executor.py:115] TRAIN Batch 80/100 loss 9.903730 loss_att 7.882027 loss_ctc 14.621037 lr 0.00081503 rank 0
I0626 04:43:28.187560 139724881797504 executor.py:115] TRAIN Batch 80/200 loss 9.958904 loss_att 6.501054 loss_ctc 18.027222 lr 0.00081471 rank 0
I0626 04:43:39.061550 139724881797504 executor.py:115] TRAIN Batch 80/300 loss 10.433891 loss_att 6.875776 loss_ctc 18.736160 lr 0.00081438 rank 0
I0626 04:43:50.463241 139724881797504 executor.py:115] TRAIN Batch 80/400 loss 5.937556 loss_att 4.806323 loss_ctc 8.577102 lr 0.00081400 rank 0
I0626 04:44:01.573936 139724881797504 executor.py:115] TRAIN Batch 80/500 loss 5.449201 loss_att 3.675760 loss_ctc 9.587229 lr 0.00081368 rank 0
I0626 04:44:11.782924 139724881797504 executor.py:115] TRAIN Batch 80/600 loss 11.868635 loss_att 6.965665 loss_ctc 23.308897 lr 0.00081336 rank 0
I0626 04:44:22.700013 139724881797504 executor.py:115] TRAIN Batch 80/700 loss 10.580563 loss_att 6.625764 loss_ctc 19.808424 lr 0.00081304 rank 0
I0626 04:44:33.827060 139724881797504 executor.py:115] TRAIN Batch 80/800 loss 10.382435 loss_att 8.235538 loss_ctc 15.391858 lr 0.00081266 rank 0
I0626 04:44:45.133839 139724881797504 executor.py:115] TRAIN Batch 80/900 loss 10.062998 loss_att 6.159050 loss_ctc 19.172211 lr 0.00081234 rank 0
I0626 04:44:56.273406 139724881797504 executor.py:115] TRAIN Batch 80/1000 loss 5.976271 loss_att 3.641321 loss_ctc 11.424486 lr 0.00081202 rank 0
I0626 04:45:07.155369 139724881797504 executor.py:115] TRAIN Batch 80/1100 loss 10.229658 loss_att 7.974676 loss_ctc 15.491285 lr 0.00081169 rank 0
I0626 04:45:18.527019 139724881797504 executor.py:115] TRAIN Batch 80/1200 loss 11.355213 loss_att 7.629411 loss_ctc 20.048750 lr 0.00081132 rank 0
I0626 04:45:29.700811 139724881797504 executor.py:115] TRAIN Batch 80/1300 loss 7.663379 loss_att 5.468409 loss_ctc 12.784977 lr 0.00081100 rank 0
I0626 04:45:41.069688 139724881797504 executor.py:115] TRAIN Batch 80/1400 loss 5.552320 loss_att 3.411860 loss_ctc 10.546722 lr 0.00081068 rank 0
I0626 04:45:52.357923 139724881797504 executor.py:115] TRAIN Batch 80/1500 loss 10.653440 loss_att 7.425661 loss_ctc 18.184929 lr 0.00081036 rank 0
I0626 04:45:57.920668 139724881797504 executor.py:152] CV Batch 80/0 loss 15.073870 loss_att 14.877079 loss_ctc 15.533051 history loss 13.398995 rank 0
I0626 04:46:01.980407 139724881797504 executor.py:152] CV Batch 80/100 loss 41.027473 loss_att 43.733376 loss_ctc 34.713699 history loss 29.125634 rank 0
I0626 04:46:04.964190 139724881797504 train.py:288] Epoch 80 CV info cv_loss 33.82552719914822
I0626 04:46:04.964411 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/80.pt
I0626 04:46:05.230912 139724881797504 train.py:274] Epoch 81 TRAIN info lr 0.000810361063877192
I0626 04:46:05.233915 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:46:05.233976 139724881797504 executor.py:46] total epoch is 81.
I0626 04:46:13.058922 139724881797504 executor.py:115] TRAIN Batch 81/0 loss 5.503117 loss_att 3.472507 loss_ctc 10.241208 lr 0.00081031 rank 0
I0626 04:46:23.880152 139724881797504 executor.py:115] TRAIN Batch 81/100 loss 10.954500 loss_att 8.075123 loss_ctc 17.673048 lr 0.00080999 rank 0
I0626 04:46:34.553716 139724881797504 executor.py:115] TRAIN Batch 81/200 loss 12.007430 loss_att 7.973216 loss_ctc 21.420597 lr 0.00080967 rank 0
I0626 04:46:45.498931 139724881797504 executor.py:115] TRAIN Batch 81/300 loss 10.005349 loss_att 6.606885 loss_ctc 17.935097 lr 0.00080935 rank 0
I0626 04:46:56.659052 139724881797504 executor.py:115] TRAIN Batch 81/400 loss 6.493456 loss_att 4.139908 loss_ctc 11.985067 lr 0.00080898 rank 0
I0626 04:47:08.413701 139724881797504 executor.py:115] TRAIN Batch 81/500 loss 5.625886 loss_att 3.692906 loss_ctc 10.136172 lr 0.00080866 rank 0
I0626 04:47:18.805406 139724881797504 executor.py:115] TRAIN Batch 81/600 loss 9.645958 loss_att 8.637147 loss_ctc 11.999849 lr 0.00080835 rank 0
I0626 04:47:29.672857 139724881797504 executor.py:115] TRAIN Batch 81/700 loss 9.604570 loss_att 6.413690 loss_ctc 17.049961 lr 0.00080803 rank 0
I0626 04:47:40.807860 139724881797504 executor.py:115] TRAIN Batch 81/800 loss 8.760703 loss_att 4.829227 loss_ctc 17.934147 lr 0.00080766 rank 0
I0626 04:47:52.189326 139724881797504 executor.py:115] TRAIN Batch 81/900 loss 7.592916 loss_att 4.406199 loss_ctc 15.028587 lr 0.00080734 rank 0
I0626 04:48:03.642682 139724881797504 executor.py:115] TRAIN Batch 81/1000 loss 6.676899 loss_att 4.476774 loss_ctc 11.810525 lr 0.00080703 rank 0
I0626 04:48:14.149075 139724881797504 executor.py:115] TRAIN Batch 81/1100 loss 13.486435 loss_att 10.043606 loss_ctc 21.519703 lr 0.00080671 rank 0
I0626 04:48:25.202242 139724881797504 executor.py:115] TRAIN Batch 81/1200 loss 10.047177 loss_att 7.077088 loss_ctc 16.977385 lr 0.00080635 rank 0
I0626 04:48:36.191122 139724881797504 executor.py:115] TRAIN Batch 81/1300 loss 9.503249 loss_att 6.761472 loss_ctc 15.900729 lr 0.00080603 rank 0
I0626 04:48:47.316922 139724881797504 executor.py:115] TRAIN Batch 81/1400 loss 7.130670 loss_att 4.630535 loss_ctc 12.964318 lr 0.00080572 rank 0
I0626 04:48:58.067625 139724881797504 executor.py:115] TRAIN Batch 81/1500 loss 9.758709 loss_att 5.636018 loss_ctc 19.378323 lr 0.00080540 rank 0
I0626 04:49:03.637585 139724881797504 executor.py:152] CV Batch 81/0 loss 16.163935 loss_att 15.572469 loss_ctc 17.544020 history loss 14.367942 rank 0
I0626 04:49:07.661538 139724881797504 executor.py:152] CV Batch 81/100 loss 39.583920 loss_att 41.789742 loss_ctc 34.436996 history loss 29.256741 rank 0
I0626 04:49:10.659871 139724881797504 train.py:288] Epoch 81 CV info cv_loss 34.0880822170388
I0626 04:49:10.660015 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/81.pt
I0626 04:49:10.950248 139724881797504 train.py:274] Epoch 82 TRAIN info lr 0.0008054046806003505
I0626 04:49:10.953095 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:49:10.953161 139724881797504 executor.py:46] total epoch is 82.
I0626 04:49:18.797572 139724881797504 executor.py:115] TRAIN Batch 82/0 loss 6.438110 loss_att 4.168138 loss_ctc 11.734713 lr 0.00080535 rank 0
I0626 04:49:29.331940 139724881797504 executor.py:115] TRAIN Batch 82/100 loss 10.395304 loss_att 7.759943 loss_ctc 16.544477 lr 0.00080504 rank 0
I0626 04:49:40.055384 139724881797504 executor.py:115] TRAIN Batch 82/200 loss 12.881105 loss_att 10.044395 loss_ctc 19.500095 lr 0.00080473 rank 0
I0626 04:49:51.409745 139724881797504 executor.py:115] TRAIN Batch 82/300 loss 11.508636 loss_att 7.859007 loss_ctc 20.024437 lr 0.00080441 rank 0
I0626 04:50:02.747062 139724881797504 executor.py:115] TRAIN Batch 82/400 loss 5.856788 loss_att 4.364962 loss_ctc 9.337716 lr 0.00080405 rank 0
I0626 04:50:14.186949 139724881797504 executor.py:115] TRAIN Batch 82/500 loss 8.558315 loss_att 5.357248 loss_ctc 16.027472 lr 0.00080374 rank 0
I0626 04:50:24.863057 139724881797504 executor.py:115] TRAIN Batch 82/600 loss 9.235026 loss_att 7.255774 loss_ctc 13.853281 lr 0.00080343 rank 0
I0626 04:50:36.213130 139724881797504 executor.py:115] TRAIN Batch 82/700 loss 12.461335 loss_att 8.710772 loss_ctc 21.212650 lr 0.00080312 rank 0
I0626 04:50:47.584484 139724881797504 executor.py:115] TRAIN Batch 82/800 loss 7.628059 loss_att 6.096851 loss_ctc 11.200880 lr 0.00080275 rank 0
I0626 04:50:59.012650 139724881797504 executor.py:115] TRAIN Batch 82/900 loss 6.617093 loss_att 4.540608 loss_ctc 11.462223 lr 0.00080244 rank 0
I0626 04:51:10.225272 139724881797504 executor.py:115] TRAIN Batch 82/1000 loss 6.484517 loss_att 4.243835 loss_ctc 11.712773 lr 0.00080213 rank 0
I0626 04:51:20.907161 139724881797504 executor.py:115] TRAIN Batch 82/1100 loss 11.816658 loss_att 8.295375 loss_ctc 20.032984 lr 0.00080182 rank 0
I0626 04:51:32.185308 139724881797504 executor.py:115] TRAIN Batch 82/1200 loss 11.082720 loss_att 6.335823 loss_ctc 22.158810 lr 0.00080146 rank 0
I0626 04:51:43.372599 139724881797504 executor.py:115] TRAIN Batch 82/1300 loss 6.991402 loss_att 4.588844 loss_ctc 12.597370 lr 0.00080115 rank 0
I0626 04:51:54.545097 139724881797504 executor.py:115] TRAIN Batch 82/1400 loss 8.334541 loss_att 5.268802 loss_ctc 15.487932 lr 0.00080085 rank 0
I0626 04:52:05.693111 139724881797504 executor.py:115] TRAIN Batch 82/1500 loss 6.759232 loss_att 5.791031 loss_ctc 9.018368 lr 0.00080054 rank 0
I0626 04:52:11.263024 139724881797504 executor.py:152] CV Batch 82/0 loss 14.961803 loss_att 14.651137 loss_ctc 15.686687 history loss 13.299381 rank 0
I0626 04:52:15.192213 139724881797504 executor.py:152] CV Batch 82/100 loss 39.062931 loss_att 41.133183 loss_ctc 34.232346 history loss 29.036081 rank 0
I0626 04:52:18.179216 139724881797504 train.py:288] Epoch 82 CV info cv_loss 33.75607247628786
I0626 04:52:18.179437 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/82.pt
I0626 04:52:18.435867 139724881797504 train.py:274] Epoch 83 TRAIN info lr 0.0008005381425084435
I0626 04:52:18.438884 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:52:18.438943 139724881797504 executor.py:46] total epoch is 83.
I0626 04:52:26.287983 139724881797504 executor.py:115] TRAIN Batch 83/0 loss 8.288454 loss_att 6.097491 loss_ctc 13.400700 lr 0.00080049 rank 0
I0626 04:52:37.587290 139724881797504 executor.py:115] TRAIN Batch 83/100 loss 10.708528 loss_att 7.400813 loss_ctc 18.426529 lr 0.00080018 rank 0
I0626 04:52:48.323943 139724881797504 executor.py:115] TRAIN Batch 83/200 loss 9.378952 loss_att 7.176644 loss_ctc 14.517670 lr 0.00079987 rank 0
I0626 04:52:59.400015 139724881797504 executor.py:115] TRAIN Batch 83/300 loss 6.958570 loss_att 4.603105 loss_ctc 12.454654 lr 0.00079957 rank 0
I0626 04:53:10.841783 139724881797504 executor.py:115] TRAIN Batch 83/400 loss 7.129138 loss_att 4.056971 loss_ctc 14.297528 lr 0.00079921 rank 0
I0626 04:53:22.121424 139724881797504 executor.py:115] TRAIN Batch 83/500 loss 6.557477 loss_att 4.033150 loss_ctc 12.447573 lr 0.00079890 rank 0
I0626 04:53:32.676627 139724881797504 executor.py:115] TRAIN Batch 83/600 loss 14.254663 loss_att 10.839969 loss_ctc 22.222282 lr 0.00079860 rank 0
I0626 04:53:43.735274 139724881797504 executor.py:115] TRAIN Batch 83/700 loss 8.525584 loss_att 5.871138 loss_ctc 14.719292 lr 0.00079829 rank 0
I0626 04:53:55.363567 139724881797504 executor.py:115] TRAIN Batch 83/800 loss 7.940635 loss_att 5.747166 loss_ctc 13.058730 lr 0.00079793 rank 0
I0626 04:54:06.970676 139724881797504 executor.py:115] TRAIN Batch 83/900 loss 6.612685 loss_att 4.123839 loss_ctc 12.419992 lr 0.00079763 rank 0
I0626 04:54:18.108404 139724881797504 executor.py:115] TRAIN Batch 83/1000 loss 5.889452 loss_att 4.178056 loss_ctc 9.882710 lr 0.00079733 rank 0
I0626 04:54:28.550333 139724881797504 executor.py:115] TRAIN Batch 83/1100 loss 9.302591 loss_att 6.977775 loss_ctc 14.727161 lr 0.00079702 rank 0
I0626 04:54:39.652278 139724881797504 executor.py:115] TRAIN Batch 83/1200 loss 10.279818 loss_att 6.619667 loss_ctc 18.820171 lr 0.00079667 rank 0
I0626 04:54:51.147089 139724881797504 executor.py:115] TRAIN Batch 83/1300 loss 7.445042 loss_att 4.419712 loss_ctc 14.504144 lr 0.00079636 rank 0
I0626 04:55:02.606363 139724881797504 executor.py:115] TRAIN Batch 83/1400 loss 8.171164 loss_att 5.460194 loss_ctc 14.496758 lr 0.00079606 rank 0
I0626 04:55:13.547240 139724881797504 executor.py:115] TRAIN Batch 83/1500 loss 7.800664 loss_att 5.770076 loss_ctc 12.538704 lr 0.00079576 rank 0
I0626 04:55:19.131730 139724881797504 executor.py:152] CV Batch 83/0 loss 14.774872 loss_att 15.022377 loss_ctc 14.197359 history loss 13.133219 rank 0
I0626 04:55:23.180155 139724881797504 executor.py:152] CV Batch 83/100 loss 40.126190 loss_att 42.350174 loss_ctc 34.936897 history loss 28.996152 rank 0
I0626 04:55:26.150081 139724881797504 train.py:288] Epoch 83 CV info cv_loss 33.599632790266675
I0626 04:55:26.150323 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/83.pt
I0626 04:55:26.413486 139724881797504 train.py:274] Epoch 84 TRAIN info lr 0.0007957587675990379
I0626 04:55:26.416577 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:55:26.416650 139724881797504 executor.py:46] total epoch is 84.
I0626 04:55:34.199223 139724881797504 executor.py:115] TRAIN Batch 84/0 loss 4.139969 loss_att 3.702489 loss_ctc 5.160755 lr 0.00079571 rank 0
I0626 04:55:45.351591 139724881797504 executor.py:115] TRAIN Batch 84/100 loss 8.752708 loss_att 5.840566 loss_ctc 15.547706 lr 0.00079541 rank 0
I0626 04:55:56.609521 139724881797504 executor.py:115] TRAIN Batch 84/200 loss 5.079625 loss_att 4.094945 loss_ctc 7.377212 lr 0.00079510 rank 0
I0626 04:56:07.793289 139724881797504 executor.py:115] TRAIN Batch 84/300 loss 8.744778 loss_att 5.528426 loss_ctc 16.249598 lr 0.00079480 rank 0
I0626 04:56:19.488209 139724881797504 executor.py:115] TRAIN Batch 84/400 loss 7.226395 loss_att 5.297953 loss_ctc 11.726093 lr 0.00079445 rank 0
I0626 04:56:31.340626 139724881797504 executor.py:115] TRAIN Batch 84/500 loss 4.980988 loss_att 3.875312 loss_ctc 7.560897 lr 0.00079415 rank 0
I0626 04:56:41.971229 139724881797504 executor.py:115] TRAIN Batch 84/600 loss 7.378913 loss_att 6.287669 loss_ctc 9.925150 lr 0.00079385 rank 0
I0626 04:56:53.132946 139724881797504 executor.py:115] TRAIN Batch 84/700 loss 8.399302 loss_att 7.134634 loss_ctc 11.350193 lr 0.00079355 rank 0
I0626 04:57:04.430603 139724881797504 executor.py:115] TRAIN Batch 84/800 loss 5.542061 loss_att 3.717745 loss_ctc 9.798796 lr 0.00079320 rank 0
I0626 04:57:15.553167 139724881797504 executor.py:115] TRAIN Batch 84/900 loss 5.931905 loss_att 3.959019 loss_ctc 10.535305 lr 0.00079290 rank 0
I0626 04:57:26.774111 139724881797504 executor.py:115] TRAIN Batch 84/1000 loss 4.535904 loss_att 3.324853 loss_ctc 7.361689 lr 0.00079260 rank 0
I0626 04:57:37.232872 139724881797504 executor.py:115] TRAIN Batch 84/1100 loss 10.412264 loss_att 7.384214 loss_ctc 17.477713 lr 0.00079230 rank 0
I0626 04:57:48.612801 139724881797504 executor.py:115] TRAIN Batch 84/1200 loss 12.279480 loss_att 7.864704 loss_ctc 22.580622 lr 0.00079196 rank 0
I0626 04:57:59.635545 139724881797504 executor.py:115] TRAIN Batch 84/1300 loss 8.821995 loss_att 5.530565 loss_ctc 16.501995 lr 0.00079166 rank 0
I0626 04:58:10.526423 139724881797504 executor.py:115] TRAIN Batch 84/1400 loss 9.023539 loss_att 5.410038 loss_ctc 17.455040 lr 0.00079136 rank 0
I0626 04:58:21.383568 139724881797504 executor.py:115] TRAIN Batch 84/1500 loss 10.600313 loss_att 6.253182 loss_ctc 20.743616 lr 0.00079106 rank 0
I0626 04:58:27.007800 139724881797504 executor.py:152] CV Batch 84/0 loss 15.018503 loss_att 14.954547 loss_ctc 15.167733 history loss 13.349781 rank 0
I0626 04:58:31.070093 139724881797504 executor.py:152] CV Batch 84/100 loss 39.889141 loss_att 42.317688 loss_ctc 34.222534 history loss 28.854127 rank 0
I0626 04:58:34.044219 139724881797504 train.py:288] Epoch 84 CV info cv_loss 33.53944309125475
I0626 04:58:34.044440 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/84.pt
I0626 04:58:34.304146 139724881797504 train.py:274] Epoch 85 TRAIN info lr 0.0007910639846338164
I0626 04:58:34.307254 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 04:58:34.307313 139724881797504 executor.py:46] total epoch is 85.
I0626 04:58:42.034045 139724881797504 executor.py:115] TRAIN Batch 85/0 loss 6.429334 loss_att 4.866114 loss_ctc 10.076847 lr 0.00079101 rank 0
I0626 04:58:53.107988 139724881797504 executor.py:115] TRAIN Batch 85/100 loss 9.378965 loss_att 6.476842 loss_ctc 16.150587 lr 0.00079072 rank 0
I0626 04:59:04.075660 139724881797504 executor.py:115] TRAIN Batch 85/200 loss 6.792337 loss_att 4.514194 loss_ctc 12.108003 lr 0.00079042 rank 0
I0626 04:59:15.178053 139724881797504 executor.py:115] TRAIN Batch 85/300 loss 8.224047 loss_att 5.673270 loss_ctc 14.175858 lr 0.00079013 rank 0
I0626 04:59:26.790317 139724881797504 executor.py:115] TRAIN Batch 85/400 loss 6.955141 loss_att 4.980531 loss_ctc 11.562566 lr 0.00078978 rank 0
I0626 04:59:38.300738 139724881797504 executor.py:115] TRAIN Batch 85/500 loss 4.372092 loss_att 2.723249 loss_ctc 8.219393 lr 0.00078948 rank 0
I0626 04:59:48.882021 139724881797504 executor.py:115] TRAIN Batch 85/600 loss 9.252795 loss_att 6.716971 loss_ctc 15.169717 lr 0.00078919 rank 0
I0626 04:59:59.998343 139724881797504 executor.py:115] TRAIN Batch 85/700 loss 10.334686 loss_att 6.903717 loss_ctc 18.340282 lr 0.00078889 rank 0
I0626 05:00:11.735820 139724881797504 executor.py:115] TRAIN Batch 85/800 loss 7.608164 loss_att 4.848657 loss_ctc 14.047014 lr 0.00078855 rank 0
I0626 05:00:23.174842 139724881797504 executor.py:115] TRAIN Batch 85/900 loss 9.041956 loss_att 5.583240 loss_ctc 17.112293 lr 0.00078826 rank 0
I0626 05:00:34.565973 139724881797504 executor.py:115] TRAIN Batch 85/1000 loss 8.111224 loss_att 4.654449 loss_ctc 16.177031 lr 0.00078796 rank 0
I0626 05:00:45.275664 139724881797504 executor.py:115] TRAIN Batch 85/1100 loss 12.834431 loss_att 8.304829 loss_ctc 23.403503 lr 0.00078767 rank 0
I0626 05:00:56.450304 139724881797504 executor.py:115] TRAIN Batch 85/1200 loss 9.169782 loss_att 6.399461 loss_ctc 15.633864 lr 0.00078733 rank 0
I0626 05:01:07.706241 139724881797504 executor.py:115] TRAIN Batch 85/1300 loss 9.208898 loss_att 5.784879 loss_ctc 17.198275 lr 0.00078704 rank 0
I0626 05:01:18.677508 139724881797504 executor.py:115] TRAIN Batch 85/1400 loss 6.620211 loss_att 5.006956 loss_ctc 10.384472 lr 0.00078674 rank 0
I0626 05:01:29.589002 139724881797504 executor.py:115] TRAIN Batch 85/1500 loss 13.272524 loss_att 9.149538 loss_ctc 22.892822 lr 0.00078645 rank 0
I0626 05:01:35.218419 139724881797504 executor.py:152] CV Batch 85/0 loss 16.199055 loss_att 15.898175 loss_ctc 16.901110 history loss 14.399160 rank 0
I0626 05:01:39.318942 139724881797504 executor.py:152] CV Batch 85/100 loss 39.865204 loss_att 41.455894 loss_ctc 36.153595 history loss 28.946745 rank 0
I0626 05:01:42.307932 139724881797504 train.py:288] Epoch 85 CV info cv_loss 33.416536303225385
I0626 05:01:42.308177 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/85.pt
I0626 05:01:42.571551 139724881797504 train.py:274] Epoch 86 TRAIN info lr 0.0007864513273256968
I0626 05:01:42.574520 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:01:42.574579 139724881797504 executor.py:46] total epoch is 86.
I0626 05:01:50.408373 139724881797504 executor.py:115] TRAIN Batch 86/0 loss 6.388742 loss_att 4.301272 loss_ctc 11.259507 lr 0.00078640 rank 0
I0626 05:02:01.465524 139724881797504 executor.py:115] TRAIN Batch 86/100 loss 7.954017 loss_att 5.898064 loss_ctc 12.751240 lr 0.00078611 rank 0
I0626 05:02:12.725454 139724881797504 executor.py:115] TRAIN Batch 86/200 loss 10.281410 loss_att 6.096635 loss_ctc 20.045883 lr 0.00078582 rank 0
I0626 05:02:24.364961 139724881797504 executor.py:115] TRAIN Batch 86/300 loss 10.399263 loss_att 6.719398 loss_ctc 18.985617 lr 0.00078553 rank 0
I0626 05:02:36.002819 139724881797504 executor.py:115] TRAIN Batch 86/400 loss 8.066478 loss_att 4.666842 loss_ctc 15.998960 lr 0.00078519 rank 0
I0626 05:02:47.401828 139724881797504 executor.py:115] TRAIN Batch 86/500 loss 6.389977 loss_att 4.347417 loss_ctc 11.155949 lr 0.00078490 rank 0
I0626 05:02:58.032355 139724881797504 executor.py:115] TRAIN Batch 86/600 loss 12.631804 loss_att 8.482449 loss_ctc 22.313629 lr 0.00078461 rank 0
I0626 05:03:09.322207 139724881797504 executor.py:115] TRAIN Batch 86/700 loss 8.175717 loss_att 5.478807 loss_ctc 14.468506 lr 0.00078432 rank 0
I0626 05:03:21.166028 139724881797504 executor.py:115] TRAIN Batch 86/800 loss 9.480728 loss_att 6.190400 loss_ctc 17.158159 lr 0.00078398 rank 0
I0626 05:03:32.590698 139724881797504 executor.py:115] TRAIN Batch 86/900 loss 6.324512 loss_att 3.939255 loss_ctc 11.890113 lr 0.00078369 rank 0
I0626 05:03:43.642467 139724881797504 executor.py:115] TRAIN Batch 86/1000 loss 6.243003 loss_att 3.899602 loss_ctc 11.710937 lr 0.00078340 rank 0
I0626 05:03:54.355173 139724881797504 executor.py:115] TRAIN Batch 86/1100 loss 9.483037 loss_att 6.994492 loss_ctc 15.289643 lr 0.00078312 rank 0
I0626 05:04:05.668771 139724881797504 executor.py:115] TRAIN Batch 86/1200 loss 11.529463 loss_att 7.309770 loss_ctc 21.375412 lr 0.00078278 rank 0
I0626 05:04:16.837689 139724881797504 executor.py:115] TRAIN Batch 86/1300 loss 8.204512 loss_att 4.924625 loss_ctc 15.857579 lr 0.00078249 rank 0
I0626 05:04:28.036441 139724881797504 executor.py:115] TRAIN Batch 86/1400 loss 6.238748 loss_att 4.005129 loss_ctc 11.450525 lr 0.00078221 rank 0
I0626 05:04:39.117887 139724881797504 executor.py:115] TRAIN Batch 86/1500 loss 10.383265 loss_att 7.153539 loss_ctc 17.919292 lr 0.00078192 rank 0
I0626 05:04:44.735149 139724881797504 executor.py:152] CV Batch 86/0 loss 14.820055 loss_att 14.139290 loss_ctc 16.408504 history loss 13.173382 rank 0
I0626 05:04:48.773530 139724881797504 executor.py:152] CV Batch 86/100 loss 38.140514 loss_att 39.734337 loss_ctc 34.421593 history loss 28.328871 rank 0
I0626 05:04:51.744341 139724881797504 train.py:288] Epoch 86 CV info cv_loss 32.98022717557749
I0626 05:04:51.744574 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/86.pt
I0626 05:04:52.003859 139724881797504 train.py:274] Epoch 87 TRAIN info lr 0.0007819184288945057
I0626 05:04:52.006853 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:04:52.006913 139724881797504 executor.py:46] total epoch is 87.
I0626 05:04:59.829787 139724881797504 executor.py:115] TRAIN Batch 87/0 loss 4.645831 loss_att 2.732674 loss_ctc 9.109865 lr 0.00078187 rank 0
I0626 05:05:10.754762 139724881797504 executor.py:115] TRAIN Batch 87/100 loss 10.796831 loss_att 8.361977 loss_ctc 16.478157 lr 0.00078158 rank 0
I0626 05:05:21.562548 139724881797504 executor.py:115] TRAIN Batch 87/200 loss 10.684980 loss_att 6.452138 loss_ctc 20.561609 lr 0.00078130 rank 0
I0626 05:05:32.733160 139724881797504 executor.py:115] TRAIN Batch 87/300 loss 10.157007 loss_att 6.643574 loss_ctc 18.355019 lr 0.00078101 rank 0
I0626 05:05:44.320502 139724881797504 executor.py:115] TRAIN Batch 87/400 loss 6.398851 loss_att 4.267184 loss_ctc 11.372742 lr 0.00078068 rank 0
I0626 05:05:55.572798 139724881797504 executor.py:115] TRAIN Batch 87/500 loss 3.086754 loss_att 2.654878 loss_ctc 4.094466 lr 0.00078039 rank 0
I0626 05:06:06.013010 139724881797504 executor.py:115] TRAIN Batch 87/600 loss 15.886860 loss_att 10.490938 loss_ctc 28.477343 lr 0.00078011 rank 0
I0626 05:06:17.141597 139724881797504 executor.py:115] TRAIN Batch 87/700 loss 11.001436 loss_att 7.838570 loss_ctc 18.381456 lr 0.00077982 rank 0
I0626 05:06:28.773709 139724881797504 executor.py:115] TRAIN Batch 87/800 loss 9.201427 loss_att 5.542348 loss_ctc 17.739281 lr 0.00077949 rank 0
I0626 05:06:40.284499 139724881797504 executor.py:115] TRAIN Batch 87/900 loss 5.650672 loss_att 4.274458 loss_ctc 8.861837 lr 0.00077921 rank 0
I0626 05:06:51.746441 139724881797504 executor.py:115] TRAIN Batch 87/1000 loss 9.239763 loss_att 5.686521 loss_ctc 17.530663 lr 0.00077892 rank 0
I0626 05:07:02.707505 139724881797504 executor.py:115] TRAIN Batch 87/1100 loss 9.108219 loss_att 6.490111 loss_ctc 15.217137 lr 0.00077864 rank 0
I0626 05:07:13.923819 139724881797504 executor.py:115] TRAIN Batch 87/1200 loss 7.814862 loss_att 5.784422 loss_ctc 12.552556 lr 0.00077831 rank 0
I0626 05:07:25.093841 139724881797504 executor.py:115] TRAIN Batch 87/1300 loss 12.658798 loss_att 7.626426 loss_ctc 24.400999 lr 0.00077803 rank 0
I0626 05:07:36.413740 139724881797504 executor.py:115] TRAIN Batch 87/1400 loss 6.422823 loss_att 4.431073 loss_ctc 11.070238 lr 0.00077775 rank 0
I0626 05:07:47.494178 139724881797504 executor.py:115] TRAIN Batch 87/1500 loss 10.503431 loss_att 6.600614 loss_ctc 19.610004 lr 0.00077746 rank 0
I0626 05:07:53.130304 139724881797504 executor.py:152] CV Batch 87/0 loss 15.601161 loss_att 15.008598 loss_ctc 16.983807 history loss 13.867699 rank 0
I0626 05:07:57.322507 139724881797504 executor.py:152] CV Batch 87/100 loss 38.654472 loss_att 39.714127 loss_ctc 36.181946 history loss 28.021565 rank 0
I0626 05:08:00.306365 139724881797504 train.py:288] Epoch 87 CV info cv_loss 32.71225564428251
I0626 05:08:00.306577 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/87.pt
I0626 05:08:00.564509 139724881797504 train.py:274] Epoch 88 TRAIN info lr 0.0007774630169639036
I0626 05:08:00.567556 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:08:00.567618 139724881797504 executor.py:46] total epoch is 88.
I0626 05:08:08.302800 139724881797504 executor.py:115] TRAIN Batch 88/0 loss 4.271480 loss_att 2.391900 loss_ctc 8.657166 lr 0.00077742 rank 0
I0626 05:08:19.512661 139724881797504 executor.py:115] TRAIN Batch 88/100 loss 11.988754 loss_att 7.984223 loss_ctc 21.332661 lr 0.00077713 rank 0
I0626 05:08:30.873876 139724881797504 executor.py:115] TRAIN Batch 88/200 loss 7.626054 loss_att 4.759562 loss_ctc 14.314533 lr 0.00077685 rank 0
I0626 05:08:42.170724 139724881797504 executor.py:115] TRAIN Batch 88/300 loss 8.321873 loss_att 5.569187 loss_ctc 14.744806 lr 0.00077657 rank 0
I0626 05:08:53.515922 139724881797504 executor.py:115] TRAIN Batch 88/400 loss 6.037968 loss_att 4.153422 loss_ctc 10.435240 lr 0.00077624 rank 0
I0626 05:09:04.924941 139724881797504 executor.py:115] TRAIN Batch 88/500 loss 4.552923 loss_att 3.355009 loss_ctc 7.348056 lr 0.00077596 rank 0
I0626 05:09:15.153102 139724881797504 executor.py:115] TRAIN Batch 88/600 loss 16.029013 loss_att 10.862864 loss_ctc 28.083359 lr 0.00077568 rank 0
I0626 05:09:26.081282 139724881797504 executor.py:115] TRAIN Batch 88/700 loss 9.083097 loss_att 6.681130 loss_ctc 14.687689 lr 0.00077540 rank 0
I0626 05:09:37.533293 139724881797504 executor.py:115] TRAIN Batch 88/800 loss 10.039541 loss_att 5.694563 loss_ctc 20.177822 lr 0.00077508 rank 0
I0626 05:09:49.024112 139724881797504 executor.py:115] TRAIN Batch 88/900 loss 5.568438 loss_att 3.525343 loss_ctc 10.335657 lr 0.00077480 rank 0
I0626 05:10:00.375453 139724881797504 executor.py:115] TRAIN Batch 88/1000 loss 5.459096 loss_att 3.214938 loss_ctc 10.695464 lr 0.00077452 rank 0
I0626 05:10:10.931281 139724881797504 executor.py:115] TRAIN Batch 88/1100 loss 11.007560 loss_att 7.027129 loss_ctc 20.295231 lr 0.00077424 rank 0
I0626 05:10:22.147755 139724881797504 executor.py:115] TRAIN Batch 88/1200 loss 7.777402 loss_att 5.899942 loss_ctc 12.158141 lr 0.00077392 rank 0
I0626 05:10:33.220521 139724881797504 executor.py:115] TRAIN Batch 88/1300 loss 7.197741 loss_att 4.844860 loss_ctc 12.687796 lr 0.00077364 rank 0
I0626 05:10:44.273865 139724881797504 executor.py:115] TRAIN Batch 88/1400 loss 5.737657 loss_att 4.076906 loss_ctc 9.612741 lr 0.00077336 rank 0
I0626 05:10:55.067379 139724881797504 executor.py:115] TRAIN Batch 88/1500 loss 10.574547 loss_att 7.525040 loss_ctc 17.690063 lr 0.00077308 rank 0
I0626 05:11:00.790875 139724881797504 executor.py:152] CV Batch 88/0 loss 16.256516 loss_att 15.405469 loss_ctc 18.242294 history loss 14.450236 rank 0
I0626 05:11:04.681534 139724881797504 executor.py:152] CV Batch 88/100 loss 37.652180 loss_att 39.549789 loss_ctc 33.224419 history loss 28.236828 rank 0
I0626 05:11:07.675608 139724881797504 train.py:288] Epoch 88 CV info cv_loss 32.763357882556264
I0626 05:11:07.675828 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/88.pt
I0626 05:11:07.933643 139724881797504 train.py:274] Epoch 89 TRAIN info lr 0.0007730829087745697
I0626 05:11:07.936717 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:11:07.936788 139724881797504 executor.py:46] total epoch is 89.
I0626 05:11:15.580701 139724881797504 executor.py:115] TRAIN Batch 89/0 loss 4.265470 loss_att 2.971362 loss_ctc 7.285055 lr 0.00077304 rank 0
I0626 05:11:26.281405 139724881797504 executor.py:115] TRAIN Batch 89/100 loss 8.419517 loss_att 6.104317 loss_ctc 13.821649 lr 0.00077276 rank 0
I0626 05:11:37.272198 139724881797504 executor.py:115] TRAIN Batch 89/200 loss 10.701809 loss_att 6.264808 loss_ctc 21.054810 lr 0.00077248 rank 0
I0626 05:11:48.484099 139724881797504 executor.py:115] TRAIN Batch 89/300 loss 6.472001 loss_att 4.876860 loss_ctc 10.193995 lr 0.00077221 rank 0
I0626 05:11:59.990815 139724881797504 executor.py:115] TRAIN Batch 89/400 loss 5.164583 loss_att 3.442569 loss_ctc 9.182617 lr 0.00077188 rank 0
I0626 05:12:11.563894 139724881797504 executor.py:115] TRAIN Batch 89/500 loss 5.913442 loss_att 4.356546 loss_ctc 9.546197 lr 0.00077161 rank 0
I0626 05:12:22.086363 139724881797504 executor.py:115] TRAIN Batch 89/600 loss 11.179981 loss_att 7.709361 loss_ctc 19.278093 lr 0.00077133 rank 0
I0626 05:12:33.562280 139724881797504 executor.py:115] TRAIN Batch 89/700 loss 12.312224 loss_att 7.113467 loss_ctc 24.442657 lr 0.00077106 rank 0
I0626 05:12:44.769198 139724881797504 executor.py:115] TRAIN Batch 89/800 loss 7.371765 loss_att 4.624185 loss_ctc 13.782784 lr 0.00077074 rank 0
I0626 05:12:56.156157 139724881797504 executor.py:115] TRAIN Batch 89/900 loss 8.311195 loss_att 5.026877 loss_ctc 15.974602 lr 0.00077046 rank 0
I0626 05:13:07.096668 139724881797504 executor.py:115] TRAIN Batch 89/1000 loss 5.675020 loss_att 3.467573 loss_ctc 10.825728 lr 0.00077019 rank 0
I0626 05:13:17.587952 139724881797504 executor.py:115] TRAIN Batch 89/1100 loss 10.727242 loss_att 8.002797 loss_ctc 17.084282 lr 0.00076991 rank 0
I0626 05:13:28.587547 139724881797504 executor.py:115] TRAIN Batch 89/1200 loss 8.759760 loss_att 6.457888 loss_ctc 14.130795 lr 0.00076960 rank 0
I0626 05:13:39.605835 139724881797504 executor.py:115] TRAIN Batch 89/1300 loss 5.885819 loss_att 4.955164 loss_ctc 8.057346 lr 0.00076932 rank 0
I0626 05:13:50.582728 139724881797504 executor.py:115] TRAIN Batch 89/1400 loss 6.271273 loss_att 4.230310 loss_ctc 11.033520 lr 0.00076905 rank 0
I0626 05:14:01.420577 139724881797504 executor.py:115] TRAIN Batch 89/1500 loss 6.969667 loss_att 4.587379 loss_ctc 12.528339 lr 0.00076878 rank 0
I0626 05:14:07.008374 139724881797504 executor.py:152] CV Batch 89/0 loss 15.907457 loss_att 14.541590 loss_ctc 19.094482 history loss 14.139962 rank 0
I0626 05:14:11.436752 139724881797504 executor.py:152] CV Batch 89/100 loss 39.805809 loss_att 41.565559 loss_ctc 35.699722 history loss 28.375159 rank 0
I0626 05:14:14.411791 139724881797504 train.py:288] Epoch 89 CV info cv_loss 32.88882069344797
I0626 05:14:14.412012 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/89.pt
I0626 05:14:14.673337 139724881797504 train.py:274] Epoch 90 TRAIN info lr 0.0007687760066907376
I0626 05:14:14.676367 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:14:14.676438 139724881797504 executor.py:46] total epoch is 90.
I0626 05:14:22.517066 139724881797504 executor.py:115] TRAIN Batch 90/0 loss 5.524655 loss_att 3.091323 loss_ctc 11.202430 lr 0.00076873 rank 0
I0626 05:14:33.244909 139724881797504 executor.py:115] TRAIN Batch 90/100 loss 8.569206 loss_att 5.466681 loss_ctc 15.808430 lr 0.00076846 rank 0
I0626 05:14:44.273531 139724881797504 executor.py:115] TRAIN Batch 90/200 loss 6.860855 loss_att 3.961711 loss_ctc 13.625524 lr 0.00076819 rank 0
I0626 05:14:55.363336 139724881797504 executor.py:115] TRAIN Batch 90/300 loss 5.768458 loss_att 3.566849 loss_ctc 10.905546 lr 0.00076791 rank 0
I0626 05:15:06.860070 139724881797504 executor.py:115] TRAIN Batch 90/400 loss 6.448133 loss_att 4.121188 loss_ctc 11.877670 lr 0.00076760 rank 0
I0626 05:15:18.449409 139724881797504 executor.py:115] TRAIN Batch 90/500 loss 4.163674 loss_att 2.511541 loss_ctc 8.018650 lr 0.00076733 rank 0
I0626 05:15:28.689467 139724881797504 executor.py:115] TRAIN Batch 90/600 loss 7.819265 loss_att 6.628408 loss_ctc 10.597932 lr 0.00076706 rank 0
I0626 05:15:39.743793 139724881797504 executor.py:115] TRAIN Batch 90/700 loss 8.215532 loss_att 5.983986 loss_ctc 13.422475 lr 0.00076678 rank 0
I0626 05:15:51.488012 139724881797504 executor.py:115] TRAIN Batch 90/800 loss 5.660720 loss_att 4.496932 loss_ctc 8.376226 lr 0.00076647 rank 0
I0626 05:16:03.099950 139724881797504 executor.py:115] TRAIN Batch 90/900 loss 6.611609 loss_att 4.524523 loss_ctc 11.481473 lr 0.00076620 rank 0
I0626 05:16:14.628208 139724881797504 executor.py:115] TRAIN Batch 90/1000 loss 5.452930 loss_att 3.247435 loss_ctc 10.599088 lr 0.00076593 rank 0
I0626 05:16:25.488976 139724881797504 executor.py:115] TRAIN Batch 90/1100 loss 9.755003 loss_att 5.900871 loss_ctc 18.747978 lr 0.00076566 rank 0
I0626 05:16:36.941312 139724881797504 executor.py:115] TRAIN Batch 90/1200 loss 9.636080 loss_att 6.471346 loss_ctc 17.020458 lr 0.00076535 rank 0
I0626 05:16:48.310716 139724881797504 executor.py:115] TRAIN Batch 90/1300 loss 7.336899 loss_att 5.111489 loss_ctc 12.529521 lr 0.00076508 rank 0
I0626 05:16:59.620796 139724881797504 executor.py:115] TRAIN Batch 90/1400 loss 6.363393 loss_att 4.273018 loss_ctc 11.240934 lr 0.00076481 rank 0
I0626 05:17:10.322555 139724881797504 executor.py:115] TRAIN Batch 90/1500 loss 6.463229 loss_att 4.794139 loss_ctc 10.357771 lr 0.00076454 rank 0
I0626 05:17:16.004413 139724881797504 executor.py:152] CV Batch 90/0 loss 16.370365 loss_att 15.118340 loss_ctc 19.291763 history loss 14.551436 rank 0
I0626 05:17:20.080737 139724881797504 executor.py:152] CV Batch 90/100 loss 38.788067 loss_att 39.987347 loss_ctc 35.989746 history loss 28.336693 rank 0
I0626 05:17:23.063516 139724881797504 train.py:288] Epoch 90 CV info cv_loss 32.72077134765284
I0626 05:17:23.063729 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/90.pt
I0626 05:17:23.321639 139724881797504 train.py:274] Epoch 91 TRAIN info lr 0.000764540293979072
I0626 05:17:23.324703 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:17:23.324776 139724881797504 executor.py:46] total epoch is 91.
I0626 05:17:31.035583 139724881797504 executor.py:115] TRAIN Batch 91/0 loss 4.300198 loss_att 3.028412 loss_ctc 7.267698 lr 0.00076450 rank 0
I0626 05:17:41.831094 139724881797504 executor.py:115] TRAIN Batch 91/100 loss 6.105060 loss_att 4.840885 loss_ctc 9.054802 lr 0.00076423 rank 0
I0626 05:17:52.726416 139724881797504 executor.py:115] TRAIN Batch 91/200 loss 6.955474 loss_att 4.232044 loss_ctc 13.310144 lr 0.00076396 rank 0
I0626 05:18:03.946669 139724881797504 executor.py:115] TRAIN Batch 91/300 loss 6.310968 loss_att 3.853989 loss_ctc 12.043919 lr 0.00076369 rank 0
I0626 05:18:15.270462 139724881797504 executor.py:115] TRAIN Batch 91/400 loss 5.115487 loss_att 3.539349 loss_ctc 8.793144 lr 0.00076338 rank 0
I0626 05:18:26.794047 139724881797504 executor.py:115] TRAIN Batch 91/500 loss 4.466144 loss_att 3.378956 loss_ctc 7.002915 lr 0.00076311 rank 0
I0626 05:18:37.400851 139724881797504 executor.py:115] TRAIN Batch 91/600 loss 10.833409 loss_att 7.867192 loss_ctc 17.754583 lr 0.00076285 rank 0
I0626 05:18:48.625290 139724881797504 executor.py:115] TRAIN Batch 91/700 loss 8.691372 loss_att 5.882823 loss_ctc 15.244650 lr 0.00076258 rank 0
I0626 05:19:00.167618 139724881797504 executor.py:115] TRAIN Batch 91/800 loss 7.068413 loss_att 5.427606 loss_ctc 10.896963 lr 0.00076227 rank 0
I0626 05:19:11.422792 139724881797504 executor.py:115] TRAIN Batch 91/900 loss 6.314943 loss_att 3.964875 loss_ctc 11.798437 lr 0.00076201 rank 0
I0626 05:19:22.599969 139724881797504 executor.py:115] TRAIN Batch 91/1000 loss 4.117589 loss_att 2.882550 loss_ctc 6.999346 lr 0.00076174 rank 0
I0626 05:19:33.362186 139724881797504 executor.py:115] TRAIN Batch 91/1100 loss 8.800919 loss_att 6.037060 loss_ctc 15.249922 lr 0.00076148 rank 0
I0626 05:19:44.760921 139724881797504 executor.py:115] TRAIN Batch 91/1200 loss 8.247524 loss_att 6.114815 loss_ctc 13.223846 lr 0.00076117 rank 0
I0626 05:19:55.841626 139724881797504 executor.py:115] TRAIN Batch 91/1300 loss 5.189432 loss_att 3.840285 loss_ctc 8.337443 lr 0.00076090 rank 0
I0626 05:20:06.893321 139724881797504 executor.py:115] TRAIN Batch 91/1400 loss 7.204858 loss_att 4.521509 loss_ctc 13.466006 lr 0.00076064 rank 0
I0626 05:20:17.738364 139724881797504 executor.py:115] TRAIN Batch 91/1500 loss 5.681814 loss_att 3.331817 loss_ctc 11.165140 lr 0.00076037 rank 0
I0626 05:20:23.299775 139724881797504 executor.py:152] CV Batch 91/0 loss 15.951002 loss_att 14.963281 loss_ctc 18.255686 history loss 14.178669 rank 0
I0626 05:20:27.535292 139724881797504 executor.py:152] CV Batch 91/100 loss 40.813293 loss_att 42.605507 loss_ctc 36.631466 history loss 28.989319 rank 0
I0626 05:20:30.519004 139724881797504 train.py:288] Epoch 91 CV info cv_loss 33.29715164940588
I0626 05:20:30.519128 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/91.pt
I0626 05:20:30.779112 139724881797504 train.py:274] Epoch 92 TRAIN info lr 0.0007603738308405919
I0626 05:20:30.782156 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:20:30.782217 139724881797504 executor.py:46] total epoch is 92.
I0626 05:20:38.638199 139724881797504 executor.py:115] TRAIN Batch 92/0 loss 7.840678 loss_att 5.085658 loss_ctc 14.269058 lr 0.00076033 rank 0
I0626 05:20:49.305643 139724881797504 executor.py:115] TRAIN Batch 92/100 loss 11.896992 loss_att 7.762950 loss_ctc 21.543087 lr 0.00076007 rank 0
I0626 05:21:00.581312 139724881797504 executor.py:115] TRAIN Batch 92/200 loss 10.876822 loss_att 7.055875 loss_ctc 19.792364 lr 0.00075980 rank 0
I0626 05:21:12.022519 139724881797504 executor.py:115] TRAIN Batch 92/300 loss 9.172194 loss_att 6.106022 loss_ctc 16.326591 lr 0.00075954 rank 0
I0626 05:21:23.572129 139724881797504 executor.py:115] TRAIN Batch 92/400 loss 9.849855 loss_att 6.437127 loss_ctc 17.812889 lr 0.00075923 rank 0
I0626 05:21:35.244924 139724881797504 executor.py:115] TRAIN Batch 92/500 loss 6.481367 loss_att 3.848203 loss_ctc 12.625416 lr 0.00075897 rank 0
I0626 05:21:46.092031 139724881797504 executor.py:115] TRAIN Batch 92/600 loss 7.067694 loss_att 5.587232 loss_ctc 10.522106 lr 0.00075871 rank 0
I0626 05:21:57.448566 139724881797504 executor.py:115] TRAIN Batch 92/700 loss 8.393476 loss_att 6.269577 loss_ctc 13.349243 lr 0.00075845 rank 0
I0626 05:22:09.038073 139724881797504 executor.py:115] TRAIN Batch 92/800 loss 6.804231 loss_att 3.715137 loss_ctc 14.012116 lr 0.00075814 rank 0
I0626 05:22:20.604818 139724881797504 executor.py:115] TRAIN Batch 92/900 loss 7.278932 loss_att 4.427164 loss_ctc 13.933058 lr 0.00075788 rank 0
I0626 05:22:31.968061 139724881797504 executor.py:115] TRAIN Batch 92/1000 loss 6.727410 loss_att 4.924490 loss_ctc 10.934222 lr 0.00075762 rank 0
I0626 05:22:42.859480 139724881797504 executor.py:115] TRAIN Batch 92/1100 loss 8.627166 loss_att 5.875578 loss_ctc 15.047535 lr 0.00075736 rank 0
I0626 05:22:53.934510 139724881797504 executor.py:115] TRAIN Batch 92/1200 loss 8.936860 loss_att 5.926634 loss_ctc 15.960719 lr 0.00075705 rank 0
I0626 05:23:05.259919 139724881797504 executor.py:115] TRAIN Batch 92/1300 loss 6.894060 loss_att 4.547288 loss_ctc 12.369861 lr 0.00075679 rank 0
I0626 05:23:16.494406 139724881797504 executor.py:115] TRAIN Batch 92/1400 loss 4.806384 loss_att 2.621973 loss_ctc 9.903343 lr 0.00075653 rank 0
I0626 05:23:27.672431 139724881797504 executor.py:115] TRAIN Batch 92/1500 loss 6.716134 loss_att 5.244863 loss_ctc 10.149099 lr 0.00075627 rank 0
I0626 05:23:33.224385 139724881797504 executor.py:152] CV Batch 92/0 loss 15.782043 loss_att 14.994999 loss_ctc 17.618481 history loss 14.028483 rank 0
I0626 05:23:37.345836 139724881797504 executor.py:152] CV Batch 92/100 loss 39.417656 loss_att 42.038025 loss_ctc 33.303459 history loss 28.267953 rank 0
I0626 05:23:40.315741 139724881797504 train.py:288] Epoch 92 CV info cv_loss 32.85743509466966
I0626 05:23:40.315962 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/92.pt
I0626 05:23:40.576958 139724881797504 train.py:274] Epoch 93 TRAIN info lr 0.0007562747506779042
I0626 05:23:40.579882 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:23:40.579943 139724881797504 executor.py:46] total epoch is 93.
I0626 05:23:48.284873 139724881797504 executor.py:115] TRAIN Batch 93/0 loss 4.038311 loss_att 2.160253 loss_ctc 8.420444 lr 0.00075623 rank 0
I0626 05:23:59.323894 139724881797504 executor.py:115] TRAIN Batch 93/100 loss 9.181109 loss_att 6.717861 loss_ctc 14.928688 lr 0.00075597 rank 0
I0626 05:24:10.129292 139724881797504 executor.py:115] TRAIN Batch 93/200 loss 10.161355 loss_att 6.948192 loss_ctc 17.658735 lr 0.00075571 rank 0
I0626 05:24:21.194024 139724881797504 executor.py:115] TRAIN Batch 93/300 loss 9.896078 loss_att 6.256434 loss_ctc 18.388578 lr 0.00075545 rank 0
I0626 05:24:32.615900 139724881797504 executor.py:115] TRAIN Batch 93/400 loss 7.337219 loss_att 4.942978 loss_ctc 12.923782 lr 0.00075515 rank 0
I0626 05:24:44.199434 139724881797504 executor.py:115] TRAIN Batch 93/500 loss 4.313153 loss_att 2.751052 loss_ctc 7.958056 lr 0.00075489 rank 0
I0626 05:24:54.420156 139724881797504 executor.py:115] TRAIN Batch 93/600 loss 11.186303 loss_att 7.739245 loss_ctc 19.229437 lr 0.00075464 rank 0
I0626 05:25:05.362405 139724881797504 executor.py:115] TRAIN Batch 93/700 loss 8.295511 loss_att 5.584149 loss_ctc 14.622023 lr 0.00075438 rank 0
I0626 05:25:16.722184 139724881797504 executor.py:115] TRAIN Batch 93/800 loss 6.544963 loss_att 4.997293 loss_ctc 10.156194 lr 0.00075408 rank 0
I0626 05:25:28.215334 139724881797504 executor.py:115] TRAIN Batch 93/900 loss 6.432427 loss_att 3.870799 loss_ctc 12.409560 lr 0.00075382 rank 0
I0626 05:25:39.589194 139724881797504 executor.py:115] TRAIN Batch 93/1000 loss 7.715170 loss_att 4.502632 loss_ctc 15.211092 lr 0.00075356 rank 0
I0626 05:25:50.383857 139724881797504 executor.py:115] TRAIN Batch 93/1100 loss 11.998959 loss_att 8.190491 loss_ctc 20.885382 lr 0.00075331 rank 0
I0626 05:26:02.102531 139724881797504 executor.py:115] TRAIN Batch 93/1200 loss 6.765801 loss_att 4.517323 loss_ctc 12.012249 lr 0.00075301 rank 0
I0626 05:26:13.619907 139724881797504 executor.py:115] TRAIN Batch 93/1300 loss 8.501006 loss_att 4.162909 loss_ctc 18.623234 lr 0.00075275 rank 0
I0626 05:26:24.713656 139724881797504 executor.py:115] TRAIN Batch 93/1400 loss 7.168754 loss_att 5.566129 loss_ctc 10.908211 lr 0.00075250 rank 0
I0626 05:26:35.420102 139724881797504 executor.py:115] TRAIN Batch 93/1500 loss 6.584808 loss_att 5.322606 loss_ctc 9.529945 lr 0.00075224 rank 0
I0626 05:26:41.002489 139724881797504 executor.py:152] CV Batch 93/0 loss 16.370075 loss_att 15.371345 loss_ctc 18.700443 history loss 14.551178 rank 0
I0626 05:26:45.052090 139724881797504 executor.py:152] CV Batch 93/100 loss 39.098869 loss_att 40.790405 loss_ctc 35.151958 history loss 27.980760 rank 0
I0626 05:26:48.033918 139724881797504 train.py:288] Epoch 93 CV info cv_loss 32.65499440291006
I0626 05:26:48.034135 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/93.pt
I0626 05:26:48.290285 139724881797504 train.py:274] Epoch 94 TRAIN info lr 0.0007522412565814335
I0626 05:26:48.293361 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:26:48.293431 139724881797504 executor.py:46] total epoch is 94.
I0626 05:26:56.091201 139724881797504 executor.py:115] TRAIN Batch 94/0 loss 5.816445 loss_att 2.821596 loss_ctc 12.804426 lr 0.00075220 rank 0
I0626 05:27:06.846944 139724881797504 executor.py:115] TRAIN Batch 94/100 loss 12.210823 loss_att 7.222057 loss_ctc 23.851276 lr 0.00075194 rank 0
I0626 05:27:17.898261 139724881797504 executor.py:115] TRAIN Batch 94/200 loss 7.689472 loss_att 5.959315 loss_ctc 11.726503 lr 0.00075169 rank 0
I0626 05:27:29.027642 139724881797504 executor.py:115] TRAIN Batch 94/300 loss 7.450376 loss_att 4.916044 loss_ctc 13.363814 lr 0.00075143 rank 0
I0626 05:27:40.674387 139724881797504 executor.py:115] TRAIN Batch 94/400 loss 4.393130 loss_att 3.174882 loss_ctc 7.235710 lr 0.00075114 rank 0
I0626 05:27:52.243277 139724881797504 executor.py:115] TRAIN Batch 94/500 loss 3.690269 loss_att 1.997084 loss_ctc 7.641033 lr 0.00075088 rank 0
I0626 05:28:03.089691 139724881797504 executor.py:115] TRAIN Batch 94/600 loss 9.756901 loss_att 7.388099 loss_ctc 15.284103 lr 0.00075063 rank 0
I0626 05:28:14.243255 139724881797504 executor.py:115] TRAIN Batch 94/700 loss 9.027776 loss_att 5.238369 loss_ctc 17.869724 lr 0.00075038 rank 0
I0626 05:28:25.705942 139724881797504 executor.py:115] TRAIN Batch 94/800 loss 6.605080 loss_att 4.356242 loss_ctc 11.852366 lr 0.00075008 rank 0
I0626 05:28:37.046655 139724881797504 executor.py:115] TRAIN Batch 94/900 loss 5.073187 loss_att 4.141394 loss_ctc 7.247372 lr 0.00074983 rank 0
I0626 05:28:48.541612 139724881797504 executor.py:115] TRAIN Batch 94/1000 loss 4.449643 loss_att 2.775854 loss_ctc 8.355151 lr 0.00074957 rank 0
I0626 05:28:59.695829 139724881797504 executor.py:115] TRAIN Batch 94/1100 loss 8.154871 loss_att 5.716646 loss_ctc 13.844063 lr 0.00074932 rank 0
I0626 05:29:11.019039 139724881797504 executor.py:115] TRAIN Batch 94/1200 loss 10.843338 loss_att 6.576313 loss_ctc 20.799728 lr 0.00074903 rank 0
I0626 05:29:22.328902 139724881797504 executor.py:115] TRAIN Batch 94/1300 loss 8.030874 loss_att 5.505775 loss_ctc 13.922772 lr 0.00074877 rank 0
I0626 05:29:33.558351 139724881797504 executor.py:115] TRAIN Batch 94/1400 loss 6.061495 loss_att 4.483862 loss_ctc 9.742640 lr 0.00074852 rank 0
I0626 05:29:44.254119 139724881797504 executor.py:115] TRAIN Batch 94/1500 loss 7.535644 loss_att 5.960863 loss_ctc 11.210135 lr 0.00074827 rank 0
I0626 05:29:49.833120 139724881797504 executor.py:152] CV Batch 94/0 loss 15.891776 loss_att 15.296807 loss_ctc 17.280037 history loss 14.126023 rank 0
I0626 05:29:53.956276 139724881797504 executor.py:152] CV Batch 94/100 loss 41.054405 loss_att 42.892204 loss_ctc 36.766205 history loss 28.829590 rank 0
I0626 05:29:56.935249 139724881797504 train.py:288] Epoch 94 CV info cv_loss 33.130921629368636
I0626 05:29:56.935620 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/94.pt
I0626 05:29:57.202847 139724881797504 train.py:274] Epoch 95 TRAIN info lr 0.0007482716180196244
I0626 05:29:57.205914 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:29:57.205973 139724881797504 executor.py:46] total epoch is 95.
I0626 05:30:04.963219 139724881797504 executor.py:115] TRAIN Batch 95/0 loss 5.357403 loss_att 4.021470 loss_ctc 8.474579 lr 0.00074823 rank 0
I0626 05:30:15.876898 139724881797504 executor.py:115] TRAIN Batch 95/100 loss 7.708397 loss_att 6.091586 loss_ctc 11.480957 lr 0.00074798 rank 0
I0626 05:30:27.005748 139724881797504 executor.py:115] TRAIN Batch 95/200 loss 10.794054 loss_att 6.665252 loss_ctc 20.427923 lr 0.00074773 rank 0
I0626 05:30:38.534452 139724881797504 executor.py:115] TRAIN Batch 95/300 loss 10.866074 loss_att 6.516923 loss_ctc 21.014091 lr 0.00074748 rank 0
I0626 05:30:50.221739 139724881797504 executor.py:115] TRAIN Batch 95/400 loss 5.089124 loss_att 3.999395 loss_ctc 7.631824 lr 0.00074718 rank 0
I0626 05:31:01.619691 139724881797504 executor.py:115] TRAIN Batch 95/500 loss 4.964255 loss_att 3.535430 loss_ctc 8.298181 lr 0.00074693 rank 0
I0626 05:31:11.761544 139724881797504 executor.py:115] TRAIN Batch 95/600 loss 8.945793 loss_att 7.450450 loss_ctc 12.434926 lr 0.00074668 rank 0
I0626 05:31:22.701932 139724881797504 executor.py:115] TRAIN Batch 95/700 loss 10.259533 loss_att 7.271192 loss_ctc 17.232328 lr 0.00074643 rank 0
I0626 05:31:34.225059 139724881797504 executor.py:115] TRAIN Batch 95/800 loss 7.746320 loss_att 5.459633 loss_ctc 13.081923 lr 0.00074614 rank 0
I0626 05:31:45.574278 139724881797504 executor.py:115] TRAIN Batch 95/900 loss 7.700735 loss_att 4.589967 loss_ctc 14.959194 lr 0.00074589 rank 0
I0626 05:31:56.671650 139724881797504 executor.py:115] TRAIN Batch 95/1000 loss 3.850807 loss_att 2.807140 loss_ctc 6.286027 lr 0.00074565 rank 0
I0626 05:32:07.689922 139724881797504 executor.py:115] TRAIN Batch 95/1100 loss 10.889010 loss_att 7.516994 loss_ctc 18.757048 lr 0.00074540 rank 0
I0626 05:32:19.108306 139724881797504 executor.py:115] TRAIN Batch 95/1200 loss 8.983351 loss_att 5.975750 loss_ctc 16.001083 lr 0.00074511 rank 0
I0626 05:32:30.717605 139724881797504 executor.py:115] TRAIN Batch 95/1300 loss 7.603971 loss_att 4.451101 loss_ctc 14.960669 lr 0.00074486 rank 0
I0626 05:32:42.237844 139724881797504 executor.py:115] TRAIN Batch 95/1400 loss 8.038961 loss_att 5.561195 loss_ctc 13.820414 lr 0.00074461 rank 0
I0626 05:32:53.205609 139724881797504 executor.py:115] TRAIN Batch 95/1500 loss 8.905495 loss_att 4.928244 loss_ctc 18.185743 lr 0.00074436 rank 0
I0626 05:32:58.772725 139724881797504 executor.py:152] CV Batch 95/0 loss 17.214249 loss_att 15.781857 loss_ctc 20.556498 history loss 15.301554 rank 0
I0626 05:33:02.910664 139724881797504 executor.py:152] CV Batch 95/100 loss 39.858849 loss_att 40.900585 loss_ctc 37.428123 history loss 27.871472 rank 0
I0626 05:33:05.909948 139724881797504 train.py:288] Epoch 95 CV info cv_loss 32.441199612135
I0626 05:33:05.910118 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/95.pt
I0626 05:33:06.172196 139724881797504 train.py:274] Epoch 96 TRAIN info lr 0.000744364167719267
I0626 05:33:06.175157 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:33:06.175216 139724881797504 executor.py:46] total epoch is 96.
I0626 05:33:13.897934 139724881797504 executor.py:115] TRAIN Batch 96/0 loss 3.657053 loss_att 1.929716 loss_ctc 7.687506 lr 0.00074432 rank 0
I0626 05:33:24.730504 139724881797504 executor.py:115] TRAIN Batch 96/100 loss 9.829180 loss_att 6.911133 loss_ctc 16.637955 lr 0.00074408 rank 0
I0626 05:33:35.727511 139724881797504 executor.py:115] TRAIN Batch 96/200 loss 10.443797 loss_att 6.151647 loss_ctc 20.458815 lr 0.00074383 rank 0
I0626 05:33:46.899245 139724881797504 executor.py:115] TRAIN Batch 96/300 loss 8.546511 loss_att 5.769274 loss_ctc 15.026729 lr 0.00074358 rank 0
I0626 05:33:58.396578 139724881797504 executor.py:115] TRAIN Batch 96/400 loss 6.240551 loss_att 4.242137 loss_ctc 10.903518 lr 0.00074329 rank 0
I0626 05:34:10.092450 139724881797504 executor.py:115] TRAIN Batch 96/500 loss 5.183240 loss_att 3.768416 loss_ctc 8.484493 lr 0.00074305 rank 0
I0626 05:34:20.783621 139724881797504 executor.py:115] TRAIN Batch 96/600 loss 9.483438 loss_att 6.091822 loss_ctc 17.397213 lr 0.00074280 rank 0
I0626 05:34:31.915636 139724881797504 executor.py:115] TRAIN Batch 96/700 loss 8.380043 loss_att 6.337172 loss_ctc 13.146744 lr 0.00074256 rank 0
I0626 05:34:43.642360 139724881797504 executor.py:115] TRAIN Batch 96/800 loss 7.348040 loss_att 5.653612 loss_ctc 11.301704 lr 0.00074227 rank 0
I0626 05:34:55.197039 139724881797504 executor.py:115] TRAIN Batch 96/900 loss 6.430652 loss_att 4.107003 loss_ctc 11.852498 lr 0.00074202 rank 0
I0626 05:35:06.216608 139724881797504 executor.py:115] TRAIN Batch 96/1000 loss 3.818258 loss_att 2.885221 loss_ctc 5.995343 lr 0.00074178 rank 0
I0626 05:35:16.948415 139724881797504 executor.py:115] TRAIN Batch 96/1100 loss 10.500868 loss_att 7.741248 loss_ctc 16.939981 lr 0.00074153 rank 0
I0626 05:35:27.888056 139724881797504 executor.py:115] TRAIN Batch 96/1200 loss 10.466324 loss_att 6.926268 loss_ctc 18.726454 lr 0.00074125 rank 0
I0626 05:35:39.275499 139724881797504 executor.py:115] TRAIN Batch 96/1300 loss 5.309847 loss_att 4.239063 loss_ctc 7.808343 lr 0.00074101 rank 0
I0626 05:35:50.733590 139724881797504 executor.py:115] TRAIN Batch 96/1400 loss 6.641628 loss_att 3.784203 loss_ctc 13.308954 lr 0.00074076 rank 0
I0626 05:36:01.732883 139724881797504 executor.py:115] TRAIN Batch 96/1500 loss 5.734501 loss_att 4.731250 loss_ctc 8.075420 lr 0.00074052 rank 0
I0626 05:36:07.312982 139724881797504 executor.py:152] CV Batch 96/0 loss 17.433891 loss_att 16.514288 loss_ctc 19.579636 history loss 15.496792 rank 0
I0626 05:36:11.338150 139724881797504 executor.py:152] CV Batch 96/100 loss 40.879879 loss_att 42.200275 loss_ctc 37.798958 history loss 28.462575 rank 0
I0626 05:36:14.324163 139724881797504 train.py:288] Epoch 96 CV info cv_loss 33.00064937168289
I0626 05:36:14.324379 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/96.pt
I0626 05:36:14.584636 139724881797504 train.py:274] Epoch 97 TRAIN info lr 0.000740517298723176
I0626 05:36:14.587393 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:36:14.587452 139724881797504 executor.py:46] total epoch is 97.
I0626 05:36:22.327034 139724881797504 executor.py:115] TRAIN Batch 97/0 loss 2.952604 loss_att 2.011614 loss_ctc 5.148248 lr 0.00074048 rank 0
I0626 05:36:33.344604 139724881797504 executor.py:115] TRAIN Batch 97/100 loss 9.898563 loss_att 7.035163 loss_ctc 16.579830 lr 0.00074023 rank 0
I0626 05:36:44.420559 139724881797504 executor.py:115] TRAIN Batch 97/200 loss 8.336272 loss_att 5.739617 loss_ctc 14.395133 lr 0.00073999 rank 0
I0626 05:36:55.901122 139724881797504 executor.py:115] TRAIN Batch 97/300 loss 6.748146 loss_att 4.105428 loss_ctc 12.914490 lr 0.00073975 rank 0
I0626 05:37:07.268157 139724881797504 executor.py:115] TRAIN Batch 97/400 loss 5.621542 loss_att 4.174550 loss_ctc 8.997858 lr 0.00073946 rank 0
I0626 05:37:18.926031 139724881797504 executor.py:115] TRAIN Batch 97/500 loss 5.776503 loss_att 4.030177 loss_ctc 9.851263 lr 0.00073922 rank 0
I0626 05:37:29.383788 139724881797504 executor.py:115] TRAIN Batch 97/600 loss 6.575152 loss_att 5.408712 loss_ctc 9.296844 lr 0.00073898 rank 0
I0626 05:37:40.470707 139724881797504 executor.py:115] TRAIN Batch 97/700 loss 7.917502 loss_att 4.710626 loss_ctc 15.400213 lr 0.00073874 rank 0
I0626 05:37:51.666460 139724881797504 executor.py:115] TRAIN Batch 97/800 loss 5.591629 loss_att 3.678142 loss_ctc 10.056431 lr 0.00073845 rank 0
I0626 05:38:03.167589 139724881797504 executor.py:115] TRAIN Batch 97/900 loss 5.215796 loss_att 2.976552 loss_ctc 10.440698 lr 0.00073821 rank 0
I0626 05:38:14.816323 139724881797504 executor.py:115] TRAIN Batch 97/1000 loss 4.896632 loss_att 3.823582 loss_ctc 7.400414 lr 0.00073797 rank 0
I0626 05:38:25.817541 139724881797504 executor.py:115] TRAIN Batch 97/1100 loss 7.188082 loss_att 4.545977 loss_ctc 13.352995 lr 0.00073773 rank 0
I0626 05:38:37.124261 139724881797504 executor.py:115] TRAIN Batch 97/1200 loss 6.605511 loss_att 4.441730 loss_ctc 11.654333 lr 0.00073745 rank 0
I0626 05:38:48.404736 139724881797504 executor.py:115] TRAIN Batch 97/1300 loss 7.295516 loss_att 4.260699 loss_ctc 14.376755 lr 0.00073721 rank 0
I0626 05:38:59.436053 139724881797504 executor.py:115] TRAIN Batch 97/1400 loss 8.539621 loss_att 5.531383 loss_ctc 15.558846 lr 0.00073697 rank 0
I0626 05:39:10.345373 139724881797504 executor.py:115] TRAIN Batch 97/1500 loss 5.033381 loss_att 4.503344 loss_ctc 6.270136 lr 0.00073673 rank 0
I0626 05:39:15.908445 139724881797504 executor.py:152] CV Batch 97/0 loss 16.814699 loss_att 15.427102 loss_ctc 20.052425 history loss 14.946399 rank 0
I0626 05:39:19.888412 139724881797504 executor.py:152] CV Batch 97/100 loss 41.660648 loss_att 42.594543 loss_ctc 39.481560 history loss 28.213711 rank 0
I0626 05:39:22.880528 139724881797504 train.py:288] Epoch 97 CV info cv_loss 32.53998801530198
I0626 05:39:22.880749 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/97.pt
I0626 05:39:23.146468 139724881797504 train.py:274] Epoch 98 TRAIN info lr 0.0007367294616134238
I0626 05:39:23.149518 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:39:23.149578 139724881797504 executor.py:46] total epoch is 98.
I0626 05:39:30.933920 139724881797504 executor.py:115] TRAIN Batch 98/0 loss 5.591979 loss_att 2.373439 loss_ctc 13.101906 lr 0.00073669 rank 0
I0626 05:39:41.994019 139724881797504 executor.py:115] TRAIN Batch 98/100 loss 7.882099 loss_att 5.566336 loss_ctc 13.285544 lr 0.00073645 rank 0
I0626 05:39:52.884551 139724881797504 executor.py:115] TRAIN Batch 98/200 loss 8.028166 loss_att 5.630811 loss_ctc 13.621992 lr 0.00073621 rank 0
I0626 05:40:03.907424 139724881797504 executor.py:115] TRAIN Batch 98/300 loss 5.894616 loss_att 3.787253 loss_ctc 10.811796 lr 0.00073597 rank 0
I0626 05:40:15.415190 139724881797504 executor.py:115] TRAIN Batch 98/400 loss 4.673207 loss_att 3.230099 loss_ctc 8.040459 lr 0.00073569 rank 0
I0626 05:40:27.047229 139724881797504 executor.py:115] TRAIN Batch 98/500 loss 4.238678 loss_att 2.427964 loss_ctc 8.463677 lr 0.00073545 rank 0
I0626 05:40:37.401407 139724881797504 executor.py:115] TRAIN Batch 98/600 loss 11.230362 loss_att 8.439675 loss_ctc 17.741964 lr 0.00073521 rank 0
I0626 05:40:48.425468 139724881797504 executor.py:115] TRAIN Batch 98/700 loss 7.826176 loss_att 5.288511 loss_ctc 13.747391 lr 0.00073498 rank 0
I0626 05:41:00.016358 139724881797504 executor.py:115] TRAIN Batch 98/800 loss 6.508503 loss_att 5.027570 loss_ctc 9.964013 lr 0.00073470 rank 0
I0626 05:41:11.648021 139724881797504 executor.py:115] TRAIN Batch 98/900 loss 7.739369 loss_att 5.014883 loss_ctc 14.096504 lr 0.00073446 rank 0
I0626 05:41:22.908508 139724881797504 executor.py:115] TRAIN Batch 98/1000 loss 6.889849 loss_att 4.166352 loss_ctc 13.244675 lr 0.00073422 rank 0
I0626 05:41:33.730096 139724881797504 executor.py:115] TRAIN Batch 98/1100 loss 7.457148 loss_att 5.944042 loss_ctc 10.987726 lr 0.00073399 rank 0
I0626 05:41:44.693241 139724881797504 executor.py:115] TRAIN Batch 98/1200 loss 7.240825 loss_att 4.536049 loss_ctc 13.551970 lr 0.00073371 rank 0
I0626 05:41:55.716952 139724881797504 executor.py:115] TRAIN Batch 98/1300 loss 9.343229 loss_att 5.996017 loss_ctc 17.153389 lr 0.00073347 rank 0
I0626 05:42:06.937863 139724881797504 executor.py:115] TRAIN Batch 98/1400 loss 7.947270 loss_att 5.008001 loss_ctc 14.805565 lr 0.00073324 rank 0
I0626 05:42:18.105670 139724881797504 executor.py:115] TRAIN Batch 98/1500 loss 5.682282 loss_att 4.195566 loss_ctc 9.151287 lr 0.00073300 rank 0
I0626 05:42:23.664958 139724881797504 executor.py:152] CV Batch 98/0 loss 16.423170 loss_att 15.539046 loss_ctc 18.486126 history loss 14.598373 rank 0
I0626 05:42:27.652097 139724881797504 executor.py:152] CV Batch 98/100 loss 40.624825 loss_att 42.496109 loss_ctc 36.258499 history loss 28.820733 rank 0
I0626 05:42:30.659708 139724881797504 train.py:288] Epoch 98 CV info cv_loss 32.95723320268204
I0626 05:42:30.659925 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/98.pt
I0626 05:42:30.923606 139724881797504 train.py:274] Epoch 99 TRAIN info lr 0.0007329991618892375
I0626 05:42:30.926652 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:42:30.926712 139724881797504 executor.py:46] total epoch is 99.
I0626 05:42:38.693973 139724881797504 executor.py:115] TRAIN Batch 99/0 loss 3.985249 loss_att 2.717633 loss_ctc 6.943019 lr 0.00073296 rank 0
I0626 05:42:49.544578 139724881797504 executor.py:115] TRAIN Batch 99/100 loss 6.472347 loss_att 4.645703 loss_ctc 10.734516 lr 0.00073272 rank 0
I0626 05:43:00.854085 139724881797504 executor.py:115] TRAIN Batch 99/200 loss 7.975859 loss_att 5.534327 loss_ctc 13.672764 lr 0.00073249 rank 0
I0626 05:43:11.850256 139724881797504 executor.py:115] TRAIN Batch 99/300 loss 8.745138 loss_att 5.287431 loss_ctc 16.813122 lr 0.00073225 rank 0
I0626 05:43:23.728791 139724881797504 executor.py:115] TRAIN Batch 99/400 loss 7.784222 loss_att 4.500137 loss_ctc 15.447085 lr 0.00073198 rank 0
I0626 05:43:35.353541 139724881797504 executor.py:115] TRAIN Batch 99/500 loss 2.530695 loss_att 1.909810 loss_ctc 3.979428 lr 0.00073174 rank 0
I0626 05:43:45.749192 139724881797504 executor.py:115] TRAIN Batch 99/600 loss 9.951526 loss_att 7.057794 loss_ctc 16.703568 lr 0.00073151 rank 0
I0626 05:43:57.088692 139724881797504 executor.py:115] TRAIN Batch 99/700 loss 6.137545 loss_att 3.726614 loss_ctc 11.763052 lr 0.00073127 rank 0
I0626 05:44:08.837286 139724881797504 executor.py:115] TRAIN Batch 99/800 loss 4.158413 loss_att 3.196438 loss_ctc 6.403020 lr 0.00073100 rank 0
I0626 05:44:20.075395 139724881797504 executor.py:115] TRAIN Batch 99/900 loss 4.527469 loss_att 3.422285 loss_ctc 7.106230 lr 0.00073076 rank 0
I0626 05:44:30.983418 139724881797504 executor.py:115] TRAIN Batch 99/1000 loss 4.264142 loss_att 2.816505 loss_ctc 7.641963 lr 0.00073053 rank 0
I0626 05:44:41.784232 139724881797504 executor.py:115] TRAIN Batch 99/1100 loss 7.095654 loss_att 5.954683 loss_ctc 9.757917 lr 0.00073030 rank 0
I0626 05:44:53.485928 139724881797504 executor.py:115] TRAIN Batch 99/1200 loss 7.430849 loss_att 5.089085 loss_ctc 12.894967 lr 0.00073002 rank 0
I0626 05:45:04.838437 139724881797504 executor.py:115] TRAIN Batch 99/1300 loss 5.606535 loss_att 3.465459 loss_ctc 10.602381 lr 0.00072979 rank 0
I0626 05:45:15.910618 139724881797504 executor.py:115] TRAIN Batch 99/1400 loss 8.321104 loss_att 5.533721 loss_ctc 14.824998 lr 0.00072956 rank 0
I0626 05:45:26.518616 139724881797504 executor.py:115] TRAIN Batch 99/1500 loss 6.642449 loss_att 5.434194 loss_ctc 9.461712 lr 0.00072932 rank 0
I0626 05:45:32.090652 139724881797504 executor.py:152] CV Batch 99/0 loss 17.346268 loss_att 15.382959 loss_ctc 21.927324 history loss 15.418905 rank 0
I0626 05:45:36.155803 139724881797504 executor.py:152] CV Batch 99/100 loss 40.895737 loss_att 41.728279 loss_ctc 38.953140 history loss 28.522660 rank 0
I0626 05:45:39.209920 139724881797504 train.py:288] Epoch 99 CV info cv_loss 32.80853701136949
I0626 05:45:39.210093 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/99.pt
I0626 05:45:39.466950 139724881797504 train.py:274] Epoch 100 TRAIN info lr 0.0007293249574894729
I0626 05:45:39.467470 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.467542 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.467600 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.467654 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.467706 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.467783 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.467837 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.467888 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.467939 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.467990 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468069 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.468127 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.468179 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468230 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468280 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468353 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.468407 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.468457 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468507 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468557 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468674 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.468728 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.468779 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468838 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468899 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.468986 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469051 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469113 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469166 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469217 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469293 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469346 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469396 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469446 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469496 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469572 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469626 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469677 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469727 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469777 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.469886 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469939 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.469989 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470040 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470090 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470162 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.470214 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.470265 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470315 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470364 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470442 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.470495 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.470545 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470595 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470645 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470716 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.470769 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.470819 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470869 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.470919 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471029 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471082 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471133 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471183 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471233 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471305 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471367 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471418 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471468 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471518 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471594 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471647 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471697 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471747 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471796 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.471867 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471919 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.471970 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472027 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472077 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472188 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.472242 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.472292 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472343 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472393 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472464 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.472516 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.472567 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472617 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472667 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472742 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.472795 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.472846 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472897 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.472946 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473016 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473073 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473124 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473174 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473223 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473334 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473389 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473439 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473489 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473539 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473610 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473662 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473713 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473762 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473813 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.473887 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473940 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.473990 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474040 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474090 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474162 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.474215 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.474265 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474315 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474365 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474475 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.474528 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.474578 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474629 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474678 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474750 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.474802 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.474852 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474901 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.474952 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475028 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475082 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475132 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475182 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475230 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475301 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475360 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475411 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475460 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475510 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475619 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475672 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475723 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475773 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475822 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.475896 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475949 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.475999 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476048 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476098 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476181 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.476234 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.476284 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476335 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476385 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476456 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.476508 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.476559 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476608 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476658 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476768 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.476821 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.476872 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476923 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.476972 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477044 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477096 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477146 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477196 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477246 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477322 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477374 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477424 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477473 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477522 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477592 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477645 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477695 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477745 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477795 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.477906 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.477960 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478010 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478060 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478110 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478180 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478232 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478283 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478332 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478382 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478458 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478511 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478560 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478610 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478659 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478731 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478783 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.478832 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478881 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.478931 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479039 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479092 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479142 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479191 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479241 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479310 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479379 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479429 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479478 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479526 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479604 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479655 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479705 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479754 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479803 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.479875 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479927 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.479977 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480027 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480076 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480186 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.480239 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.480289 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480339 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480388 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480459 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.480511 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.480560 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480609 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480658 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480734 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.480785 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.480834 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480884 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.480933 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.481002 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.481054 139724881797504 tensor_quantizer.py:180] Enable MaxCalibrator
I0626 05:45:39.481104 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.481153 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.481204 139724881797504 tensor_quantizer.py:184] Disable `quant` stage.
I0626 05:45:39.483892 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:45:39.483959 139724881797504 executor.py:46] total epoch is 100.
I0626 05:45:47.065529 139724881797504 max.py:61] Calibrator encountered negative values. It shouldn't happen after ReLU. Make sure this is the right tensor to calibrate.
I0626 05:45:47.335699 139724881797504 executor.py:115] TRAIN Batch 100/0 loss 4.019780 loss_att 2.348825 loss_ctc 7.918674 lr 0.00072929 rank 0
I0626 05:46:01.017227 139724881797504 executor.py:115] TRAIN Batch 100/100 loss 8.663811 loss_att 6.293995 loss_ctc 14.193382 lr 0.00072905 rank 0
I0626 05:46:14.662823 139724881797504 executor.py:115] TRAIN Batch 100/200 loss 7.813941 loss_att 5.679285 loss_ctc 12.794804 lr 0.00072882 rank 0
I0626 05:46:28.700417 139724881797504 executor.py:115] TRAIN Batch 100/300 loss 5.111083 loss_att 3.915606 loss_ctc 7.900529 lr 0.00072859 rank 0
I0626 05:46:43.000343 139724881797504 executor.py:115] TRAIN Batch 100/400 loss 5.142224 loss_att 3.096408 loss_ctc 9.915795 lr 0.00072832 rank 0
I0626 05:46:57.217523 139724881797504 executor.py:115] TRAIN Batch 100/500 loss 5.324832 loss_att 3.053526 loss_ctc 10.624547 lr 0.00072809 rank 0
I0626 05:47:10.050103 139724881797504 executor.py:115] TRAIN Batch 100/600 loss 6.798524 loss_att 5.513049 loss_ctc 9.797966 lr 0.00072786 rank 0
I0626 05:47:23.723068 139724881797504 executor.py:115] TRAIN Batch 100/700 loss 6.344388 loss_att 4.948422 loss_ctc 9.601641 lr 0.00072762 rank 0
I0626 05:47:37.689034 139724881797504 executor.py:115] TRAIN Batch 100/800 loss 9.782752 loss_att 5.405362 loss_ctc 19.996662 lr 0.00072735 rank 0
I0626 05:47:51.906888 139724881797504 executor.py:115] TRAIN Batch 100/900 loss 6.807019 loss_att 4.150651 loss_ctc 13.005210 lr 0.00072712 rank 0
I0626 05:48:05.780795 139724881797504 executor.py:115] TRAIN Batch 100/1000 loss 5.375620 loss_att 3.582455 loss_ctc 9.559669 lr 0.00072689 rank 0
I0626 05:48:19.413495 139724881797504 executor.py:115] TRAIN Batch 100/1100 loss 5.350803 loss_att 4.147285 loss_ctc 8.159014 lr 0.00072666 rank 0
I0626 05:48:33.470124 139724881797504 executor.py:115] TRAIN Batch 100/1200 loss 7.211874 loss_att 5.513935 loss_ctc 11.173731 lr 0.00072639 rank 0
I0626 05:48:47.447563 139724881797504 executor.py:115] TRAIN Batch 100/1300 loss 6.026355 loss_att 3.941593 loss_ctc 10.890800 lr 0.00072616 rank 0
I0626 05:49:01.358536 139724881797504 executor.py:115] TRAIN Batch 100/1400 loss 5.919519 loss_att 4.078922 loss_ctc 10.214247 lr 0.00072593 rank 0
I0626 05:49:15.073014 139724881797504 executor.py:115] TRAIN Batch 100/1500 loss 5.124105 loss_att 4.238555 loss_ctc 7.190391 lr 0.00072571 rank 0
I0626 05:49:20.681697 139724881797504 executor.py:152] CV Batch 100/0 loss 16.102814 loss_att 14.890675 loss_ctc 18.931139 history loss 14.313612 rank 0
I0626 05:49:27.381749 139724881797504 executor.py:152] CV Batch 100/100 loss 40.124294 loss_att 41.113758 loss_ctc 37.815544 history loss 28.360509 rank 0
I0626 05:49:32.390011 139724881797504 train.py:288] Epoch 100 CV info cv_loss 32.97482916332173
I0626 05:49:32.390186 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/100.pt
I0626 05:49:32.647904 139724881797504 train.py:274] Epoch 101 TRAIN info lr 0.0007257054564503447
W0626 05:49:32.648414 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.648488 139724881797504 tensor_quantizer.py:239] Call .cuda() if running on GPU after loading calibrated amax.
W0626 05:49:32.648575 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.648650 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.648712 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.648771 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.648823 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.648875 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.648944 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.649016 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.649085 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.649140 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.649191 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.649241 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.649291 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.649361 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.649432 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.649500 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.649565 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.649626 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.649687 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.649748 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.649827 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.649902 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.649967 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.650021 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.650072 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.650122 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.650172 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.650278 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.650350 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.650417 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.650472 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.650523 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.650574 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.650624 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.650688 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.650759 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.650825 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.650879 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.650930 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.650980 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.651030 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.651099 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.651169 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.651235 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.651289 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.651348 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.651399 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.651449 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.651515 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.651587 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.651654 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.651708 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.651760 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.651811 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.651861 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.651964 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.652035 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.652102 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.652155 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.652206 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.652255 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.652305 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.652370 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.652440 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.652507 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.652561 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.652611 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.652661 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.652711 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.652780 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.652851 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.652916 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.652970 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.653022 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.653072 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.653123 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.653186 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.653256 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.653322 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.653375 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.653425 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.653475 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.653525 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.653628 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.653699 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.653766 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.653820 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.653872 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.653922 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.653973 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.654037 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.654106 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.654172 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.654225 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.654278 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.654328 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.654378 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.654448 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.654517 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.654582 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.654636 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.654687 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.654737 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.654787 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.654852 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.654923 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.654992 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.655046 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.655098 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.655149 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.655200 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.655303 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.655393 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.655460 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.655514 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.655566 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.655617 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.655668 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.655732 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.655802 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.655869 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.655922 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.655974 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.656024 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.656075 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.656143 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.656212 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.656278 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.656332 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.656383 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.656433 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.656483 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.656547 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.656618 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.656683 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.656738 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.656788 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.656838 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.656889 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.656992 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.657065 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.657132 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.657186 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.657238 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.657289 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.657339 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.657403 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.657474 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.657538 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.657592 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.657642 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.657692 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.657741 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.657809 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.657880 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.657945 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.657998 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.658048 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.658098 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.658147 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.658210 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.658278 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.658345 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.658398 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.658450 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.658500 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.658549 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.658651 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.658721 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.658786 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.658838 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.658889 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.658938 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.658987 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.659050 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.659119 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.659186 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.659380 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.659435 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.659486 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.659535 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.659605 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.659677 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.659742 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.659797 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.659848 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.659897 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.660348 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.660416 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.660486 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.660549 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.660599 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.660647 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.660694 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.660742 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.660841 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.660910 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.660974 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.661025 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.661073 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.661121 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.661168 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.661231 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.661299 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.661364 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.661416 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.661466 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.661514 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.661561 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.661635 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.661704 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.661767 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.661817 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.661865 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.661913 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.661961 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.662022 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.662091 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.662156 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.662208 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.662257 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.662305 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.662352 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.662451 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.662520 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.662585 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.662638 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.662687 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.662736 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.662783 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.662845 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.662912 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.662976 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.663027 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.663076 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.663124 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.663171 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.663237 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.663305 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.663386 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.663439 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.663488 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.663536 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.663583 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.663645 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.663713 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.663776 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.663827 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.663875 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.663923 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.663971 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.664075 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.664143 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.664208 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.664262 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.664310 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.664358 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.664405 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.664465 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.664531 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.664594 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.664645 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.664693 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.664741 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.664788 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.664854 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.664920 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.664983 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.665034 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.665082 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.665129 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.665177 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.665238 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.665306 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.665370 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.665422 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.665471 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.665518 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.665566 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.665664 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.665735 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.665799 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.665851 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.665899 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.665947 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.665994 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.666055 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.666122 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.666186 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.666239 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.666287 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.666335 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.666383 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.666449 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.666517 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.666581 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.666632 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.666682 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.666729 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.666778 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.666840 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.666907 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.666972 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.667023 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.667072 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.667119 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.667166 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.667267 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.667343 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.667409 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.667461 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.667509 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.667556 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.667603 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.667665 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.667730 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.667794 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.667847 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.667895 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.667944 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.667992 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.668063 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.668132 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.668194 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.668245 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.668293 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.668340 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.668387 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
W0626 05:49:32.668447 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.668513 139724881797504 tensor_quantizer.py:238] Load calibrated amax, shape=torch.Size([]).
W0626 05:49:32.668577 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
W0626 05:49:32.668628 139724881797504 tensor_quantizer.py:174] Disable MaxCalibrator
I0626 05:49:32.668676 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.668723 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.668771 139724881797504 tensor_quantizer.py:188] Enable `quant` stage.
I0626 05:49:32.671456 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:49:32.671515 139724881797504 executor.py:46] total epoch is 101.
I0626 05:49:40.571992 139724881797504 executor.py:115] TRAIN Batch 101/0 loss 5.833925 loss_att 3.541231 loss_ctc 11.183546 lr 0.00072567 rank 0
I0626 05:49:57.555613 139724881797504 executor.py:115] TRAIN Batch 101/100 loss 8.154426 loss_att 6.555987 loss_ctc 11.884115 lr 0.00072544 rank 0
I0626 05:50:14.525402 139724881797504 executor.py:115] TRAIN Batch 101/200 loss 5.469970 loss_att 3.722950 loss_ctc 9.546350 lr 0.00072521 rank 0
I0626 05:50:31.965505 139724881797504 executor.py:115] TRAIN Batch 101/300 loss 6.469341 loss_att 4.163591 loss_ctc 11.849424 lr 0.00072498 rank 0
I0626 05:50:50.160311 139724881797504 executor.py:115] TRAIN Batch 101/400 loss 3.711940 loss_att 2.763815 loss_ctc 5.924232 lr 0.00072471 rank 0
I0626 05:51:08.313431 139724881797504 executor.py:115] TRAIN Batch 101/500 loss 6.258914 loss_att 4.036862 loss_ctc 11.443702 lr 0.00072449 rank 0
I0626 05:51:24.889340 139724881797504 executor.py:115] TRAIN Batch 101/600 loss 7.987095 loss_att 6.540906 loss_ctc 11.361536 lr 0.00072426 rank 0
I0626 05:51:42.869812 139724881797504 executor.py:115] TRAIN Batch 101/700 loss 8.125658 loss_att 5.297063 loss_ctc 14.725712 lr 0.00072403 rank 0
I0626 05:52:00.984481 139724881797504 executor.py:115] TRAIN Batch 101/800 loss 6.883920 loss_att 4.223779 loss_ctc 13.090914 lr 0.00072376 rank 0
I0626 05:52:18.679737 139724881797504 executor.py:115] TRAIN Batch 101/900 loss 5.408472 loss_att 3.301389 loss_ctc 10.324999 lr 0.00072354 rank 0
I0626 05:52:36.359658 139724881797504 executor.py:115] TRAIN Batch 101/1000 loss 6.296568 loss_att 4.124392 loss_ctc 11.364979 lr 0.00072331 rank 0
I0626 05:52:53.303154 139724881797504 executor.py:115] TRAIN Batch 101/1100 loss 7.536586 loss_att 5.807901 loss_ctc 11.570185 lr 0.00072308 rank 0
I0626 05:53:11.086270 139724881797504 executor.py:115] TRAIN Batch 101/1200 loss 8.180878 loss_att 5.836828 loss_ctc 13.650328 lr 0.00072282 rank 0
I0626 05:53:28.314669 139724881797504 executor.py:115] TRAIN Batch 101/1300 loss 6.951750 loss_att 4.662873 loss_ctc 12.292461 lr 0.00072259 rank 0
I0626 05:53:45.438341 139724881797504 executor.py:115] TRAIN Batch 101/1400 loss 5.611692 loss_att 3.502002 loss_ctc 10.534302 lr 0.00072237 rank 0
I0626 05:54:02.706304 139724881797504 executor.py:115] TRAIN Batch 101/1500 loss 6.366559 loss_att 3.891785 loss_ctc 12.141029 lr 0.00072214 rank 0
I0626 05:54:08.333679 139724881797504 executor.py:152] CV Batch 101/0 loss 15.288616 loss_att 13.878310 loss_ctc 18.579329 history loss 13.589881 rank 0
I0626 05:54:17.558555 139724881797504 executor.py:152] CV Batch 101/100 loss 39.271511 loss_att 40.345810 loss_ctc 36.764812 history loss 27.719209 rank 0
I0626 05:54:24.475858 139724881797504 train.py:288] Epoch 101 CV info cv_loss 32.44181198684469
I0626 05:54:24.475983 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/101.pt
I0626 05:54:24.741949 139724881797504 train.py:274] Epoch 102 TRAIN info lr 0.0007221393146897689
I0626 05:54:24.744996 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:54:24.745068 139724881797504 executor.py:46] total epoch is 102.
I0626 05:54:32.855410 139724881797504 executor.py:115] TRAIN Batch 102/0 loss 5.636739 loss_att 2.813239 loss_ctc 12.224904 lr 0.00072210 rank 0
I0626 05:54:50.285998 139724881797504 executor.py:115] TRAIN Batch 102/100 loss 8.088850 loss_att 5.432833 loss_ctc 14.286221 lr 0.00072188 rank 0
I0626 05:55:07.245843 139724881797504 executor.py:115] TRAIN Batch 102/200 loss 7.250504 loss_att 4.977577 loss_ctc 12.554003 lr 0.00072165 rank 0
I0626 05:55:24.677138 139724881797504 executor.py:115] TRAIN Batch 102/300 loss 6.243150 loss_att 3.841513 loss_ctc 11.846969 lr 0.00072142 rank 0
I0626 05:55:42.324400 139724881797504 executor.py:115] TRAIN Batch 102/400 loss 4.610582 loss_att 3.331215 loss_ctc 7.595772 lr 0.00072116 rank 0
I0626 05:56:00.201626 139724881797504 executor.py:115] TRAIN Batch 102/500 loss 4.531947 loss_att 3.371738 loss_ctc 7.239100 lr 0.00072094 rank 0
I0626 05:56:16.693292 139724881797504 executor.py:115] TRAIN Batch 102/600 loss 9.404439 loss_att 6.904420 loss_ctc 15.237817 lr 0.00072071 rank 0
I0626 05:56:34.101348 139724881797504 executor.py:115] TRAIN Batch 102/700 loss 8.723445 loss_att 4.795774 loss_ctc 17.888012 lr 0.00072049 rank 0
I0626 05:56:51.829236 139724881797504 executor.py:115] TRAIN Batch 102/800 loss 6.340462 loss_att 3.769650 loss_ctc 12.339022 lr 0.00072023 rank 0
I0626 05:57:09.359613 139724881797504 executor.py:115] TRAIN Batch 102/900 loss 3.783077 loss_att 2.672118 loss_ctc 6.375312 lr 0.00072000 rank 0
I0626 05:57:27.019382 139724881797504 executor.py:115] TRAIN Batch 102/1000 loss 3.213542 loss_att 2.570888 loss_ctc 4.713067 lr 0.00071978 rank 0
I0626 05:57:44.052863 139724881797504 executor.py:115] TRAIN Batch 102/1100 loss 5.168017 loss_att 3.908074 loss_ctc 8.107884 lr 0.00071955 rank 0
I0626 05:58:01.790118 139724881797504 executor.py:115] TRAIN Batch 102/1200 loss 8.216705 loss_att 5.679555 loss_ctc 14.136722 lr 0.00071929 rank 0
I0626 05:58:19.612433 139724881797504 executor.py:115] TRAIN Batch 102/1300 loss 5.381746 loss_att 3.596817 loss_ctc 9.546580 lr 0.00071907 rank 0
I0626 05:58:37.518772 139724881797504 executor.py:115] TRAIN Batch 102/1400 loss 3.767994 loss_att 3.033295 loss_ctc 5.482292 lr 0.00071885 rank 0
I0626 05:58:55.024841 139724881797504 executor.py:115] TRAIN Batch 102/1500 loss 9.546232 loss_att 6.390781 loss_ctc 16.908949 lr 0.00071863 rank 0
I0626 05:59:00.675992 139724881797504 executor.py:152] CV Batch 102/0 loss 15.920534 loss_att 15.127564 loss_ctc 17.770794 history loss 14.151586 rank 0
I0626 05:59:09.959682 139724881797504 executor.py:152] CV Batch 102/100 loss 40.668930 loss_att 41.706871 loss_ctc 38.247066 history loss 27.546385 rank 0
I0626 05:59:16.909000 139724881797504 train.py:288] Epoch 102 CV info cv_loss 32.02928876643903
I0626 05:59:16.909253 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/102.pt
I0626 05:59:17.184481 139724881797504 train.py:274] Epoch 103 TRAIN info lr 0.0007186252339103059
I0626 05:59:17.187420 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 05:59:17.187490 139724881797504 executor.py:46] total epoch is 103.
I0626 05:59:25.121879 139724881797504 executor.py:115] TRAIN Batch 103/0 loss 4.460197 loss_att 2.589014 loss_ctc 8.826290 lr 0.00071859 rank 0
I0626 05:59:42.083471 139724881797504 executor.py:115] TRAIN Batch 103/100 loss 5.936461 loss_att 4.692723 loss_ctc 8.838518 lr 0.00071837 rank 0
I0626 05:59:59.634794 139724881797504 executor.py:115] TRAIN Batch 103/200 loss 5.796937 loss_att 3.900423 loss_ctc 10.222137 lr 0.00071814 rank 0
I0626 06:00:17.240204 139724881797504 executor.py:115] TRAIN Batch 103/300 loss 5.993342 loss_att 4.380985 loss_ctc 9.755509 lr 0.00071792 rank 0
I0626 06:00:34.878529 139724881797504 executor.py:115] TRAIN Batch 103/400 loss 7.337866 loss_att 4.207948 loss_ctc 14.641008 lr 0.00071766 rank 0
I0626 06:00:52.926990 139724881797504 executor.py:115] TRAIN Batch 103/500 loss 5.547805 loss_att 2.832442 loss_ctc 11.883654 lr 0.00071744 rank 0
I0626 06:01:09.372355 139724881797504 executor.py:115] TRAIN Batch 103/600 loss 7.531475 loss_att 5.096594 loss_ctc 13.212862 lr 0.00071722 rank 0
I0626 06:01:26.830906 139724881797504 executor.py:115] TRAIN Batch 103/700 loss 7.441231 loss_att 5.930015 loss_ctc 10.967401 lr 0.00071700 rank 0
I0626 06:01:45.035514 139724881797504 executor.py:115] TRAIN Batch 103/800 loss 8.080043 loss_att 5.705002 loss_ctc 13.621805 lr 0.00071674 rank 0
I0626 06:02:02.783286 139724881797504 executor.py:115] TRAIN Batch 103/900 loss 7.162125 loss_att 4.064358 loss_ctc 14.390247 lr 0.00071652 rank 0
I0626 06:02:19.951706 139724881797504 executor.py:115] TRAIN Batch 103/1000 loss 7.074229 loss_att 4.070573 loss_ctc 14.082760 lr 0.00071630 rank 0
I0626 06:02:37.105542 139724881797504 executor.py:115] TRAIN Batch 103/1100 loss 7.393990 loss_att 5.952300 loss_ctc 10.757933 lr 0.00071608 rank 0
I0626 06:02:54.593325 139724881797504 executor.py:115] TRAIN Batch 103/1200 loss 9.538324 loss_att 6.381155 loss_ctc 16.905050 lr 0.00071582 rank 0
I0626 06:03:12.166656 139724881797504 executor.py:115] TRAIN Batch 103/1300 loss 7.122532 loss_att 4.477329 loss_ctc 13.294672 lr 0.00071560 rank 0
I0626 06:03:29.870353 139724881797504 executor.py:115] TRAIN Batch 103/1400 loss 6.421361 loss_att 4.337665 loss_ctc 11.283319 lr 0.00071538 rank 0
I0626 06:03:47.036845 139724881797504 executor.py:115] TRAIN Batch 103/1500 loss 7.682683 loss_att 5.376603 loss_ctc 13.063538 lr 0.00071516 rank 0
I0626 06:03:52.673377 139724881797504 executor.py:152] CV Batch 103/0 loss 15.260125 loss_att 14.002863 loss_ctc 18.193737 history loss 13.564556 rank 0
I0626 06:04:01.925567 139724881797504 executor.py:152] CV Batch 103/100 loss 39.849983 loss_att 40.975292 loss_ctc 37.224258 history loss 27.470744 rank 0
I0626 06:04:08.840611 139724881797504 train.py:288] Epoch 103 CV info cv_loss 31.948022079733978
I0626 06:04:08.840823 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/103.pt
I0626 06:04:09.113692 139724881797504 train.py:274] Epoch 104 TRAIN info lr 0.0007151619596132788
I0626 06:04:09.116877 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:04:09.116946 139724881797504 executor.py:46] total epoch is 104.
I0626 06:04:16.895269 139724881797504 executor.py:115] TRAIN Batch 104/0 loss 3.128279 loss_att 1.414024 loss_ctc 7.128208 lr 0.00071513 rank 0
I0626 06:04:33.879873 139724881797504 executor.py:115] TRAIN Batch 104/100 loss 6.273913 loss_att 4.351950 loss_ctc 10.758494 lr 0.00071491 rank 0
I0626 06:04:50.851739 139724881797504 executor.py:115] TRAIN Batch 104/200 loss 6.198649 loss_att 4.500373 loss_ctc 10.161293 lr 0.00071469 rank 0
I0626 06:05:08.591221 139724881797504 executor.py:115] TRAIN Batch 104/300 loss 4.775891 loss_att 3.817693 loss_ctc 7.011687 lr 0.00071447 rank 0
I0626 06:05:26.579842 139724881797504 executor.py:115] TRAIN Batch 104/400 loss 4.420530 loss_att 2.805289 loss_ctc 8.189426 lr 0.00071421 rank 0
I0626 06:05:44.714430 139724881797504 executor.py:115] TRAIN Batch 104/500 loss 4.020628 loss_att 2.223614 loss_ctc 8.213661 lr 0.00071399 rank 0
I0626 06:06:01.005159 139724881797504 executor.py:115] TRAIN Batch 104/600 loss 8.489311 loss_att 6.130362 loss_ctc 13.993527 lr 0.00071378 rank 0
I0626 06:06:18.699734 139724881797504 executor.py:115] TRAIN Batch 104/700 loss 6.231042 loss_att 5.038914 loss_ctc 9.012674 lr 0.00071356 rank 0
I0626 06:06:37.120824 139724881797504 executor.py:115] TRAIN Batch 104/800 loss 6.466065 loss_att 4.377173 loss_ctc 11.340147 lr 0.00071330 rank 0
I0626 06:06:54.898713 139724881797504 executor.py:115] TRAIN Batch 104/900 loss 6.879853 loss_att 4.307164 loss_ctc 12.882794 lr 0.00071309 rank 0
I0626 06:07:12.571869 139724881797504 executor.py:115] TRAIN Batch 104/1000 loss 5.021074 loss_att 2.801310 loss_ctc 10.200523 lr 0.00071287 rank 0
I0626 06:07:29.311278 139724881797504 executor.py:115] TRAIN Batch 104/1100 loss 6.970730 loss_att 5.311614 loss_ctc 10.841999 lr 0.00071265 rank 0
I0626 06:07:46.729322 139724881797504 executor.py:115] TRAIN Batch 104/1200 loss 7.880829 loss_att 5.269518 loss_ctc 13.973888 lr 0.00071240 rank 0
I0626 06:08:04.068014 139724881797504 executor.py:115] TRAIN Batch 104/1300 loss 5.876793 loss_att 4.353854 loss_ctc 9.430317 lr 0.00071218 rank 0
I0626 06:08:21.654569 139724881797504 executor.py:115] TRAIN Batch 104/1400 loss 4.896600 loss_att 2.993912 loss_ctc 9.336205 lr 0.00071196 rank 0
I0626 06:08:38.916220 139724881797504 executor.py:115] TRAIN Batch 104/1500 loss 9.127395 loss_att 5.664600 loss_ctc 17.207249 lr 0.00071175 rank 0
I0626 06:08:44.604952 139724881797504 executor.py:152] CV Batch 104/0 loss 15.202782 loss_att 13.589940 loss_ctc 18.966082 history loss 13.513584 rank 0
I0626 06:08:54.009871 139724881797504 executor.py:152] CV Batch 104/100 loss 39.990189 loss_att 41.055153 loss_ctc 37.505264 history loss 27.798315 rank 0
I0626 06:09:00.955914 139724881797504 train.py:288] Epoch 104 CV info cv_loss 32.16397853406758
I0626 06:09:00.956129 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/104.pt
I0626 06:09:01.225178 139724881797504 train.py:274] Epoch 105 TRAIN info lr 0.0007117482792171624
I0626 06:09:01.228205 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:09:01.228276 139724881797504 executor.py:46] total epoch is 105.
I0626 06:09:09.011058 139724881797504 executor.py:115] TRAIN Batch 105/0 loss 6.458381 loss_att 3.384295 loss_ctc 13.631247 lr 0.00071171 rank 0
I0626 06:09:26.128377 139724881797504 executor.py:115] TRAIN Batch 105/100 loss 6.304564 loss_att 5.363976 loss_ctc 8.499269 lr 0.00071150 rank 0
I0626 06:09:43.596068 139724881797504 executor.py:115] TRAIN Batch 105/200 loss 8.250016 loss_att 5.779440 loss_ctc 14.014693 lr 0.00071128 rank 0
I0626 06:10:01.175919 139724881797504 executor.py:115] TRAIN Batch 105/300 loss 8.884798 loss_att 5.858788 loss_ctc 15.945487 lr 0.00071106 rank 0
I0626 06:10:19.123878 139724881797504 executor.py:115] TRAIN Batch 105/400 loss 5.215747 loss_att 3.263141 loss_ctc 9.771826 lr 0.00071081 rank 0
I0626 06:10:37.623371 139724881797504 executor.py:115] TRAIN Batch 105/500 loss 5.791471 loss_att 3.812372 loss_ctc 10.409369 lr 0.00071060 rank 0
I0626 06:10:54.593802 139724881797504 executor.py:115] TRAIN Batch 105/600 loss 9.230440 loss_att 6.974843 loss_ctc 14.493498 lr 0.00071038 rank 0
I0626 06:11:12.315558 139724881797504 executor.py:115] TRAIN Batch 105/700 loss 7.709180 loss_att 4.873197 loss_ctc 14.326473 lr 0.00071017 rank 0
I0626 06:11:30.084591 139724881797504 executor.py:115] TRAIN Batch 105/800 loss 7.077103 loss_att 4.735288 loss_ctc 12.541338 lr 0.00070992 rank 0
I0626 06:11:47.945647 139724881797504 executor.py:115] TRAIN Batch 105/900 loss 5.329223 loss_att 3.977503 loss_ctc 8.483234 lr 0.00070970 rank 0
I0626 06:12:05.348369 139724881797504 executor.py:115] TRAIN Batch 105/1000 loss 4.701779 loss_att 2.967713 loss_ctc 8.747932 lr 0.00070949 rank 0
I0626 06:12:21.989803 139724881797504 executor.py:115] TRAIN Batch 105/1100 loss 5.203866 loss_att 4.499654 loss_ctc 6.847025 lr 0.00070927 rank 0
I0626 06:12:39.373734 139724881797504 executor.py:115] TRAIN Batch 105/1200 loss 7.480099 loss_att 5.530828 loss_ctc 12.028399 lr 0.00070902 rank 0
I0626 06:12:56.589457 139724881797504 executor.py:115] TRAIN Batch 105/1300 loss 4.715670 loss_att 4.422853 loss_ctc 5.398911 lr 0.00070881 rank 0
I0626 06:13:13.722731 139724881797504 executor.py:115] TRAIN Batch 105/1400 loss 4.312559 loss_att 3.057721 loss_ctc 7.240516 lr 0.00070860 rank 0
I0626 06:13:31.106004 139724881797504 executor.py:115] TRAIN Batch 105/1500 loss 5.040551 loss_att 3.937150 loss_ctc 7.615154 lr 0.00070838 rank 0
I0626 06:13:36.711282 139724881797504 executor.py:152] CV Batch 105/0 loss 15.594622 loss_att 13.945303 loss_ctc 19.443031 history loss 13.861886 rank 0
I0626 06:13:45.949514 139724881797504 executor.py:152] CV Batch 105/100 loss 38.152561 loss_att 39.974545 loss_ctc 33.901268 history loss 27.732179 rank 0
I0626 06:13:52.884969 139724881797504 train.py:288] Epoch 105 CV info cv_loss 32.18861930618233
I0626 06:13:52.885193 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/105.pt
I0626 06:13:53.190812 139724881797504 train.py:274] Epoch 106 TRAIN info lr 0.0007083830202738377
I0626 06:13:53.193793 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:13:53.193851 139724881797504 executor.py:46] total epoch is 106.
I0626 06:14:01.279025 139724881797504 executor.py:115] TRAIN Batch 106/0 loss 3.509190 loss_att 2.198260 loss_ctc 6.568024 lr 0.00070835 rank 0
I0626 06:14:18.550624 139724881797504 executor.py:115] TRAIN Batch 106/100 loss 6.507291 loss_att 5.871753 loss_ctc 7.990213 lr 0.00070813 rank 0
I0626 06:14:35.597666 139724881797504 executor.py:115] TRAIN Batch 106/200 loss 7.926717 loss_att 5.247032 loss_ctc 14.179316 lr 0.00070792 rank 0
I0626 06:14:53.103596 139724881797504 executor.py:115] TRAIN Batch 106/300 loss 5.591852 loss_att 4.368434 loss_ctc 8.446493 lr 0.00070771 rank 0
I0626 06:15:11.149973 139724881797504 executor.py:115] TRAIN Batch 106/400 loss 6.654865 loss_att 4.814934 loss_ctc 10.948038 lr 0.00070746 rank 0
I0626 06:15:29.382797 139724881797504 executor.py:115] TRAIN Batch 106/500 loss 5.640112 loss_att 4.322335 loss_ctc 8.714925 lr 0.00070725 rank 0
I0626 06:15:46.346199 139724881797504 executor.py:115] TRAIN Batch 106/600 loss 5.031536 loss_att 4.021351 loss_ctc 7.388635 lr 0.00070704 rank 0
I0626 06:16:04.210431 139724881797504 executor.py:115] TRAIN Batch 106/700 loss 8.428881 loss_att 4.959496 loss_ctc 16.524109 lr 0.00070682 rank 0
I0626 06:16:21.902317 139724881797504 executor.py:115] TRAIN Batch 106/800 loss 8.403363 loss_att 5.810818 loss_ctc 14.452637 lr 0.00070658 rank 0
I0626 06:16:39.437210 139724881797504 executor.py:115] TRAIN Batch 106/900 loss 6.022177 loss_att 3.219797 loss_ctc 12.561063 lr 0.00070637 rank 0
I0626 06:16:57.316519 139724881797504 executor.py:115] TRAIN Batch 106/1000 loss 6.981738 loss_att 3.938176 loss_ctc 14.083383 lr 0.00070615 rank 0
I0626 06:17:14.368456 139724881797504 executor.py:115] TRAIN Batch 106/1100 loss 10.204693 loss_att 7.055862 loss_ctc 17.551964 lr 0.00070594 rank 0
I0626 06:17:32.135407 139724881797504 executor.py:115] TRAIN Batch 106/1200 loss 7.887392 loss_att 4.967451 loss_ctc 14.700586 lr 0.00070570 rank 0
I0626 06:17:49.266781 139724881797504 executor.py:115] TRAIN Batch 106/1300 loss 5.453798 loss_att 4.671734 loss_ctc 7.278616 lr 0.00070549 rank 0
I0626 06:18:06.499816 139724881797504 executor.py:115] TRAIN Batch 106/1400 loss 5.568730 loss_att 4.172320 loss_ctc 8.827021 lr 0.00070528 rank 0
I0626 06:18:23.932402 139724881797504 executor.py:115] TRAIN Batch 106/1500 loss 7.965073 loss_att 6.097757 loss_ctc 12.322144 lr 0.00070507 rank 0
I0626 06:18:29.553825 139724881797504 executor.py:152] CV Batch 106/0 loss 16.506720 loss_att 14.821377 loss_ctc 20.439186 history loss 14.672640 rank 0
I0626 06:18:38.755168 139724881797504 executor.py:152] CV Batch 106/100 loss 39.280304 loss_att 40.279411 loss_ctc 36.949062 history loss 27.905916 rank 0
I0626 06:18:45.668933 139724881797504 train.py:288] Epoch 106 CV info cv_loss 32.40959062063852
I0626 06:18:45.669045 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/106.pt
I0626 06:18:45.934167 139724881797504 train.py:274] Epoch 107 TRAIN info lr 0.0007050650487767507
I0626 06:18:45.937220 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:18:45.937282 139724881797504 executor.py:46] total epoch is 107.
I0626 06:18:53.821133 139724881797504 executor.py:115] TRAIN Batch 107/0 loss 5.184124 loss_att 2.964329 loss_ctc 10.363647 lr 0.00070503 rank 0
I0626 06:19:11.633784 139724881797504 executor.py:115] TRAIN Batch 107/100 loss 7.648255 loss_att 5.161996 loss_ctc 13.449527 lr 0.00070482 rank 0
I0626 06:19:28.859883 139724881797504 executor.py:115] TRAIN Batch 107/200 loss 6.388183 loss_att 4.460478 loss_ctc 10.886160 lr 0.00070461 rank 0
I0626 06:19:46.548637 139724881797504 executor.py:115] TRAIN Batch 107/300 loss 7.022182 loss_att 4.246974 loss_ctc 13.497670 lr 0.00070440 rank 0
I0626 06:20:04.258591 139724881797504 executor.py:115] TRAIN Batch 107/400 loss 3.841668 loss_att 2.615681 loss_ctc 6.702302 lr 0.00070416 rank 0
I0626 06:20:22.613578 139724881797504 executor.py:115] TRAIN Batch 107/500 loss 4.660087 loss_att 3.360215 loss_ctc 7.693119 lr 0.00070395 rank 0
I0626 06:20:39.324301 139724881797504 executor.py:115] TRAIN Batch 107/600 loss 6.403390 loss_att 4.658741 loss_ctc 10.474236 lr 0.00070374 rank 0
I0626 06:20:57.086375 139724881797504 executor.py:115] TRAIN Batch 107/700 loss 7.897034 loss_att 4.728221 loss_ctc 15.290930 lr 0.00070353 rank 0
I0626 06:21:14.841261 139724881797504 executor.py:115] TRAIN Batch 107/800 loss 4.696153 loss_att 4.008866 loss_ctc 6.299824 lr 0.00070328 rank 0
I0626 06:21:32.627068 139724881797504 executor.py:115] TRAIN Batch 107/900 loss 4.761582 loss_att 3.377293 loss_ctc 7.991592 lr 0.00070308 rank 0
I0626 06:21:50.315867 139724881797504 executor.py:115] TRAIN Batch 107/1000 loss 3.014936 loss_att 2.147254 loss_ctc 5.039525 lr 0.00070287 rank 0
I0626 06:22:07.166054 139724881797504 executor.py:115] TRAIN Batch 107/1100 loss 8.467286 loss_att 6.333045 loss_ctc 13.447182 lr 0.00070266 rank 0
I0626 06:22:24.822088 139724881797504 executor.py:115] TRAIN Batch 107/1200 loss 8.972828 loss_att 5.129257 loss_ctc 17.941160 lr 0.00070242 rank 0
I0626 06:22:42.015915 139724881797504 executor.py:115] TRAIN Batch 107/1300 loss 8.874245 loss_att 6.087997 loss_ctc 15.375489 lr 0.00070221 rank 0
I0626 06:22:59.383637 139724881797504 executor.py:115] TRAIN Batch 107/1400 loss 5.884181 loss_att 4.161279 loss_ctc 9.904284 lr 0.00070200 rank 0
I0626 06:23:16.950419 139724881797504 executor.py:115] TRAIN Batch 107/1500 loss 6.479573 loss_att 4.475862 loss_ctc 11.154899 lr 0.00070179 rank 0
I0626 06:23:22.564920 139724881797504 executor.py:152] CV Batch 107/0 loss 15.915803 loss_att 14.502497 loss_ctc 19.213516 history loss 14.147380 rank 0
I0626 06:23:31.958674 139724881797504 executor.py:152] CV Batch 107/100 loss 39.405342 loss_att 40.234673 loss_ctc 37.470230 history loss 27.759976 rank 0
I0626 06:23:38.886336 139724881797504 train.py:288] Epoch 107 CV info cv_loss 32.30232334070412
I0626 06:23:38.886553 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/107.pt
I0626 06:23:39.155010 139724881797504 train.py:274] Epoch 108 TRAIN info lr 0.0007017932675554324
I0626 06:23:39.158073 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:23:39.158135 139724881797504 executor.py:46] total epoch is 108.
I0626 06:23:47.062357 139724881797504 executor.py:115] TRAIN Batch 108/0 loss 5.885646 loss_att 3.902216 loss_ctc 10.513649 lr 0.00070176 rank 0
I0626 06:24:04.496696 139724881797504 executor.py:115] TRAIN Batch 108/100 loss 7.743360 loss_att 5.943888 loss_ctc 11.942128 lr 0.00070155 rank 0
I0626 06:24:21.544258 139724881797504 executor.py:115] TRAIN Batch 108/200 loss 9.403291 loss_att 6.197748 loss_ctc 16.882891 lr 0.00070134 rank 0
I0626 06:24:39.291339 139724881797504 executor.py:115] TRAIN Batch 108/300 loss 5.883104 loss_att 3.910691 loss_ctc 10.485401 lr 0.00070114 rank 0
I0626 06:24:57.074551 139724881797504 executor.py:115] TRAIN Batch 108/400 loss 10.714801 loss_att 6.627018 loss_ctc 20.252960 lr 0.00070090 rank 0
I0626 06:25:14.951859 139724881797504 executor.py:115] TRAIN Batch 108/500 loss 4.636481 loss_att 2.929626 loss_ctc 8.619141 lr 0.00070069 rank 0
I0626 06:25:31.217912 139724881797504 executor.py:115] TRAIN Batch 108/600 loss 8.398434 loss_att 5.434762 loss_ctc 15.313667 lr 0.00070048 rank 0
I0626 06:25:48.513968 139724881797504 executor.py:115] TRAIN Batch 108/700 loss 4.940481 loss_att 4.217170 loss_ctc 6.628208 lr 0.00070028 rank 0
I0626 06:26:06.337342 139724881797504 executor.py:115] TRAIN Batch 108/800 loss 6.491606 loss_att 3.726013 loss_ctc 12.944655 lr 0.00070004 rank 0
I0626 06:26:24.121578 139724881797504 executor.py:115] TRAIN Batch 108/900 loss 7.469181 loss_att 4.482967 loss_ctc 14.437013 lr 0.00069983 rank 0
I0626 06:26:41.571100 139724881797504 executor.py:115] TRAIN Batch 108/1000 loss 3.030865 loss_att 1.776680 loss_ctc 5.957297 lr 0.00069963 rank 0
I0626 06:26:58.409302 139724881797504 executor.py:115] TRAIN Batch 108/1100 loss 9.451147 loss_att 6.380383 loss_ctc 16.616264 lr 0.00069942 rank 0
I0626 06:27:16.376214 139724881797504 executor.py:115] TRAIN Batch 108/1200 loss 3.383572 loss_att 2.865492 loss_ctc 4.592425 lr 0.00069918 rank 0
I0626 06:27:33.878746 139724881797504 executor.py:115] TRAIN Batch 108/1300 loss 6.120562 loss_att 4.624014 loss_ctc 9.612506 lr 0.00069898 rank 0
I0626 06:27:51.571593 139724881797504 executor.py:115] TRAIN Batch 108/1400 loss 4.942489 loss_att 3.524162 loss_ctc 8.251916 lr 0.00069877 rank 0
I0626 06:28:08.803817 139724881797504 executor.py:115] TRAIN Batch 108/1500 loss 5.038332 loss_att 3.018803 loss_ctc 9.750566 lr 0.00069857 rank 0
I0626 06:28:14.447227 139724881797504 executor.py:152] CV Batch 108/0 loss 15.973652 loss_att 14.436581 loss_ctc 19.560154 history loss 14.198802 rank 0
I0626 06:28:23.647556 139724881797504 executor.py:152] CV Batch 108/100 loss 40.270348 loss_att 40.056778 loss_ctc 40.768673 history loss 28.003528 rank 0
I0626 06:28:30.558826 139724881797504 train.py:288] Epoch 108 CV info cv_loss 32.51242946563529
I0626 06:28:30.558962 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/108.pt
I0626 06:28:30.826609 139724881797504 train.py:274] Epoch 109 TRAIN info lr 0.0006985666147512158
I0626 06:28:30.829652 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:28:30.829712 139724881797504 executor.py:46] total epoch is 109.
I0626 06:28:38.820754 139724881797504 executor.py:115] TRAIN Batch 109/0 loss 3.678507 loss_att 2.143184 loss_ctc 7.260929 lr 0.00069853 rank 0
I0626 06:28:56.033500 139724881797504 executor.py:115] TRAIN Batch 109/100 loss 9.023853 loss_att 6.026360 loss_ctc 16.018003 lr 0.00069833 rank 0
I0626 06:29:13.593905 139724881797504 executor.py:115] TRAIN Batch 109/200 loss 7.422863 loss_att 4.687120 loss_ctc 13.806261 lr 0.00069812 rank 0
I0626 06:29:31.709168 139724881797504 executor.py:115] TRAIN Batch 109/300 loss 7.313591 loss_att 3.973190 loss_ctc 15.107859 lr 0.00069792 rank 0
I0626 06:29:49.816602 139724881797504 executor.py:115] TRAIN Batch 109/400 loss 3.381318 loss_att 1.960736 loss_ctc 6.696008 lr 0.00069768 rank 0
I0626 06:30:07.671262 139724881797504 executor.py:115] TRAIN Batch 109/500 loss 3.910702 loss_att 2.179792 loss_ctc 7.949490 lr 0.00069748 rank 0
I0626 06:30:24.276381 139724881797504 executor.py:115] TRAIN Batch 109/600 loss 5.922698 loss_att 4.404185 loss_ctc 9.465893 lr 0.00069727 rank 0
I0626 06:30:41.769717 139724881797504 executor.py:115] TRAIN Batch 109/700 loss 7.980426 loss_att 5.613359 loss_ctc 13.503580 lr 0.00069707 rank 0
I0626 06:30:59.289039 139724881797504 executor.py:115] TRAIN Batch 109/800 loss 6.495284 loss_att 4.459886 loss_ctc 11.244545 lr 0.00069683 rank 0
I0626 06:31:16.812564 139724881797504 executor.py:115] TRAIN Batch 109/900 loss 4.150171 loss_att 2.853951 loss_ctc 7.174686 lr 0.00069663 rank 0
I0626 06:31:34.115237 139724881797504 executor.py:115] TRAIN Batch 109/1000 loss 2.443016 loss_att 1.867336 loss_ctc 3.786269 lr 0.00069643 rank 0
I0626 06:31:51.157368 139724881797504 executor.py:115] TRAIN Batch 109/1100 loss 7.204785 loss_att 5.693882 loss_ctc 10.730227 lr 0.00069623 rank 0
I0626 06:32:08.491522 139724881797504 executor.py:115] TRAIN Batch 109/1200 loss 5.255602 loss_att 3.258907 loss_ctc 9.914558 lr 0.00069599 rank 0
I0626 06:32:25.971473 139724881797504 executor.py:115] TRAIN Batch 109/1300 loss 6.118969 loss_att 5.014922 loss_ctc 8.695080 lr 0.00069579 rank 0
I0626 06:32:43.420105 139724881797504 executor.py:115] TRAIN Batch 109/1400 loss 5.850428 loss_att 3.968743 loss_ctc 10.241024 lr 0.00069559 rank 0
I0626 06:33:01.006173 139724881797504 executor.py:115] TRAIN Batch 109/1500 loss 7.746340 loss_att 5.343769 loss_ctc 13.352340 lr 0.00069538 rank 0
I0626 06:33:06.612265 139724881797504 executor.py:152] CV Batch 109/0 loss 15.606339 loss_att 13.605238 loss_ctc 20.275574 history loss 13.872301 rank 0
I0626 06:33:15.778965 139724881797504 executor.py:152] CV Batch 109/100 loss 39.054207 loss_att 39.497578 loss_ctc 38.019676 history loss 27.714257 rank 0
I0626 06:33:22.694982 139724881797504 train.py:288] Epoch 109 CV info cv_loss 32.09141400504262
I0626 06:33:22.695192 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/109.pt
I0626 06:33:22.962777 139724881797504 train.py:274] Epoch 110 TRAIN info lr 0.0006953840623693442
I0626 06:33:22.965731 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:33:22.965791 139724881797504 executor.py:46] total epoch is 110.
I0626 06:33:30.887850 139724881797504 executor.py:115] TRAIN Batch 110/0 loss 3.234447 loss_att 2.576064 loss_ctc 4.770673 lr 0.00069535 rank 0
I0626 06:33:47.702050 139724881797504 executor.py:115] TRAIN Batch 110/100 loss 7.108083 loss_att 5.209765 loss_ctc 11.537490 lr 0.00069515 rank 0
I0626 06:34:05.103757 139724881797504 executor.py:115] TRAIN Batch 110/200 loss 7.842245 loss_att 5.171541 loss_ctc 14.073888 lr 0.00069495 rank 0
I0626 06:34:22.932933 139724881797504 executor.py:115] TRAIN Batch 110/300 loss 8.314520 loss_att 5.397296 loss_ctc 15.121373 lr 0.00069475 rank 0
I0626 06:34:40.823436 139724881797504 executor.py:115] TRAIN Batch 110/400 loss 6.327512 loss_att 4.367656 loss_ctc 10.900507 lr 0.00069451 rank 0
I0626 06:34:58.855659 139724881797504 executor.py:115] TRAIN Batch 110/500 loss 4.080242 loss_att 2.639749 loss_ctc 7.441391 lr 0.00069431 rank 0
I0626 06:35:15.275958 139724881797504 executor.py:115] TRAIN Batch 110/600 loss 8.063787 loss_att 5.843551 loss_ctc 13.244335 lr 0.00069411 rank 0
I0626 06:35:33.665537 139724881797504 executor.py:115] TRAIN Batch 110/700 loss 5.655717 loss_att 3.714293 loss_ctc 10.185704 lr 0.00069391 rank 0
I0626 06:35:51.058850 139724881797504 executor.py:115] TRAIN Batch 110/800 loss 4.005683 loss_att 3.549405 loss_ctc 5.070333 lr 0.00069368 rank 0
I0626 06:36:08.669378 139724881797504 executor.py:115] TRAIN Batch 110/900 loss 4.262428 loss_att 3.221281 loss_ctc 6.691772 lr 0.00069348 rank 0
I0626 06:36:26.235064 139724881797504 executor.py:115] TRAIN Batch 110/1000 loss 4.998381 loss_att 2.913551 loss_ctc 9.862984 lr 0.00069328 rank 0
I0626 06:36:43.117467 139724881797504 executor.py:115] TRAIN Batch 110/1100 loss 8.077415 loss_att 6.342436 loss_ctc 12.125698 lr 0.00069308 rank 0
I0626 06:37:01.238008 139724881797504 executor.py:115] TRAIN Batch 110/1200 loss 7.183086 loss_att 4.794249 loss_ctc 12.757039 lr 0.00069284 rank 0
I0626 06:37:18.709354 139724881797504 executor.py:115] TRAIN Batch 110/1300 loss 5.474656 loss_att 4.650631 loss_ctc 7.397379 lr 0.00069264 rank 0
I0626 06:37:35.978378 139724881797504 executor.py:115] TRAIN Batch 110/1400 loss 3.506881 loss_att 2.420658 loss_ctc 6.041402 lr 0.00069244 rank 0
I0626 06:37:53.041301 139724881797504 executor.py:115] TRAIN Batch 110/1500 loss 5.665697 loss_att 5.298046 loss_ctc 6.523549 lr 0.00069224 rank 0
I0626 06:37:58.653759 139724881797504 executor.py:152] CV Batch 110/0 loss 14.960537 loss_att 13.740149 loss_ctc 17.808107 history loss 13.298255 rank 0
I0626 06:38:08.076094 139724881797504 executor.py:152] CV Batch 110/100 loss 41.342705 loss_att 41.994919 loss_ctc 39.820866 history loss 27.629238 rank 0
I0626 06:38:15.032022 139724881797504 train.py:288] Epoch 110 CV info cv_loss 31.91740136369491
I0626 06:38:15.032243 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/110.pt
I0626 06:38:15.307179 139724881797504 train.py:274] Epoch 111 TRAIN info lr 0.0006922446149029845
I0626 06:38:15.310250 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:38:15.310311 139724881797504 executor.py:46] total epoch is 111.
I0626 06:38:23.162426 139724881797504 executor.py:115] TRAIN Batch 111/0 loss 2.706825 loss_att 1.762118 loss_ctc 4.911139 lr 0.00069221 rank 0
I0626 06:38:40.420989 139724881797504 executor.py:115] TRAIN Batch 111/100 loss 6.703288 loss_att 5.833694 loss_ctc 8.732340 lr 0.00069201 rank 0
I0626 06:38:57.689760 139724881797504 executor.py:115] TRAIN Batch 111/200 loss 5.261808 loss_att 3.704988 loss_ctc 8.894387 lr 0.00069181 rank 0
I0626 06:39:15.567485 139724881797504 executor.py:115] TRAIN Batch 111/300 loss 6.820067 loss_att 5.203358 loss_ctc 10.592388 lr 0.00069162 rank 0
I0626 06:39:33.297567 139724881797504 executor.py:115] TRAIN Batch 111/400 loss 6.769940 loss_att 4.499807 loss_ctc 12.066917 lr 0.00069138 rank 0
I0626 06:39:51.327401 139724881797504 executor.py:115] TRAIN Batch 111/500 loss 3.064634 loss_att 1.681917 loss_ctc 6.290973 lr 0.00069119 rank 0
I0626 06:40:07.850879 139724881797504 executor.py:115] TRAIN Batch 111/600 loss 7.215243 loss_att 5.696305 loss_ctc 10.759430 lr 0.00069099 rank 0
I0626 06:40:25.403276 139724881797504 executor.py:115] TRAIN Batch 111/700 loss 6.885814 loss_att 4.578506 loss_ctc 12.269531 lr 0.00069079 rank 0
I0626 06:40:42.730492 139724881797504 executor.py:115] TRAIN Batch 111/800 loss 6.024274 loss_att 4.503134 loss_ctc 9.573601 lr 0.00069056 rank 0
I0626 06:41:00.714640 139724881797504 executor.py:115] TRAIN Batch 111/900 loss 6.416598 loss_att 4.475275 loss_ctc 10.946352 lr 0.00069036 rank 0
I0626 06:41:18.662976 139724881797504 executor.py:115] TRAIN Batch 111/1000 loss 2.788695 loss_att 2.371745 loss_ctc 3.761578 lr 0.00069016 rank 0
I0626 06:41:35.512792 139724881797504 executor.py:115] TRAIN Batch 111/1100 loss 6.625661 loss_att 4.990783 loss_ctc 10.440375 lr 0.00068997 rank 0
I0626 06:41:52.886013 139724881797504 executor.py:115] TRAIN Batch 111/1200 loss 9.104989 loss_att 5.263040 loss_ctc 18.069538 lr 0.00068974 rank 0
I0626 06:42:10.156442 139724881797504 executor.py:115] TRAIN Batch 111/1300 loss 5.484366 loss_att 3.943938 loss_ctc 9.078697 lr 0.00068954 rank 0
I0626 06:42:27.595881 139724881797504 executor.py:115] TRAIN Batch 111/1400 loss 8.343681 loss_att 5.532891 loss_ctc 14.902192 lr 0.00068934 rank 0
I0626 06:42:44.925082 139724881797504 executor.py:115] TRAIN Batch 111/1500 loss 7.746114 loss_att 3.514204 loss_ctc 17.620569 lr 0.00068915 rank 0
I0626 06:42:50.563620 139724881797504 executor.py:152] CV Batch 111/0 loss 16.237020 loss_att 14.738826 loss_ctc 19.732807 history loss 14.432907 rank 0
I0626 06:42:59.770347 139724881797504 executor.py:152] CV Batch 111/100 loss 40.575855 loss_att 42.367821 loss_ctc 36.394604 history loss 27.561066 rank 0
I0626 06:43:06.688341 139724881797504 train.py:288] Epoch 111 CV info cv_loss 31.978990777089557
I0626 06:43:06.688564 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/111.pt
I0626 06:43:06.994262 139724881797504 train.py:274] Epoch 112 TRAIN info lr 0.000689147308024964
I0626 06:43:06.997341 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:43:06.997407 139724881797504 executor.py:46] total epoch is 112.
I0626 06:43:14.933381 139724881797504 executor.py:115] TRAIN Batch 112/0 loss 3.735520 loss_att 2.225480 loss_ctc 7.258949 lr 0.00068911 rank 0
I0626 06:43:32.629360 139724881797504 executor.py:115] TRAIN Batch 112/100 loss 10.253024 loss_att 6.820754 loss_ctc 18.261654 lr 0.00068892 rank 0
I0626 06:43:49.793550 139724881797504 executor.py:115] TRAIN Batch 112/200 loss 4.558303 loss_att 3.196206 loss_ctc 7.736530 lr 0.00068872 rank 0
I0626 06:44:07.601230 139724881797504 executor.py:115] TRAIN Batch 112/300 loss 5.292059 loss_att 4.369171 loss_ctc 7.445464 lr 0.00068853 rank 0
I0626 06:44:25.825688 139724881797504 executor.py:115] TRAIN Batch 112/400 loss 4.733052 loss_att 2.908559 loss_ctc 8.990204 lr 0.00068830 rank 0
I0626 06:44:44.102682 139724881797504 executor.py:115] TRAIN Batch 112/500 loss 2.253801 loss_att 1.472873 loss_ctc 4.075966 lr 0.00068810 rank 0
I0626 06:45:01.005653 139724881797504 executor.py:115] TRAIN Batch 112/600 loss 5.869179 loss_att 4.846803 loss_ctc 8.254721 lr 0.00068791 rank 0
I0626 06:45:18.881702 139724881797504 executor.py:115] TRAIN Batch 112/700 loss 7.559726 loss_att 6.021376 loss_ctc 11.149207 lr 0.00068771 rank 0
I0626 06:45:36.906582 139724881797504 executor.py:115] TRAIN Batch 112/800 loss 3.704871 loss_att 3.181927 loss_ctc 4.925072 lr 0.00068748 rank 0
I0626 06:45:54.482372 139724881797504 executor.py:115] TRAIN Batch 112/900 loss 4.871052 loss_att 3.266082 loss_ctc 8.615981 lr 0.00068729 rank 0
I0626 06:46:11.865588 139724881797504 executor.py:115] TRAIN Batch 112/1000 loss 7.086843 loss_att 3.603151 loss_ctc 15.215459 lr 0.00068709 rank 0
I0626 06:46:28.825334 139724881797504 executor.py:115] TRAIN Batch 112/1100 loss 8.834917 loss_att 5.387276 loss_ctc 16.879410 lr 0.00068690 rank 0
I0626 06:46:46.773104 139724881797504 executor.py:115] TRAIN Batch 112/1200 loss 8.164769 loss_att 5.849745 loss_ctc 13.566492 lr 0.00068667 rank 0
I0626 06:47:04.350881 139724881797504 executor.py:115] TRAIN Batch 112/1300 loss 4.426699 loss_att 2.972235 loss_ctc 7.820446 lr 0.00068648 rank 0
I0626 06:47:22.123354 139724881797504 executor.py:115] TRAIN Batch 112/1400 loss 4.050453 loss_att 2.709480 loss_ctc 7.179388 lr 0.00068629 rank 0
I0626 06:47:39.725515 139724881797504 executor.py:115] TRAIN Batch 112/1500 loss 7.053284 loss_att 5.490228 loss_ctc 10.700415 lr 0.00068609 rank 0
I0626 06:47:45.311321 139724881797504 executor.py:152] CV Batch 112/0 loss 16.318241 loss_att 14.815510 loss_ctc 19.824612 history loss 14.505103 rank 0
I0626 06:47:54.561612 139724881797504 executor.py:152] CV Batch 112/100 loss 40.217396 loss_att 40.792870 loss_ctc 38.874626 history loss 27.676012 rank 0
I0626 06:48:01.483980 139724881797504 train.py:288] Epoch 112 CV info cv_loss 31.943889205304377
I0626 06:48:01.484111 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/112.pt
I0626 06:48:01.752187 139724881797504 train.py:274] Epoch 113 TRAIN info lr 0.0006860912073433271
I0626 06:48:01.755194 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:48:01.755254 139724881797504 executor.py:46] total epoch is 113.
I0626 06:48:09.693037 139724881797504 executor.py:115] TRAIN Batch 113/0 loss 3.415158 loss_att 2.333361 loss_ctc 5.939353 lr 0.00068606 rank 0
I0626 06:48:26.996412 139724881797504 executor.py:115] TRAIN Batch 113/100 loss 10.880651 loss_att 7.101697 loss_ctc 19.698212 lr 0.00068587 rank 0
I0626 06:48:44.499891 139724881797504 executor.py:115] TRAIN Batch 113/200 loss 4.679739 loss_att 3.097981 loss_ctc 8.370508 lr 0.00068567 rank 0
I0626 06:49:02.242282 139724881797504 executor.py:115] TRAIN Batch 113/300 loss 8.529987 loss_att 5.278292 loss_ctc 16.117277 lr 0.00068548 rank 0
I0626 06:49:19.982207 139724881797504 executor.py:115] TRAIN Batch 113/400 loss 4.188529 loss_att 3.360927 loss_ctc 6.119599 lr 0.00068525 rank 0
I0626 06:49:38.067542 139724881797504 executor.py:115] TRAIN Batch 113/500 loss 3.268434 loss_att 2.104509 loss_ctc 5.984257 lr 0.00068506 rank 0
I0626 06:49:54.550983 139724881797504 executor.py:115] TRAIN Batch 113/600 loss 7.486454 loss_att 5.274588 loss_ctc 12.647473 lr 0.00068487 rank 0
I0626 06:50:11.926802 139724881797504 executor.py:115] TRAIN Batch 113/700 loss 6.240027 loss_att 4.439922 loss_ctc 10.440272 lr 0.00068467 rank 0
I0626 06:50:29.737779 139724881797504 executor.py:115] TRAIN Batch 113/800 loss 5.840121 loss_att 4.281468 loss_ctc 9.476978 lr 0.00068445 rank 0
I0626 06:50:47.898626 139724881797504 executor.py:115] TRAIN Batch 113/900 loss 6.898589 loss_att 3.998299 loss_ctc 13.665932 lr 0.00068426 rank 0
I0626 06:51:05.463048 139724881797504 executor.py:115] TRAIN Batch 113/1000 loss 4.169818 loss_att 2.715192 loss_ctc 7.563947 lr 0.00068407 rank 0
I0626 06:51:22.866006 139724881797504 executor.py:115] TRAIN Batch 113/1100 loss 7.070234 loss_att 5.435101 loss_ctc 10.885544 lr 0.00068387 rank 0
I0626 06:51:40.543001 139724881797504 executor.py:115] TRAIN Batch 113/1200 loss 4.166606 loss_att 3.544480 loss_ctc 5.618232 lr 0.00068365 rank 0
I0626 06:51:57.718432 139724881797504 executor.py:115] TRAIN Batch 113/1300 loss 6.019655 loss_att 3.525441 loss_ctc 11.839488 lr 0.00068346 rank 0
I0626 06:52:15.058657 139724881797504 executor.py:115] TRAIN Batch 113/1400 loss 5.777205 loss_att 3.200637 loss_ctc 11.789199 lr 0.00068327 rank 0
I0626 06:52:32.366462 139724881797504 executor.py:115] TRAIN Batch 113/1500 loss 10.568442 loss_att 7.345799 loss_ctc 18.087942 lr 0.00068308 rank 0
I0626 06:52:37.990692 139724881797504 executor.py:152] CV Batch 113/0 loss 16.569668 loss_att 14.147372 loss_ctc 22.221691 history loss 14.728594 rank 0
I0626 06:52:47.383774 139724881797504 executor.py:152] CV Batch 113/100 loss 39.196896 loss_att 39.923607 loss_ctc 37.501236 history loss 27.637661 rank 0
I0626 06:52:54.342612 139724881797504 train.py:288] Epoch 113 CV info cv_loss 31.90004856405105
I0626 06:52:54.342783 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/113.pt
I0626 06:52:54.608170 139724881797504 train.py:274] Epoch 114 TRAIN info lr 0.0006830754072170623
I0626 06:52:54.611190 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:52:54.611248 139724881797504 executor.py:46] total epoch is 114.
I0626 06:53:02.510584 139724881797504 executor.py:115] TRAIN Batch 114/0 loss 4.407382 loss_att 3.049457 loss_ctc 7.575874 lr 0.00068304 rank 0
I0626 06:53:19.853580 139724881797504 executor.py:115] TRAIN Batch 114/100 loss 5.706457 loss_att 4.828346 loss_ctc 7.755383 lr 0.00068285 rank 0
I0626 06:53:36.954803 139724881797504 executor.py:115] TRAIN Batch 114/200 loss 7.079075 loss_att 4.304302 loss_ctc 13.553546 lr 0.00068266 rank 0
I0626 06:53:54.341076 139724881797504 executor.py:115] TRAIN Batch 114/300 loss 5.014489 loss_att 3.883087 loss_ctc 7.654425 lr 0.00068247 rank 0
I0626 06:54:12.024057 139724881797504 executor.py:115] TRAIN Batch 114/400 loss 6.356112 loss_att 4.592556 loss_ctc 10.471077 lr 0.00068225 rank 0
I0626 06:54:30.154794 139724881797504 executor.py:115] TRAIN Batch 114/500 loss 9.970114 loss_att 6.932520 loss_ctc 17.057835 lr 0.00068206 rank 0
I0626 06:54:46.665783 139724881797504 executor.py:115] TRAIN Batch 114/600 loss 8.461942 loss_att 5.378772 loss_ctc 15.656005 lr 0.00068187 rank 0
I0626 06:55:04.470787 139724881797504 executor.py:115] TRAIN Batch 114/700 loss 6.573564 loss_att 4.408623 loss_ctc 11.625092 lr 0.00068168 rank 0
I0626 06:55:22.835474 139724881797504 executor.py:115] TRAIN Batch 114/800 loss 5.738127 loss_att 4.148602 loss_ctc 9.447018 lr 0.00068146 rank 0
I0626 06:55:41.100706 139724881797504 executor.py:115] TRAIN Batch 114/900 loss 5.630892 loss_att 3.938272 loss_ctc 9.580339 lr 0.00068127 rank 0
I0626 06:55:58.644193 139724881797504 executor.py:115] TRAIN Batch 114/1000 loss 5.095912 loss_att 2.844037 loss_ctc 10.350287 lr 0.00068108 rank 0
I0626 06:56:15.917993 139724881797504 executor.py:115] TRAIN Batch 114/1100 loss 10.112906 loss_att 6.240714 loss_ctc 19.148022 lr 0.00068089 rank 0
I0626 06:56:33.348010 139724881797504 executor.py:115] TRAIN Batch 114/1200 loss 5.796911 loss_att 4.867059 loss_ctc 7.966566 lr 0.00068067 rank 0
I0626 06:56:50.575408 139724881797504 executor.py:115] TRAIN Batch 114/1300 loss 6.674160 loss_att 3.474501 loss_ctc 14.140031 lr 0.00068048 rank 0
I0626 06:57:08.214419 139724881797504 executor.py:115] TRAIN Batch 114/1400 loss 6.549845 loss_att 3.980253 loss_ctc 12.545558 lr 0.00068029 rank 0
I0626 06:57:25.438736 139724881797504 executor.py:115] TRAIN Batch 114/1500 loss 4.236820 loss_att 3.969599 loss_ctc 4.860334 lr 0.00068010 rank 0
I0626 06:57:31.043143 139724881797504 executor.py:152] CV Batch 114/0 loss 18.192177 loss_att 15.779798 loss_ctc 23.821064 history loss 16.170824 rank 0
I0626 06:57:40.234180 139724881797504 executor.py:152] CV Batch 114/100 loss 41.356636 loss_att 41.283730 loss_ctc 41.526749 history loss 28.219626 rank 0
I0626 06:57:47.133882 139724881797504 train.py:288] Epoch 114 CV info cv_loss 32.42628255925308
I0626 06:57:47.134072 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/114.pt
I0626 06:57:47.405220 139724881797504 train.py:274] Epoch 115 TRAIN info lr 0.0006800990296285958
I0626 06:57:47.408065 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 06:57:47.408135 139724881797504 executor.py:46] total epoch is 115.
I0626 06:57:55.347493 139724881797504 executor.py:115] TRAIN Batch 115/0 loss 3.913968 loss_att 2.828098 loss_ctc 6.447665 lr 0.00068007 rank 0
I0626 06:58:12.115381 139724881797504 executor.py:115] TRAIN Batch 115/100 loss 9.041744 loss_att 6.740498 loss_ctc 14.411320 lr 0.00067988 rank 0
I0626 06:58:28.915795 139724881797504 executor.py:115] TRAIN Batch 115/200 loss 4.999551 loss_att 3.747460 loss_ctc 7.921094 lr 0.00067969 rank 0
I0626 06:58:46.054652 139724881797504 executor.py:115] TRAIN Batch 115/300 loss 4.120020 loss_att 2.973758 loss_ctc 6.794632 lr 0.00067950 rank 0
I0626 06:59:03.571226 139724881797504 executor.py:115] TRAIN Batch 115/400 loss 3.751482 loss_att 2.817386 loss_ctc 5.931042 lr 0.00067928 rank 0
I0626 06:59:21.742035 139724881797504 executor.py:115] TRAIN Batch 115/500 loss 3.070873 loss_att 1.585593 loss_ctc 6.536526 lr 0.00067909 rank 0
I0626 06:59:38.456521 139724881797504 executor.py:115] TRAIN Batch 115/600 loss 7.774567 loss_att 5.175134 loss_ctc 13.839911 lr 0.00067891 rank 0
I0626 06:59:55.734743 139724881797504 executor.py:115] TRAIN Batch 115/700 loss 8.071825 loss_att 5.901700 loss_ctc 13.135450 lr 0.00067872 rank 0
I0626 07:00:13.550656 139724881797504 executor.py:115] TRAIN Batch 115/800 loss 6.364409 loss_att 4.240990 loss_ctc 11.319054 lr 0.00067850 rank 0
I0626 07:00:31.398789 139724881797504 executor.py:115] TRAIN Batch 115/900 loss 5.070201 loss_att 3.321389 loss_ctc 9.150761 lr 0.00067831 rank 0
I0626 07:00:49.058342 139724881797504 executor.py:115] TRAIN Batch 115/1000 loss 3.094392 loss_att 2.304451 loss_ctc 4.937585 lr 0.00067813 rank 0
I0626 07:01:06.090991 139724881797504 executor.py:115] TRAIN Batch 115/1100 loss 9.404919 loss_att 5.172101 loss_ctc 19.281492 lr 0.00067794 rank 0
I0626 07:01:23.954452 139724881797504 executor.py:115] TRAIN Batch 115/1200 loss 5.399247 loss_att 3.886270 loss_ctc 8.929527 lr 0.00067772 rank 0
I0626 07:01:41.547378 139724881797504 executor.py:115] TRAIN Batch 115/1300 loss 3.794461 loss_att 2.985711 loss_ctc 5.681542 lr 0.00067753 rank 0
I0626 07:01:58.976139 139724881797504 executor.py:115] TRAIN Batch 115/1400 loss 5.727882 loss_att 3.542026 loss_ctc 10.828214 lr 0.00067735 rank 0
I0626 07:02:16.367600 139724881797504 executor.py:115] TRAIN Batch 115/1500 loss 6.371527 loss_att 4.813554 loss_ctc 10.006796 lr 0.00067716 rank 0
I0626 07:02:22.015059 139724881797504 executor.py:152] CV Batch 115/0 loss 17.041483 loss_att 14.709707 loss_ctc 22.482290 history loss 15.147985 rank 0
I0626 07:02:31.237184 139724881797504 executor.py:152] CV Batch 115/100 loss 40.302055 loss_att 40.725868 loss_ctc 39.313156 history loss 27.444096 rank 0
I0626 07:02:38.164409 139724881797504 train.py:288] Epoch 115 CV info cv_loss 31.929855059179825
I0626 07:02:38.164555 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/115.pt
I0626 07:02:38.433648 139724881797504 train.py:274] Epoch 116 TRAIN info lr 0.0006771612231098582
I0626 07:02:38.436748 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:02:38.436819 139724881797504 executor.py:46] total epoch is 116.
I0626 07:02:46.303641 139724881797504 executor.py:115] TRAIN Batch 116/0 loss 3.410295 loss_att 2.338490 loss_ctc 5.911172 lr 0.00067713 rank 0
I0626 07:03:03.385618 139724881797504 executor.py:115] TRAIN Batch 116/100 loss 8.886110 loss_att 6.648309 loss_ctc 14.107644 lr 0.00067694 rank 0
I0626 07:03:20.717911 139724881797504 executor.py:115] TRAIN Batch 116/200 loss 5.015547 loss_att 3.656890 loss_ctc 8.185748 lr 0.00067676 rank 0
I0626 07:03:38.406747 139724881797504 executor.py:115] TRAIN Batch 116/300 loss 6.070782 loss_att 4.115685 loss_ctc 10.632672 lr 0.00067657 rank 0
I0626 07:03:56.280730 139724881797504 executor.py:115] TRAIN Batch 116/400 loss 4.280869 loss_att 2.956100 loss_ctc 7.371996 lr 0.00067636 rank 0
I0626 07:04:14.247539 139724881797504 executor.py:115] TRAIN Batch 116/500 loss 4.582886 loss_att 4.272988 loss_ctc 5.305980 lr 0.00067617 rank 0
I0626 07:04:31.135640 139724881797504 executor.py:115] TRAIN Batch 116/600 loss 3.990169 loss_att 3.381449 loss_ctc 5.410515 lr 0.00067598 rank 0
I0626 07:04:48.619408 139724881797504 executor.py:115] TRAIN Batch 116/700 loss 2.963334 loss_att 3.153572 loss_ctc 2.519447 lr 0.00067580 rank 0
I0626 07:05:06.471779 139724881797504 executor.py:115] TRAIN Batch 116/800 loss 3.458178 loss_att 2.995036 loss_ctc 4.538843 lr 0.00067558 rank 0
I0626 07:05:24.070945 139724881797504 executor.py:115] TRAIN Batch 116/900 loss 8.056733 loss_att 4.904102 loss_ctc 15.412871 lr 0.00067540 rank 0
I0626 07:05:41.486378 139724881797504 executor.py:115] TRAIN Batch 116/1000 loss 4.209388 loss_att 2.512632 loss_ctc 8.168488 lr 0.00067521 rank 0
I0626 07:05:58.849900 139724881797504 executor.py:115] TRAIN Batch 116/1100 loss 7.397896 loss_att 5.784710 loss_ctc 11.161997 lr 0.00067503 rank 0
I0626 07:06:16.358162 139724881797504 executor.py:115] TRAIN Batch 116/1200 loss 7.070564 loss_att 4.395633 loss_ctc 13.312071 lr 0.00067481 rank 0
I0626 07:06:33.731474 139724881797504 executor.py:115] TRAIN Batch 116/1300 loss 4.546876 loss_att 3.326768 loss_ctc 7.393795 lr 0.00067463 rank 0
I0626 07:06:51.166262 139724881797504 executor.py:115] TRAIN Batch 116/1400 loss 5.488797 loss_att 4.149777 loss_ctc 8.613177 lr 0.00067445 rank 0
I0626 07:07:08.371994 139724881797504 executor.py:115] TRAIN Batch 116/1500 loss 4.625896 loss_att 3.222429 loss_ctc 7.900651 lr 0.00067426 rank 0
I0626 07:07:14.030946 139724881797504 executor.py:152] CV Batch 116/0 loss 17.074450 loss_att 15.046023 loss_ctc 21.807442 history loss 15.177288 rank 0
I0626 07:07:23.266927 139724881797504 executor.py:152] CV Batch 116/100 loss 40.198906 loss_att 41.446354 loss_ctc 37.288197 history loss 27.179188 rank 0
I0626 07:07:30.207401 139724881797504 train.py:288] Epoch 116 CV info cv_loss 31.633154954930244
I0626 07:07:30.207636 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/116.pt
I0626 07:07:30.474514 139724881797504 train.py:274] Epoch 117 TRAIN info lr 0.0006742611617189457
I0626 07:07:30.477532 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:07:30.477593 139724881797504 executor.py:46] total epoch is 117.
I0626 07:07:38.431071 139724881797504 executor.py:115] TRAIN Batch 117/0 loss 5.074188 loss_att 3.437725 loss_ctc 8.892601 lr 0.00067423 rank 0
I0626 07:07:55.448502 139724881797504 executor.py:115] TRAIN Batch 117/100 loss 6.239366 loss_att 5.095883 loss_ctc 8.907491 lr 0.00067405 rank 0
I0626 07:08:12.443855 139724881797504 executor.py:115] TRAIN Batch 117/200 loss 7.935090 loss_att 5.247284 loss_ctc 14.206637 lr 0.00067386 rank 0
I0626 07:08:29.739363 139724881797504 executor.py:115] TRAIN Batch 117/300 loss 4.380631 loss_att 2.706901 loss_ctc 8.286002 lr 0.00067368 rank 0
I0626 07:08:47.374033 139724881797504 executor.py:115] TRAIN Batch 117/400 loss 6.437290 loss_att 3.751454 loss_ctc 12.704241 lr 0.00067347 rank 0
I0626 07:09:05.216581 139724881797504 executor.py:115] TRAIN Batch 117/500 loss 4.255305 loss_att 2.970470 loss_ctc 7.253254 lr 0.00067328 rank 0
I0626 07:09:21.962635 139724881797504 executor.py:115] TRAIN Batch 117/600 loss 6.262578 loss_att 5.414389 loss_ctc 8.241688 lr 0.00067310 rank 0
I0626 07:09:39.703903 139724881797504 executor.py:115] TRAIN Batch 117/700 loss 5.383519 loss_att 4.116609 loss_ctc 8.339642 lr 0.00067292 rank 0
I0626 07:09:57.376572 139724881797504 executor.py:115] TRAIN Batch 117/800 loss 7.665596 loss_att 4.543939 loss_ctc 14.949462 lr 0.00067270 rank 0
I0626 07:10:15.266044 139724881797504 executor.py:115] TRAIN Batch 117/900 loss 5.479627 loss_att 4.233690 loss_ctc 8.386812 lr 0.00067252 rank 0
I0626 07:10:32.406950 139724881797504 executor.py:115] TRAIN Batch 117/1000 loss 4.031348 loss_att 3.113459 loss_ctc 6.173088 lr 0.00067234 rank 0
I0626 07:10:49.162435 139724881797504 executor.py:115] TRAIN Batch 117/1100 loss 7.809327 loss_att 5.340545 loss_ctc 13.569817 lr 0.00067216 rank 0
I0626 07:11:06.651298 139724881797504 executor.py:115] TRAIN Batch 117/1200 loss 5.562285 loss_att 4.290317 loss_ctc 8.530213 lr 0.00067194 rank 0
I0626 07:11:23.972191 139724881797504 executor.py:115] TRAIN Batch 117/1300 loss 8.686239 loss_att 5.556109 loss_ctc 15.989875 lr 0.00067176 rank 0
I0626 07:11:41.557161 139724881797504 executor.py:115] TRAIN Batch 117/1400 loss 5.650226 loss_att 2.561369 loss_ctc 12.857557 lr 0.00067158 rank 0
I0626 07:11:58.957490 139724881797504 executor.py:115] TRAIN Batch 117/1500 loss 7.230279 loss_att 5.577636 loss_ctc 11.086447 lr 0.00067140 rank 0
I0626 07:12:04.639973 139724881797504 executor.py:152] CV Batch 117/0 loss 17.091400 loss_att 14.913635 loss_ctc 22.172848 history loss 15.192356 rank 0
I0626 07:12:13.900262 139724881797504 executor.py:152] CV Batch 117/100 loss 39.735825 loss_att 40.155251 loss_ctc 38.757164 history loss 27.301158 rank 0
I0626 07:12:20.827001 139724881797504 train.py:288] Epoch 117 CV info cv_loss 31.804349511965786
I0626 07:12:20.827223 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/117.pt
I0626 07:12:21.097059 139724881797504 train.py:274] Epoch 118 TRAIN info lr 0.0006713980440645823
I0626 07:12:21.100170 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:12:21.100242 139724881797504 executor.py:46] total epoch is 118.
I0626 07:12:29.049099 139724881797504 executor.py:115] TRAIN Batch 118/0 loss 4.374858 loss_att 2.771537 loss_ctc 8.115938 lr 0.00067137 rank 0
I0626 07:12:45.984712 139724881797504 executor.py:115] TRAIN Batch 118/100 loss 6.945531 loss_att 4.870443 loss_ctc 11.787401 lr 0.00067119 rank 0
I0626 07:13:03.106498 139724881797504 executor.py:115] TRAIN Batch 118/200 loss 5.349947 loss_att 3.452696 loss_ctc 9.776869 lr 0.00067100 rank 0
I0626 07:13:20.546604 139724881797504 executor.py:115] TRAIN Batch 118/300 loss 4.422677 loss_att 3.190352 loss_ctc 7.298100 lr 0.00067082 rank 0
I0626 07:13:37.857043 139724881797504 executor.py:115] TRAIN Batch 118/400 loss 3.396652 loss_att 2.534544 loss_ctc 5.408237 lr 0.00067061 rank 0
I0626 07:13:55.610131 139724881797504 executor.py:115] TRAIN Batch 118/500 loss 2.616729 loss_att 1.845237 loss_ctc 4.416877 lr 0.00067043 rank 0
I0626 07:14:12.014822 139724881797504 executor.py:115] TRAIN Batch 118/600 loss 5.421371 loss_att 4.248119 loss_ctc 8.158958 lr 0.00067025 rank 0
I0626 07:14:29.469995 139724881797504 executor.py:115] TRAIN Batch 118/700 loss 6.987164 loss_att 5.633478 loss_ctc 10.145766 lr 0.00067007 rank 0
I0626 07:14:47.394611 139724881797504 executor.py:115] TRAIN Batch 118/800 loss 4.398333 loss_att 3.107456 loss_ctc 7.410378 lr 0.00066986 rank 0
I0626 07:15:05.215046 139724881797504 executor.py:115] TRAIN Batch 118/900 loss 5.931437 loss_att 3.287534 loss_ctc 12.100540 lr 0.00066968 rank 0
I0626 07:15:22.811320 139724881797504 executor.py:115] TRAIN Batch 118/1000 loss 3.432287 loss_att 3.069930 loss_ctc 4.277788 lr 0.00066950 rank 0
I0626 07:15:39.849938 139724881797504 executor.py:115] TRAIN Batch 118/1100 loss 6.508468 loss_att 4.742932 loss_ctc 10.628050 lr 0.00066932 rank 0
I0626 07:15:57.629539 139724881797504 executor.py:115] TRAIN Batch 118/1200 loss 5.468995 loss_att 3.855725 loss_ctc 9.233291 lr 0.00066911 rank 0
I0626 07:16:15.112255 139724881797504 executor.py:115] TRAIN Batch 118/1300 loss 8.747007 loss_att 5.577299 loss_ctc 16.142992 lr 0.00066893 rank 0
I0626 07:16:32.885575 139724881797504 executor.py:115] TRAIN Batch 118/1400 loss 5.641388 loss_att 3.857941 loss_ctc 9.802765 lr 0.00066875 rank 0
I0626 07:16:50.132510 139724881797504 executor.py:115] TRAIN Batch 118/1500 loss 7.916439 loss_att 5.109049 loss_ctc 14.467016 lr 0.00066857 rank 0
I0626 07:16:55.766175 139724881797504 executor.py:152] CV Batch 118/0 loss 16.191517 loss_att 14.553567 loss_ctc 20.013397 history loss 14.392459 rank 0
I0626 07:17:05.144583 139724881797504 executor.py:152] CV Batch 118/100 loss 39.431721 loss_att 41.105713 loss_ctc 35.525738 history loss 27.345840 rank 0
I0626 07:17:12.064645 139724881797504 train.py:288] Epoch 118 CV info cv_loss 31.864216928954566
I0626 07:17:12.064877 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/118.pt
I0626 07:17:12.361405 139724881797504 train.py:274] Epoch 119 TRAIN info lr 0.0006685710923757638
I0626 07:17:12.364414 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:17:12.364483 139724881797504 executor.py:46] total epoch is 119.
I0626 07:17:20.182274 139724881797504 executor.py:115] TRAIN Batch 119/0 loss 5.433757 loss_att 3.385634 loss_ctc 10.212709 lr 0.00066854 rank 0
I0626 07:17:37.048242 139724881797504 executor.py:115] TRAIN Batch 119/100 loss 7.289548 loss_att 5.420712 loss_ctc 11.650164 lr 0.00066836 rank 0
I0626 07:17:54.221810 139724881797504 executor.py:115] TRAIN Batch 119/200 loss 4.905153 loss_att 3.824005 loss_ctc 7.427832 lr 0.00066818 rank 0
I0626 07:18:11.878619 139724881797504 executor.py:115] TRAIN Batch 119/300 loss 3.380902 loss_att 2.754105 loss_ctc 4.843427 lr 0.00066800 rank 0
I0626 07:18:29.574313 139724881797504 executor.py:115] TRAIN Batch 119/400 loss 5.480257 loss_att 4.033161 loss_ctc 8.856813 lr 0.00066780 rank 0
I0626 07:18:47.157182 139724881797504 executor.py:115] TRAIN Batch 119/500 loss 2.387405 loss_att 1.842897 loss_ctc 3.657924 lr 0.00066762 rank 0
I0626 07:19:03.580697 139724881797504 executor.py:115] TRAIN Batch 119/600 loss 7.230335 loss_att 4.691824 loss_ctc 13.153526 lr 0.00066744 rank 0
I0626 07:19:21.031427 139724881797504 executor.py:115] TRAIN Batch 119/700 loss 4.271830 loss_att 3.940543 loss_ctc 5.044832 lr 0.00066726 rank 0
I0626 07:19:38.543018 139724881797504 executor.py:115] TRAIN Batch 119/800 loss 6.914814 loss_att 3.778266 loss_ctc 14.233426 lr 0.00066705 rank 0
I0626 07:19:56.312972 139724881797504 executor.py:115] TRAIN Batch 119/900 loss 4.512995 loss_att 3.384849 loss_ctc 7.145334 lr 0.00066687 rank 0
I0626 07:20:14.139894 139724881797504 executor.py:115] TRAIN Batch 119/1000 loss 4.050750 loss_att 2.492128 loss_ctc 7.687534 lr 0.00066670 rank 0
I0626 07:20:31.060249 139724881797504 executor.py:115] TRAIN Batch 119/1100 loss 8.106384 loss_att 5.351323 loss_ctc 14.534859 lr 0.00066652 rank 0
I0626 07:20:48.541109 139724881797504 executor.py:115] TRAIN Batch 119/1200 loss 5.311632 loss_att 3.706207 loss_ctc 9.057624 lr 0.00066631 rank 0
I0626 07:21:05.798590 139724881797504 executor.py:115] TRAIN Batch 119/1300 loss 6.414804 loss_att 4.838665 loss_ctc 10.092461 lr 0.00066613 rank 0
I0626 07:21:23.213981 139724881797504 executor.py:115] TRAIN Batch 119/1400 loss 3.920397 loss_att 2.731732 loss_ctc 6.693946 lr 0.00066596 rank 0
I0626 07:21:40.579816 139724881797504 executor.py:115] TRAIN Batch 119/1500 loss 6.371693 loss_att 4.633286 loss_ctc 10.427974 lr 0.00066578 rank 0
I0626 07:21:46.187481 139724881797504 executor.py:152] CV Batch 119/0 loss 16.630608 loss_att 14.920191 loss_ctc 20.621582 history loss 14.782762 rank 0
I0626 07:21:55.406036 139724881797504 executor.py:152] CV Batch 119/100 loss 40.597961 loss_att 41.297176 loss_ctc 38.966450 history loss 27.945241 rank 0
I0626 07:22:02.320565 139724881797504 train.py:288] Epoch 119 CV info cv_loss 32.33468104041047
I0626 07:22:02.320726 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/119.pt
I0626 07:22:02.587182 139724881797504 train.py:274] Epoch 120 TRAIN info lr 0.0006657795516141343
I0626 07:22:02.590024 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:22:02.590081 139724881797504 executor.py:46] total epoch is 120.
I0626 07:22:10.578333 139724881797504 executor.py:115] TRAIN Batch 120/0 loss 5.684005 loss_att 2.968091 loss_ctc 12.021137 lr 0.00066575 rank 0
I0626 07:22:28.058409 139724881797504 executor.py:115] TRAIN Batch 120/100 loss 6.466921 loss_att 5.545249 loss_ctc 8.617487 lr 0.00066557 rank 0
I0626 07:22:45.705594 139724881797504 executor.py:115] TRAIN Batch 120/200 loss 4.931338 loss_att 4.305332 loss_ctc 6.392020 lr 0.00066540 rank 0
I0626 07:23:03.591855 139724881797504 executor.py:115] TRAIN Batch 120/300 loss 9.646881 loss_att 5.786709 loss_ctc 18.653948 lr 0.00066522 rank 0
I0626 07:23:21.671957 139724881797504 executor.py:115] TRAIN Batch 120/400 loss 3.841093 loss_att 3.116883 loss_ctc 5.530916 lr 0.00066501 rank 0
I0626 07:23:39.671942 139724881797504 executor.py:115] TRAIN Batch 120/500 loss 3.815722 loss_att 3.081002 loss_ctc 5.530070 lr 0.00066484 rank 0
I0626 07:23:56.074169 139724881797504 executor.py:115] TRAIN Batch 120/600 loss 7.043629 loss_att 4.573719 loss_ctc 12.806753 lr 0.00066466 rank 0
I0626 07:24:13.717558 139724881797504 executor.py:115] TRAIN Batch 120/700 loss 5.385421 loss_att 3.659008 loss_ctc 9.413717 lr 0.00066448 rank 0
I0626 07:24:31.526793 139724881797504 executor.py:115] TRAIN Batch 120/800 loss 3.461890 loss_att 2.739637 loss_ctc 5.147146 lr 0.00066428 rank 0
I0626 07:24:49.082799 139724881797504 executor.py:115] TRAIN Batch 120/900 loss 4.062146 loss_att 2.573583 loss_ctc 7.535460 lr 0.00066410 rank 0
I0626 07:25:07.127701 139724881797504 executor.py:115] TRAIN Batch 120/1000 loss 3.815562 loss_att 2.451293 loss_ctc 6.998856 lr 0.00066393 rank 0
I0626 07:25:24.410791 139724881797504 executor.py:115] TRAIN Batch 120/1100 loss 8.986491 loss_att 5.789912 loss_ctc 16.445175 lr 0.00066375 rank 0
I0626 07:25:42.258582 139724881797504 executor.py:115] TRAIN Batch 120/1200 loss 4.002166 loss_att 3.649409 loss_ctc 4.825267 lr 0.00066355 rank 0
I0626 07:25:59.954294 139724881797504 executor.py:115] TRAIN Batch 120/1300 loss 7.631684 loss_att 4.516000 loss_ctc 14.901612 lr 0.00066337 rank 0
I0626 07:26:17.308060 139724881797504 executor.py:115] TRAIN Batch 120/1400 loss 3.837055 loss_att 2.979893 loss_ctc 5.837101 lr 0.00066320 rank 0
I0626 07:26:34.842967 139724881797504 executor.py:115] TRAIN Batch 120/1500 loss 4.597768 loss_att 3.089602 loss_ctc 8.116819 lr 0.00066302 rank 0
I0626 07:26:40.491164 139724881797504 executor.py:152] CV Batch 120/0 loss 17.050884 loss_att 14.565542 loss_ctc 22.850014 history loss 15.156342 rank 0
I0626 07:26:49.740309 139724881797504 executor.py:152] CV Batch 120/100 loss 40.385418 loss_att 41.143528 loss_ctc 38.616497 history loss 27.504800 rank 0
I0626 07:26:56.672621 139724881797504 train.py:288] Epoch 120 CV info cv_loss 31.934456052600463
I0626 07:26:56.672839 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/120.pt
I0626 07:26:56.939640 139724881797504 train.py:274] Epoch 121 TRAIN info lr 0.0006630226886267935
I0626 07:26:56.942605 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:26:56.942666 139724881797504 executor.py:46] total epoch is 121.
I0626 07:27:04.906842 139724881797504 executor.py:115] TRAIN Batch 121/0 loss 2.615790 loss_att 1.733797 loss_ctc 4.673775 lr 0.00066299 rank 0
I0626 07:27:22.755437 139724881797504 executor.py:115] TRAIN Batch 121/100 loss 6.237041 loss_att 5.123375 loss_ctc 8.835596 lr 0.00066282 rank 0
I0626 07:27:39.942148 139724881797504 executor.py:115] TRAIN Batch 121/200 loss 4.181356 loss_att 3.521612 loss_ctc 5.720760 lr 0.00066264 rank 0
I0626 07:27:57.538807 139724881797504 executor.py:115] TRAIN Batch 121/300 loss 7.328489 loss_att 4.540062 loss_ctc 13.834817 lr 0.00066247 rank 0
I0626 07:28:15.423756 139724881797504 executor.py:115] TRAIN Batch 121/400 loss 6.163407 loss_att 4.791198 loss_ctc 9.365229 lr 0.00066227 rank 0
I0626 07:28:33.336024 139724881797504 executor.py:115] TRAIN Batch 121/500 loss 6.407659 loss_att 3.137100 loss_ctc 14.038963 lr 0.00066209 rank 0
I0626 07:28:49.882110 139724881797504 executor.py:115] TRAIN Batch 121/600 loss 5.183049 loss_att 4.204265 loss_ctc 7.466878 lr 0.00066192 rank 0
I0626 07:29:07.500476 139724881797504 executor.py:115] TRAIN Batch 121/700 loss 5.228743 loss_att 4.888613 loss_ctc 6.022379 lr 0.00066174 rank 0
I0626 07:29:25.087991 139724881797504 executor.py:115] TRAIN Batch 121/800 loss 3.369341 loss_att 2.503167 loss_ctc 5.390412 lr 0.00066154 rank 0
I0626 07:29:42.821698 139724881797504 executor.py:115] TRAIN Batch 121/900 loss 4.845040 loss_att 2.934643 loss_ctc 9.302632 lr 0.00066137 rank 0
I0626 07:30:00.271999 139724881797504 executor.py:115] TRAIN Batch 121/1000 loss 3.662426 loss_att 1.794505 loss_ctc 8.020907 lr 0.00066119 rank 0
I0626 07:30:17.151425 139724881797504 executor.py:115] TRAIN Batch 121/1100 loss 6.085313 loss_att 4.236574 loss_ctc 10.399036 lr 0.00066102 rank 0
I0626 07:30:34.562405 139724881797504 executor.py:115] TRAIN Batch 121/1200 loss 4.583642 loss_att 3.242669 loss_ctc 7.712578 lr 0.00066082 rank 0
I0626 07:30:51.685020 139724881797504 executor.py:115] TRAIN Batch 121/1300 loss 7.317432 loss_att 4.405976 loss_ctc 14.110828 lr 0.00066065 rank 0
I0626 07:31:09.238423 139724881797504 executor.py:115] TRAIN Batch 121/1400 loss 3.973446 loss_att 2.769939 loss_ctc 6.781628 lr 0.00066047 rank 0
I0626 07:31:26.560638 139724881797504 executor.py:115] TRAIN Batch 121/1500 loss 4.750558 loss_att 3.652956 loss_ctc 7.311629 lr 0.00066030 rank 0
I0626 07:31:32.178384 139724881797504 executor.py:152] CV Batch 121/0 loss 17.001217 loss_att 15.053521 loss_ctc 21.545837 history loss 15.112193 rank 0
I0626 07:31:41.463675 139724881797504 executor.py:152] CV Batch 121/100 loss 39.536018 loss_att 40.842228 loss_ctc 36.488190 history loss 27.334963 rank 0
I0626 07:31:48.393446 139724881797504 train.py:288] Epoch 121 CV info cv_loss 31.70624098741168
I0626 07:31:48.393608 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/121.pt
I0626 07:31:48.661572 139724881797504 train.py:274] Epoch 122 TRAIN info lr 0.0006602997913373753
I0626 07:31:48.664747 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:31:48.664816 139724881797504 executor.py:46] total epoch is 122.
I0626 07:31:56.525724 139724881797504 executor.py:115] TRAIN Batch 122/0 loss 4.046873 loss_att 2.478863 loss_ctc 7.705564 lr 0.00066027 rank 0
I0626 07:32:13.426805 139724881797504 executor.py:115] TRAIN Batch 122/100 loss 8.740742 loss_att 6.009189 loss_ctc 15.114365 lr 0.00066010 rank 0
I0626 07:32:30.490337 139724881797504 executor.py:115] TRAIN Batch 122/200 loss 4.124417 loss_att 3.511612 loss_ctc 5.554296 lr 0.00065993 rank 0
I0626 07:32:48.058232 139724881797504 executor.py:115] TRAIN Batch 122/300 loss 4.314858 loss_att 2.986076 loss_ctc 7.415351 lr 0.00065975 rank 0
I0626 07:33:06.146333 139724881797504 executor.py:115] TRAIN Batch 122/400 loss 4.356726 loss_att 3.188629 loss_ctc 7.082284 lr 0.00065955 rank 0
I0626 07:33:24.055410 139724881797504 executor.py:115] TRAIN Batch 122/500 loss 3.247411 loss_att 2.081756 loss_ctc 5.967274 lr 0.00065938 rank 0
I0626 07:33:40.608575 139724881797504 executor.py:115] TRAIN Batch 122/600 loss 3.846754 loss_att 3.564804 loss_ctc 4.504637 lr 0.00065921 rank 0
I0626 07:33:58.050194 139724881797504 executor.py:115] TRAIN Batch 122/700 loss 3.874192 loss_att 2.779368 loss_ctc 6.428782 lr 0.00065904 rank 0
I0626 07:34:15.445205 139724881797504 executor.py:115] TRAIN Batch 122/800 loss 3.569963 loss_att 2.835129 loss_ctc 5.284576 lr 0.00065884 rank 0
I0626 07:34:32.943574 139724881797504 executor.py:115] TRAIN Batch 122/900 loss 5.725552 loss_att 3.334094 loss_ctc 11.305619 lr 0.00065866 rank 0
I0626 07:34:50.293212 139724881797504 executor.py:115] TRAIN Batch 122/1000 loss 3.697582 loss_att 2.284007 loss_ctc 6.995923 lr 0.00065849 rank 0
I0626 07:35:07.329527 139724881797504 executor.py:115] TRAIN Batch 122/1100 loss 6.649249 loss_att 5.563238 loss_ctc 9.183275 lr 0.00065832 rank 0
I0626 07:35:24.701770 139724881797504 executor.py:115] TRAIN Batch 122/1200 loss 3.186391 loss_att 2.602706 loss_ctc 4.548322 lr 0.00065812 rank 0
I0626 07:35:42.079156 139724881797504 executor.py:115] TRAIN Batch 122/1300 loss 3.648767 loss_att 2.847249 loss_ctc 5.518975 lr 0.00065795 rank 0
I0626 07:35:59.459605 139724881797504 executor.py:115] TRAIN Batch 122/1400 loss 3.506557 loss_att 2.684966 loss_ctc 5.423602 lr 0.00065778 rank 0
I0626 07:36:16.642295 139724881797504 executor.py:115] TRAIN Batch 122/1500 loss 4.618470 loss_att 3.253606 loss_ctc 7.803153 lr 0.00065761 rank 0
I0626 07:36:22.270435 139724881797504 executor.py:152] CV Batch 122/0 loss 15.704390 loss_att 14.409859 loss_ctc 18.724960 history loss 13.959457 rank 0
I0626 07:36:31.542596 139724881797504 executor.py:152] CV Batch 122/100 loss 40.625435 loss_att 41.695999 loss_ctc 38.127449 history loss 27.640375 rank 0
I0626 07:36:38.451205 139724881797504 train.py:288] Epoch 122 CV info cv_loss 32.21406536009124
I0626 07:36:38.451381 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/122.pt
I0626 07:36:38.716561 139724881797504 train.py:274] Epoch 123 TRAIN info lr 0.0006576101679733735
I0626 07:36:38.719619 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:36:38.719686 139724881797504 executor.py:46] total epoch is 123.
I0626 07:36:46.736092 139724881797504 executor.py:115] TRAIN Batch 123/0 loss 3.753717 loss_att 2.033938 loss_ctc 7.766533 lr 0.00065758 rank 0
I0626 07:37:04.053142 139724881797504 executor.py:115] TRAIN Batch 123/100 loss 8.377295 loss_att 5.405486 loss_ctc 15.311518 lr 0.00065741 rank 0
I0626 07:37:21.736394 139724881797504 executor.py:115] TRAIN Batch 123/200 loss 5.828995 loss_att 4.498543 loss_ctc 8.933384 lr 0.00065724 rank 0
I0626 07:37:39.569851 139724881797504 executor.py:115] TRAIN Batch 123/300 loss 3.409650 loss_att 1.904343 loss_ctc 6.922033 lr 0.00065707 rank 0
I0626 07:37:57.152787 139724881797504 executor.py:115] TRAIN Batch 123/400 loss 3.850806 loss_att 2.591856 loss_ctc 6.788358 lr 0.00065687 rank 0
I0626 07:38:15.304676 139724881797504 executor.py:115] TRAIN Batch 123/500 loss 7.688725 loss_att 4.531478 loss_ctc 15.055633 lr 0.00065670 rank 0
I0626 07:38:31.892079 139724881797504 executor.py:115] TRAIN Batch 123/600 loss 7.222207 loss_att 5.281614 loss_ctc 11.750256 lr 0.00065653 rank 0
I0626 07:38:49.625011 139724881797504 executor.py:115] TRAIN Batch 123/700 loss 5.495865 loss_att 3.339452 loss_ctc 10.527496 lr 0.00065636 rank 0
I0626 07:39:07.044768 139724881797504 executor.py:115] TRAIN Batch 123/800 loss 7.043470 loss_att 4.251483 loss_ctc 13.558104 lr 0.00065616 rank 0
I0626 07:39:24.801553 139724881797504 executor.py:115] TRAIN Batch 123/900 loss 7.740197 loss_att 4.546408 loss_ctc 15.192373 lr 0.00065600 rank 0
I0626 07:39:42.055673 139724881797504 executor.py:115] TRAIN Batch 123/1000 loss 4.256242 loss_att 3.024505 loss_ctc 7.130295 lr 0.00065583 rank 0
I0626 07:39:58.787551 139724881797504 executor.py:115] TRAIN Batch 123/1100 loss 5.460837 loss_att 4.117105 loss_ctc 8.596212 lr 0.00065566 rank 0
I0626 07:40:16.307422 139724881797504 executor.py:115] TRAIN Batch 123/1200 loss 4.867905 loss_att 3.576496 loss_ctc 7.881192 lr 0.00065546 rank 0
I0626 07:40:33.476560 139724881797504 executor.py:115] TRAIN Batch 123/1300 loss 3.412265 loss_att 2.683501 loss_ctc 5.112716 lr 0.00065529 rank 0
I0626 07:40:51.058341 139724881797504 executor.py:115] TRAIN Batch 123/1400 loss 7.559413 loss_att 4.379167 loss_ctc 14.979986 lr 0.00065512 rank 0
I0626 07:41:08.787092 139724881797504 executor.py:115] TRAIN Batch 123/1500 loss 3.451049 loss_att 3.049737 loss_ctc 4.387444 lr 0.00065495 rank 0
I0626 07:41:14.396431 139724881797504 executor.py:152] CV Batch 123/0 loss 16.546280 loss_att 13.968700 loss_ctc 22.560627 history loss 14.707804 rank 0
I0626 07:41:23.636517 139724881797504 executor.py:152] CV Batch 123/100 loss 39.553093 loss_att 39.936001 loss_ctc 38.659649 history loss 27.834760 rank 0
I0626 07:41:30.553997 139724881797504 train.py:288] Epoch 123 CV info cv_loss 32.255972436983434
I0626 07:41:30.554339 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/123.pt
I0626 07:41:30.825351 139724881797504 train.py:274] Epoch 124 TRAIN info lr 0.0006549531463278075
I0626 07:41:30.828466 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:41:30.828535 139724881797504 executor.py:46] total epoch is 124.
I0626 07:41:38.711686 139724881797504 executor.py:115] TRAIN Batch 124/0 loss 3.612746 loss_att 2.368191 loss_ctc 6.516709 lr 0.00065493 rank 0
I0626 07:41:55.847642 139724881797504 executor.py:115] TRAIN Batch 124/100 loss 5.158734 loss_att 4.319928 loss_ctc 7.115949 lr 0.00065476 rank 0
I0626 07:42:13.042688 139724881797504 executor.py:115] TRAIN Batch 124/200 loss 5.091381 loss_att 3.793471 loss_ctc 8.119836 lr 0.00065459 rank 0
I0626 07:42:30.636783 139724881797504 executor.py:115] TRAIN Batch 124/300 loss 4.518957 loss_att 2.907500 loss_ctc 8.279024 lr 0.00065442 rank 0
I0626 07:42:48.226440 139724881797504 executor.py:115] TRAIN Batch 124/400 loss 3.339285 loss_att 2.581506 loss_ctc 5.107436 lr 0.00065422 rank 0
I0626 07:43:05.940080 139724881797504 executor.py:115] TRAIN Batch 124/500 loss 3.877715 loss_att 3.049985 loss_ctc 5.809083 lr 0.00065406 rank 0
I0626 07:43:22.457252 139724881797504 executor.py:115] TRAIN Batch 124/600 loss 5.167857 loss_att 3.763186 loss_ctc 8.445424 lr 0.00065389 rank 0
I0626 07:43:39.633747 139724881797504 executor.py:115] TRAIN Batch 124/700 loss 6.545594 loss_att 4.325538 loss_ctc 11.725724 lr 0.00065372 rank 0
I0626 07:43:57.085518 139724881797504 executor.py:115] TRAIN Batch 124/800 loss 6.222209 loss_att 4.349113 loss_ctc 10.592766 lr 0.00065352 rank 0
I0626 07:44:14.502977 139724881797504 executor.py:115] TRAIN Batch 124/900 loss 2.880851 loss_att 2.472107 loss_ctc 3.834586 lr 0.00065336 rank 0
I0626 07:44:31.969033 139724881797504 executor.py:115] TRAIN Batch 124/1000 loss 2.880462 loss_att 1.969140 loss_ctc 5.006879 lr 0.00065319 rank 0
I0626 07:44:48.693697 139724881797504 executor.py:115] TRAIN Batch 124/1100 loss 8.241117 loss_att 6.483700 loss_ctc 12.341759 lr 0.00065302 rank 0
I0626 07:45:06.069585 139724881797504 executor.py:115] TRAIN Batch 124/1200 loss 4.317035 loss_att 3.042475 loss_ctc 7.291009 lr 0.00065283 rank 0
I0626 07:45:23.476525 139724881797504 executor.py:115] TRAIN Batch 124/1300 loss 4.495374 loss_att 3.335256 loss_ctc 7.202316 lr 0.00065266 rank 0
I0626 07:45:40.811195 139724881797504 executor.py:115] TRAIN Batch 124/1400 loss 6.222497 loss_att 3.428978 loss_ctc 12.740707 lr 0.00065249 rank 0
I0626 07:45:58.390877 139724881797504 executor.py:115] TRAIN Batch 124/1500 loss 3.659697 loss_att 3.480137 loss_ctc 4.078671 lr 0.00065233 rank 0
I0626 07:46:04.049564 139724881797504 executor.py:152] CV Batch 124/0 loss 16.414574 loss_att 14.212452 loss_ctc 21.552855 history loss 14.590732 rank 0
I0626 07:46:13.390020 139724881797504 executor.py:152] CV Batch 124/100 loss 38.961018 loss_att 39.452728 loss_ctc 37.813698 history loss 27.053045 rank 0
I0626 07:46:20.286054 139724881797504 train.py:288] Epoch 124 CV info cv_loss 31.6499179699
I0626 07:46:20.286223 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/124.pt
I0626 07:46:20.554496 139724881797504 train.py:274] Epoch 125 TRAIN info lr 0.0006523280730534422
I0626 07:46:20.557350 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:46:20.557408 139724881797504 executor.py:46] total epoch is 125.
I0626 07:46:28.476157 139724881797504 executor.py:115] TRAIN Batch 125/0 loss 4.021708 loss_att 2.597846 loss_ctc 7.344052 lr 0.00065230 rank 0
I0626 07:46:45.464823 139724881797504 executor.py:115] TRAIN Batch 125/100 loss 6.045321 loss_att 4.146298 loss_ctc 10.476375 lr 0.00065213 rank 0
I0626 07:47:02.341650 139724881797504 executor.py:115] TRAIN Batch 125/200 loss 5.427032 loss_att 4.138374 loss_ctc 8.433900 lr 0.00065197 rank 0
I0626 07:47:19.955385 139724881797504 executor.py:115] TRAIN Batch 125/300 loss 3.370502 loss_att 2.473823 loss_ctc 5.462753 lr 0.00065180 rank 0
I0626 07:47:37.605147 139724881797504 executor.py:115] TRAIN Batch 125/400 loss 6.195166 loss_att 4.507348 loss_ctc 10.133406 lr 0.00065161 rank 0
I0626 07:47:55.456924 139724881797504 executor.py:115] TRAIN Batch 125/500 loss 7.271960 loss_att 3.907548 loss_ctc 15.122252 lr 0.00065144 rank 0
I0626 07:48:11.773162 139724881797504 executor.py:115] TRAIN Batch 125/600 loss 8.278351 loss_att 5.418032 loss_ctc 14.952426 lr 0.00065128 rank 0
I0626 07:48:29.187909 139724881797504 executor.py:115] TRAIN Batch 125/700 loss 7.972979 loss_att 4.588758 loss_ctc 15.869491 lr 0.00065111 rank 0
I0626 07:48:46.818680 139724881797504 executor.py:115] TRAIN Batch 125/800 loss 5.264607 loss_att 3.511943 loss_ctc 9.354156 lr 0.00065092 rank 0
I0626 07:49:04.432974 139724881797504 executor.py:115] TRAIN Batch 125/900 loss 1.452481 loss_att 1.906125 loss_ctc 0.393977 lr 0.00065075 rank 0
I0626 07:49:21.836046 139724881797504 executor.py:115] TRAIN Batch 125/1000 loss 3.851896 loss_att 2.364153 loss_ctc 7.323296 lr 0.00065059 rank 0
I0626 07:49:38.723954 139724881797504 executor.py:115] TRAIN Batch 125/1100 loss 5.779592 loss_att 4.481174 loss_ctc 8.809233 lr 0.00065042 rank 0
I0626 07:49:56.205163 139724881797504 executor.py:115] TRAIN Batch 125/1200 loss 5.857995 loss_att 3.677828 loss_ctc 10.945051 lr 0.00065023 rank 0
I0626 07:50:13.573603 139724881797504 executor.py:115] TRAIN Batch 125/1300 loss 5.536020 loss_att 3.969158 loss_ctc 9.192033 lr 0.00065006 rank 0
I0626 07:50:31.098812 139724881797504 executor.py:115] TRAIN Batch 125/1400 loss 3.861974 loss_att 2.876322 loss_ctc 6.161826 lr 0.00064990 rank 0
I0626 07:50:48.326060 139724881797504 executor.py:115] TRAIN Batch 125/1500 loss 5.157555 loss_att 4.695002 loss_ctc 6.236845 lr 0.00064973 rank 0
I0626 07:50:53.940607 139724881797504 executor.py:152] CV Batch 125/0 loss 16.621681 loss_att 13.988552 loss_ctc 22.765650 history loss 14.774828 rank 0
I0626 07:51:03.131246 139724881797504 executor.py:152] CV Batch 125/100 loss 38.986565 loss_att 39.618725 loss_ctc 37.511520 history loss 27.122278 rank 0
I0626 07:51:10.040235 139724881797504 train.py:288] Epoch 125 CV info cv_loss 31.705952236391163
I0626 07:51:10.040346 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/125.pt
I0626 07:51:10.307449 139724881797504 train.py:274] Epoch 126 TRAIN info lr 0.0006497343129878754
I0626 07:51:10.310208 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:51:10.310266 139724881797504 executor.py:46] total epoch is 126.
I0626 07:51:18.180069 139724881797504 executor.py:115] TRAIN Batch 126/0 loss 4.240975 loss_att 2.725307 loss_ctc 7.777534 lr 0.00064971 rank 0
I0626 07:51:35.337381 139724881797504 executor.py:115] TRAIN Batch 126/100 loss 6.273727 loss_att 4.313793 loss_ctc 10.846907 lr 0.00064954 rank 0
I0626 07:51:52.793638 139724881797504 executor.py:115] TRAIN Batch 126/200 loss 6.480931 loss_att 4.700711 loss_ctc 10.634779 lr 0.00064938 rank 0
I0626 07:52:10.665580 139724881797504 executor.py:115] TRAIN Batch 126/300 loss 4.876141 loss_att 3.357443 loss_ctc 8.419767 lr 0.00064921 rank 0
I0626 07:52:28.669468 139724881797504 executor.py:115] TRAIN Batch 126/400 loss 4.201334 loss_att 2.645448 loss_ctc 7.831735 lr 0.00064902 rank 0
I0626 07:52:46.830647 139724881797504 executor.py:115] TRAIN Batch 126/500 loss 4.593602 loss_att 2.808127 loss_ctc 8.759710 lr 0.00064886 rank 0
I0626 07:53:03.674367 139724881797504 executor.py:115] TRAIN Batch 126/600 loss 5.388872 loss_att 5.124606 loss_ctc 6.005494 lr 0.00064869 rank 0
I0626 07:53:21.027031 139724881797504 executor.py:115] TRAIN Batch 126/700 loss 7.189837 loss_att 4.442939 loss_ctc 13.599263 lr 0.00064853 rank 0
I0626 07:53:38.574448 139724881797504 executor.py:115] TRAIN Batch 126/800 loss 3.543979 loss_att 2.543190 loss_ctc 5.879152 lr 0.00064834 rank 0
I0626 07:53:56.244881 139724881797504 executor.py:115] TRAIN Batch 126/900 loss 3.364292 loss_att 2.371777 loss_ctc 5.680161 lr 0.00064818 rank 0
I0626 07:54:13.631942 139724881797504 executor.py:115] TRAIN Batch 126/1000 loss 1.832380 loss_att 1.392630 loss_ctc 2.858461 lr 0.00064801 rank 0
I0626 07:54:30.676196 139724881797504 executor.py:115] TRAIN Batch 126/1100 loss 7.220939 loss_att 5.038190 loss_ctc 12.314018 lr 0.00064785 rank 0
I0626 07:54:48.171887 139724881797504 executor.py:115] TRAIN Batch 126/1200 loss 5.046770 loss_att 3.588312 loss_ctc 8.449839 lr 0.00064766 rank 0
I0626 07:55:05.871415 139724881797504 executor.py:115] TRAIN Batch 126/1300 loss 7.669533 loss_att 4.912199 loss_ctc 14.103312 lr 0.00064750 rank 0
I0626 07:55:23.330911 139724881797504 executor.py:115] TRAIN Batch 126/1400 loss 4.815216 loss_att 2.660724 loss_ctc 9.842364 lr 0.00064733 rank 0
I0626 07:55:41.212444 139724881797504 executor.py:115] TRAIN Batch 126/1500 loss 3.588904 loss_att 2.699492 loss_ctc 5.664199 lr 0.00064717 rank 0
I0626 07:55:46.847168 139724881797504 executor.py:152] CV Batch 126/0 loss 16.295549 loss_att 14.544233 loss_ctc 20.381950 history loss 14.484933 rank 0
I0626 07:55:56.413513 139724881797504 executor.py:152] CV Batch 126/100 loss 38.071838 loss_att 37.762337 loss_ctc 38.794006 history loss 26.847479 rank 0
I0626 07:56:03.333159 139724881797504 train.py:288] Epoch 126 CV info cv_loss 31.203922982492067
I0626 07:56:03.333302 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/126.pt
I0626 07:56:03.598642 139724881797504 train.py:274] Epoch 127 TRAIN info lr 0.0006471712485079141
I0626 07:56:03.601732 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 07:56:03.601794 139724881797504 executor.py:46] total epoch is 127.
I0626 07:56:11.580930 139724881797504 executor.py:115] TRAIN Batch 127/0 loss 2.390577 loss_att 1.354585 loss_ctc 4.807890 lr 0.00064714 rank 0
I0626 07:56:28.698062 139724881797504 executor.py:115] TRAIN Batch 127/100 loss 6.873597 loss_att 4.727527 loss_ctc 11.881094 lr 0.00064698 rank 0
I0626 07:56:46.029957 139724881797504 executor.py:115] TRAIN Batch 127/200 loss 4.807123 loss_att 4.487359 loss_ctc 5.553240 lr 0.00064682 rank 0
I0626 07:57:03.623974 139724881797504 executor.py:115] TRAIN Batch 127/300 loss 7.653994 loss_att 5.248121 loss_ctc 13.267696 lr 0.00064666 rank 0
I0626 07:57:21.681468 139724881797504 executor.py:115] TRAIN Batch 127/400 loss 7.544402 loss_att 3.763818 loss_ctc 16.365767 lr 0.00064647 rank 0
I0626 07:57:39.984591 139724881797504 executor.py:115] TRAIN Batch 127/500 loss 3.976189 loss_att 2.271420 loss_ctc 7.953980 lr 0.00064631 rank 0
I0626 07:57:56.708957 139724881797504 executor.py:115] TRAIN Batch 127/600 loss 6.903847 loss_att 4.488465 loss_ctc 12.539739 lr 0.00064614 rank 0
I0626 07:58:14.373517 139724881797504 executor.py:115] TRAIN Batch 127/700 loss 3.369835 loss_att 2.939313 loss_ctc 4.374387 lr 0.00064598 rank 0
I0626 07:58:32.173711 139724881797504 executor.py:115] TRAIN Batch 127/800 loss 4.434702 loss_att 3.179235 loss_ctc 7.364125 lr 0.00064579 rank 0
I0626 07:58:49.795463 139724881797504 executor.py:115] TRAIN Batch 127/900 loss 4.657309 loss_att 3.102403 loss_ctc 8.285423 lr 0.00064563 rank 0
I0626 07:59:07.473512 139724881797504 executor.py:115] TRAIN Batch 127/1000 loss 3.789317 loss_att 2.631623 loss_ctc 6.490602 lr 0.00064547 rank 0
I0626 07:59:24.257849 139724881797504 executor.py:115] TRAIN Batch 127/1100 loss 6.568223 loss_att 4.262789 loss_ctc 11.947571 lr 0.00064531 rank 0
I0626 07:59:41.711122 139724881797504 executor.py:115] TRAIN Batch 127/1200 loss 5.975624 loss_att 4.030386 loss_ctc 10.514512 lr 0.00064512 rank 0
I0626 07:59:59.084135 139724881797504 executor.py:115] TRAIN Batch 127/1300 loss 5.595312 loss_att 4.034724 loss_ctc 9.236686 lr 0.00064496 rank 0
I0626 08:00:16.582882 139724881797504 executor.py:115] TRAIN Batch 127/1400 loss 6.416999 loss_att 3.657341 loss_ctc 12.856202 lr 0.00064480 rank 0
I0626 08:00:33.882460 139724881797504 executor.py:115] TRAIN Batch 127/1500 loss 4.791040 loss_att 3.405643 loss_ctc 8.023631 lr 0.00064464 rank 0
I0626 08:00:39.492102 139724881797504 executor.py:152] CV Batch 127/0 loss 15.893515 loss_att 13.996580 loss_ctc 20.319698 history loss 14.127569 rank 0
I0626 08:00:48.751863 139724881797504 executor.py:152] CV Batch 127/100 loss 39.000778 loss_att 39.718220 loss_ctc 37.326744 history loss 27.132391 rank 0
I0626 08:00:55.669254 139724881797504 train.py:288] Epoch 127 CV info cv_loss 31.582039603468264
I0626 08:00:55.669463 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/127.pt
I0626 08:00:55.937081 139724881797504 train.py:274] Epoch 128 TRAIN info lr 0.0006446382789117459
I0626 08:00:55.940141 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:00:55.940210 139724881797504 executor.py:46] total epoch is 128.
I0626 08:01:03.789238 139724881797504 executor.py:115] TRAIN Batch 128/0 loss 5.329953 loss_att 2.839938 loss_ctc 11.139987 lr 0.00064461 rank 0
I0626 08:01:20.910122 139724881797504 executor.py:115] TRAIN Batch 128/100 loss 6.464910 loss_att 4.674685 loss_ctc 10.642103 lr 0.00064445 rank 0
I0626 08:01:37.798166 139724881797504 executor.py:115] TRAIN Batch 128/200 loss 6.869456 loss_att 4.229115 loss_ctc 13.030252 lr 0.00064429 rank 0
I0626 08:01:55.020485 139724881797504 executor.py:115] TRAIN Batch 128/300 loss 5.475443 loss_att 3.635233 loss_ctc 9.769266 lr 0.00064413 rank 0
I0626 08:02:12.570030 139724881797504 executor.py:115] TRAIN Batch 128/400 loss 3.647312 loss_att 2.126319 loss_ctc 7.196294 lr 0.00064394 rank 0
I0626 08:02:30.258655 139724881797504 executor.py:115] TRAIN Batch 128/500 loss 1.394419 loss_att 1.422873 loss_ctc 1.328024 lr 0.00064378 rank 0
I0626 08:02:46.491635 139724881797504 executor.py:115] TRAIN Batch 128/600 loss 4.331255 loss_att 4.197149 loss_ctc 4.644169 lr 0.00064362 rank 0
I0626 08:03:04.028126 139724881797504 executor.py:115] TRAIN Batch 128/700 loss 6.135249 loss_att 4.686432 loss_ctc 9.515821 lr 0.00064346 rank 0
I0626 08:03:21.760748 139724881797504 executor.py:115] TRAIN Batch 128/800 loss 3.387697 loss_att 3.040897 loss_ctc 4.196898 lr 0.00064328 rank 0
I0626 08:03:39.300485 139724881797504 executor.py:115] TRAIN Batch 128/900 loss 3.459850 loss_att 2.277153 loss_ctc 6.219475 lr 0.00064312 rank 0
I0626 08:03:56.810196 139724881797504 executor.py:115] TRAIN Batch 128/1000 loss 5.948170 loss_att 3.405524 loss_ctc 11.881008 lr 0.00064296 rank 0
I0626 08:04:13.523026 139724881797504 executor.py:115] TRAIN Batch 128/1100 loss 6.826210 loss_att 4.803599 loss_ctc 11.545633 lr 0.00064280 rank 0
I0626 08:04:31.003068 139724881797504 executor.py:115] TRAIN Batch 128/1200 loss 6.467638 loss_att 4.166203 loss_ctc 11.837651 lr 0.00064261 rank 0
I0626 08:04:48.162017 139724881797504 executor.py:115] TRAIN Batch 128/1300 loss 3.900892 loss_att 3.076241 loss_ctc 5.825078 lr 0.00064245 rank 0
I0626 08:05:05.383173 139724881797504 executor.py:115] TRAIN Batch 128/1400 loss 3.398353 loss_att 3.023533 loss_ctc 4.272933 lr 0.00064229 rank 0
I0626 08:05:22.827425 139724881797504 executor.py:115] TRAIN Batch 128/1500 loss 11.868513 loss_att 6.619346 loss_ctc 24.116570 lr 0.00064213 rank 0
I0626 08:05:28.440843 139724881797504 executor.py:152] CV Batch 128/0 loss 15.738440 loss_att 13.789398 loss_ctc 20.286205 history loss 13.989724 rank 0
I0626 08:05:37.674841 139724881797504 executor.py:152] CV Batch 128/100 loss 38.140823 loss_att 38.100262 loss_ctc 38.235462 history loss 26.763688 rank 0
I0626 08:05:44.584092 139724881797504 train.py:288] Epoch 128 CV info cv_loss 31.36063332301457
I0626 08:05:44.584308 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/128.pt
I0626 08:05:44.850559 139724881797504 train.py:274] Epoch 129 TRAIN info lr 0.0006421348198275033
I0626 08:05:44.853403 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:05:44.853462 139724881797504 executor.py:46] total epoch is 129.
I0626 08:05:52.857278 139724881797504 executor.py:115] TRAIN Batch 129/0 loss 3.232843 loss_att 1.533355 loss_ctc 7.198316 lr 0.00064211 rank 0
I0626 08:06:10.093651 139724881797504 executor.py:115] TRAIN Batch 129/100 loss 5.414360 loss_att 3.757963 loss_ctc 9.279287 lr 0.00064195 rank 0
I0626 08:06:27.599435 139724881797504 executor.py:115] TRAIN Batch 129/200 loss 5.284036 loss_att 3.796616 loss_ctc 8.754683 lr 0.00064179 rank 0
I0626 08:06:45.533208 139724881797504 executor.py:115] TRAIN Batch 129/300 loss 4.473799 loss_att 2.824430 loss_ctc 8.322327 lr 0.00064163 rank 0
I0626 08:07:03.258378 139724881797504 executor.py:115] TRAIN Batch 129/400 loss 4.821761 loss_att 3.096782 loss_ctc 8.846712 lr 0.00064145 rank 0
I0626 08:07:21.558879 139724881797504 executor.py:115] TRAIN Batch 129/500 loss 2.398795 loss_att 1.684487 loss_ctc 4.065514 lr 0.00064129 rank 0
I0626 08:07:38.483811 139724881797504 executor.py:115] TRAIN Batch 129/600 loss 5.660151 loss_att 4.637282 loss_ctc 8.046846 lr 0.00064113 rank 0
I0626 08:07:56.324289 139724881797504 executor.py:115] TRAIN Batch 129/700 loss 6.849263 loss_att 5.008018 loss_ctc 11.145499 lr 0.00064097 rank 0
I0626 08:08:14.118969 139724881797504 executor.py:115] TRAIN Batch 129/800 loss 4.896637 loss_att 3.020598 loss_ctc 9.274060 lr 0.00064079 rank 0
I0626 08:08:32.149076 139724881797504 executor.py:115] TRAIN Batch 129/900 loss 3.715947 loss_att 2.909364 loss_ctc 5.597975 lr 0.00064063 rank 0
I0626 08:08:50.179309 139724881797504 executor.py:115] TRAIN Batch 129/1000 loss 3.371110 loss_att 1.812304 loss_ctc 7.008326 lr 0.00064047 rank 0
I0626 08:09:07.395513 139724881797504 executor.py:115] TRAIN Batch 129/1100 loss 7.427914 loss_att 4.878283 loss_ctc 13.377054 lr 0.00064032 rank 0
I0626 08:09:24.926243 139724881797504 executor.py:115] TRAIN Batch 129/1200 loss 5.308622 loss_att 3.758513 loss_ctc 8.925546 lr 0.00064013 rank 0
I0626 08:09:42.226706 139724881797504 executor.py:115] TRAIN Batch 129/1300 loss 3.274009 loss_att 2.344568 loss_ctc 5.442704 lr 0.00063997 rank 0
I0626 08:09:59.491477 139724881797504 executor.py:115] TRAIN Batch 129/1400 loss 3.609781 loss_att 2.600459 loss_ctc 5.964865 lr 0.00063982 rank 0
I0626 08:10:16.730836 139724881797504 executor.py:115] TRAIN Batch 129/1500 loss 4.423960 loss_att 3.074743 loss_ctc 7.572132 lr 0.00063966 rank 0
I0626 08:10:22.400686 139724881797504 executor.py:152] CV Batch 129/0 loss 16.475325 loss_att 14.675132 loss_ctc 20.675772 history loss 14.644733 rank 0
I0626 08:10:31.763906 139724881797504 executor.py:152] CV Batch 129/100 loss 39.521133 loss_att 39.768211 loss_ctc 38.944622 history loss 27.270848 rank 0
I0626 08:10:38.687726 139724881797504 train.py:288] Epoch 129 CV info cv_loss 31.654500643190435
I0626 08:10:38.687961 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/129.pt
I0626 08:10:38.960044 139724881797504 train.py:274] Epoch 130 TRAIN info lr 0.0006396603026469003
I0626 08:10:38.962955 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:10:38.963013 139724881797504 executor.py:46] total epoch is 130.
I0626 08:10:46.818852 139724881797504 executor.py:115] TRAIN Batch 130/0 loss 3.183732 loss_att 2.152735 loss_ctc 5.589393 lr 0.00063963 rank 0
I0626 08:11:03.851992 139724881797504 executor.py:115] TRAIN Batch 130/100 loss 6.331087 loss_att 4.982259 loss_ctc 9.478352 lr 0.00063948 rank 0
I0626 08:11:20.737658 139724881797504 executor.py:115] TRAIN Batch 130/200 loss 4.233503 loss_att 3.167628 loss_ctc 6.720546 lr 0.00063932 rank 0
I0626 08:11:38.017389 139724881797504 executor.py:115] TRAIN Batch 130/300 loss 3.837105 loss_att 2.717529 loss_ctc 6.449450 lr 0.00063916 rank 0
I0626 08:11:55.505153 139724881797504 executor.py:115] TRAIN Batch 130/400 loss 4.016926 loss_att 2.674882 loss_ctc 7.148361 lr 0.00063898 rank 0
I0626 08:12:13.156434 139724881797504 executor.py:115] TRAIN Batch 130/500 loss 2.775422 loss_att 1.335167 loss_ctc 6.136019 lr 0.00063882 rank 0
I0626 08:12:29.547766 139724881797504 executor.py:115] TRAIN Batch 130/600 loss 5.646423 loss_att 4.141838 loss_ctc 9.157122 lr 0.00063867 rank 0
I0626 08:12:47.179065 139724881797504 executor.py:115] TRAIN Batch 130/700 loss 6.239738 loss_att 3.629191 loss_ctc 12.331015 lr 0.00063851 rank 0
I0626 08:13:04.699565 139724881797504 executor.py:115] TRAIN Batch 130/800 loss 4.347730 loss_att 2.781376 loss_ctc 8.002557 lr 0.00063833 rank 0
I0626 08:13:22.402503 139724881797504 executor.py:115] TRAIN Batch 130/900 loss 6.101478 loss_att 3.285045 loss_ctc 12.673154 lr 0.00063817 rank 0
I0626 08:13:39.858698 139724881797504 executor.py:115] TRAIN Batch 130/1000 loss 3.552282 loss_att 2.006425 loss_ctc 7.159282 lr 0.00063802 rank 0
I0626 08:13:56.572950 139724881797504 executor.py:115] TRAIN Batch 130/1100 loss 7.329247 loss_att 5.297119 loss_ctc 12.070881 lr 0.00063786 rank 0
I0626 08:14:14.201639 139724881797504 executor.py:115] TRAIN Batch 130/1200 loss 6.809022 loss_att 4.177970 loss_ctc 12.948143 lr 0.00063768 rank 0
I0626 08:14:31.473417 139724881797504 executor.py:115] TRAIN Batch 130/1300 loss 7.024874 loss_att 4.221874 loss_ctc 13.565207 lr 0.00063752 rank 0
I0626 08:14:48.882368 139724881797504 executor.py:115] TRAIN Batch 130/1400 loss 4.717561 loss_att 3.064518 loss_ctc 8.574661 lr 0.00063737 rank 0
I0626 08:15:06.401964 139724881797504 executor.py:115] TRAIN Batch 130/1500 loss 4.147466 loss_att 3.066224 loss_ctc 6.670363 lr 0.00063721 rank 0
I0626 08:15:12.028658 139724881797504 executor.py:152] CV Batch 130/0 loss 16.189629 loss_att 13.944721 loss_ctc 21.427748 history loss 14.390781 rank 0
I0626 08:15:21.284242 139724881797504 executor.py:152] CV Batch 130/100 loss 38.710808 loss_att 38.815857 loss_ctc 38.465691 history loss 27.332982 rank 0
I0626 08:15:28.203013 139724881797504 train.py:288] Epoch 130 CV info cv_loss 31.881867839336728
I0626 08:15:28.203252 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/130.pt
I0626 08:15:28.503701 139724881797504 train.py:274] Epoch 131 TRAIN info lr 0.0006372141739826913
I0626 08:15:28.506650 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:15:28.506709 139724881797504 executor.py:46] total epoch is 131.
I0626 08:15:36.481354 139724881797504 executor.py:115] TRAIN Batch 131/0 loss 4.813283 loss_att 3.220765 loss_ctc 8.529160 lr 0.00063719 rank 0
I0626 08:15:53.833103 139724881797504 executor.py:115] TRAIN Batch 131/100 loss 4.656809 loss_att 4.103745 loss_ctc 5.947290 lr 0.00063703 rank 0
I0626 08:16:11.226399 139724881797504 executor.py:115] TRAIN Batch 131/200 loss 2.921777 loss_att 2.814352 loss_ctc 3.172435 lr 0.00063688 rank 0
I0626 08:16:28.619929 139724881797504 executor.py:115] TRAIN Batch 131/300 loss 6.748038 loss_att 3.675124 loss_ctc 13.918171 lr 0.00063672 rank 0
I0626 08:16:46.065952 139724881797504 executor.py:115] TRAIN Batch 131/400 loss 4.210985 loss_att 3.533087 loss_ctc 5.792747 lr 0.00063654 rank 0
I0626 08:17:03.995791 139724881797504 executor.py:115] TRAIN Batch 131/500 loss 4.663924 loss_att 2.974658 loss_ctc 8.605543 lr 0.00063639 rank 0
I0626 08:17:20.425269 139724881797504 executor.py:115] TRAIN Batch 131/600 loss 5.534710 loss_att 3.758272 loss_ctc 9.679730 lr 0.00063623 rank 0
I0626 08:17:38.083810 139724881797504 executor.py:115] TRAIN Batch 131/700 loss 8.532989 loss_att 6.103368 loss_ctc 14.202101 lr 0.00063608 rank 0
I0626 08:17:55.597419 139724881797504 executor.py:115] TRAIN Batch 131/800 loss 4.811753 loss_att 3.742941 loss_ctc 7.305650 lr 0.00063590 rank 0
I0626 08:18:13.568965 139724881797504 executor.py:115] TRAIN Batch 131/900 loss 5.386208 loss_att 2.960836 loss_ctc 11.045407 lr 0.00063574 rank 0
I0626 08:18:31.069304 139724881797504 executor.py:115] TRAIN Batch 131/1000 loss 3.929215 loss_att 2.454981 loss_ctc 7.369093 lr 0.00063559 rank 0
I0626 08:18:47.713936 139724881797504 executor.py:115] TRAIN Batch 131/1100 loss 6.713820 loss_att 4.962111 loss_ctc 10.801140 lr 0.00063544 rank 0
I0626 08:19:05.250870 139724881797504 executor.py:115] TRAIN Batch 131/1200 loss 5.515148 loss_att 4.388833 loss_ctc 8.143215 lr 0.00063526 rank 0
I0626 08:19:22.730811 139724881797504 executor.py:115] TRAIN Batch 131/1300 loss 7.333367 loss_att 4.979281 loss_ctc 12.826236 lr 0.00063510 rank 0
I0626 08:19:40.451459 139724881797504 executor.py:115] TRAIN Batch 131/1400 loss 4.099596 loss_att 2.668197 loss_ctc 7.439527 lr 0.00063495 rank 0
I0626 08:19:57.847293 139724881797504 executor.py:115] TRAIN Batch 131/1500 loss 4.823749 loss_att 2.992636 loss_ctc 9.096344 lr 0.00063480 rank 0
I0626 08:20:03.552085 139724881797504 executor.py:152] CV Batch 131/0 loss 15.601418 loss_att 13.960990 loss_ctc 19.429081 history loss 13.867927 rank 0
I0626 08:20:12.778172 139724881797504 executor.py:152] CV Batch 131/100 loss 38.869865 loss_att 39.468353 loss_ctc 37.473389 history loss 27.188668 rank 0
I0626 08:20:19.704206 139724881797504 train.py:288] Epoch 131 CV info cv_loss 31.547439452741667
I0626 08:20:19.704447 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/131.pt
I0626 08:20:19.971985 139724881797504 train.py:274] Epoch 132 TRAIN info lr 0.0006347958951487819
I0626 08:20:19.975084 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:20:19.975147 139724881797504 executor.py:46] total epoch is 132.
I0626 08:20:27.854161 139724881797504 executor.py:115] TRAIN Batch 132/0 loss 5.006722 loss_att 2.405844 loss_ctc 11.075438 lr 0.00063477 rank 0
I0626 08:20:44.938579 139724881797504 executor.py:115] TRAIN Batch 132/100 loss 4.682727 loss_att 3.276531 loss_ctc 7.963852 lr 0.00063462 rank 0
I0626 08:21:02.059238 139724881797504 executor.py:115] TRAIN Batch 132/200 loss 5.922297 loss_att 4.026790 loss_ctc 10.345145 lr 0.00063446 rank 0
I0626 08:21:19.364071 139724881797504 executor.py:115] TRAIN Batch 132/300 loss 5.069601 loss_att 2.899963 loss_ctc 10.132090 lr 0.00063431 rank 0
I0626 08:21:37.460705 139724881797504 executor.py:115] TRAIN Batch 132/400 loss 3.772186 loss_att 2.513633 loss_ctc 6.708808 lr 0.00063413 rank 0
I0626 08:21:55.578975 139724881797504 executor.py:115] TRAIN Batch 132/500 loss 1.878897 loss_att 1.358854 loss_ctc 3.092331 lr 0.00063398 rank 0
I0626 08:22:12.559237 139724881797504 executor.py:115] TRAIN Batch 132/600 loss 5.926518 loss_att 4.231321 loss_ctc 9.881977 lr 0.00063383 rank 0
I0626 08:22:30.136074 139724881797504 executor.py:115] TRAIN Batch 132/700 loss 3.609118 loss_att 3.073136 loss_ctc 4.859741 lr 0.00063367 rank 0
I0626 08:22:48.013769 139724881797504 executor.py:115] TRAIN Batch 132/800 loss 4.751283 loss_att 2.877882 loss_ctc 9.122553 lr 0.00063350 rank 0
I0626 08:23:05.756472 139724881797504 executor.py:115] TRAIN Batch 132/900 loss 5.930937 loss_att 2.806747 loss_ctc 13.220712 lr 0.00063334 rank 0
I0626 08:23:23.067368 139724881797504 executor.py:115] TRAIN Batch 132/1000 loss 5.059180 loss_att 3.182252 loss_ctc 9.438681 lr 0.00063319 rank 0
I0626 08:23:39.908437 139724881797504 executor.py:115] TRAIN Batch 132/1100 loss 6.196866 loss_att 5.192988 loss_ctc 8.539246 lr 0.00063304 rank 0
I0626 08:23:58.107401 139724881797504 executor.py:115] TRAIN Batch 132/1200 loss 6.579908 loss_att 4.210905 loss_ctc 12.107582 lr 0.00063286 rank 0
I0626 08:24:15.597277 139724881797504 executor.py:115] TRAIN Batch 132/1300 loss 5.538037 loss_att 3.744451 loss_ctc 9.723073 lr 0.00063271 rank 0
I0626 08:24:32.916064 139724881797504 executor.py:115] TRAIN Batch 132/1400 loss 4.600932 loss_att 2.924166 loss_ctc 8.513384 lr 0.00063256 rank 0
I0626 08:24:50.320946 139724881797504 executor.py:115] TRAIN Batch 132/1500 loss 4.549971 loss_att 3.636671 loss_ctc 6.681003 lr 0.00063240 rank 0
I0626 08:24:55.938321 139724881797504 executor.py:152] CV Batch 132/0 loss 16.471439 loss_att 14.548432 loss_ctc 20.958456 history loss 14.641279 rank 0
I0626 08:25:05.147735 139724881797504 executor.py:152] CV Batch 132/100 loss 39.796097 loss_att 39.905746 loss_ctc 39.540253 history loss 27.292705 rank 0
I0626 08:25:12.044068 139724881797504 train.py:288] Epoch 132 CV info cv_loss 31.70190312025519
I0626 08:25:12.044182 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/132.pt
I0626 08:25:12.310369 139724881797504 train.py:274] Epoch 133 TRAIN info lr 0.0006324049416618769
I0626 08:25:12.313275 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:25:12.313346 139724881797504 executor.py:46] total epoch is 133.
I0626 08:25:20.443332 139724881797504 executor.py:115] TRAIN Batch 133/0 loss 2.076869 loss_att 1.451770 loss_ctc 3.535435 lr 0.00063238 rank 0
I0626 08:25:37.439162 139724881797504 executor.py:115] TRAIN Batch 133/100 loss 5.984716 loss_att 4.470054 loss_ctc 9.518929 lr 0.00063223 rank 0
I0626 08:25:54.320561 139724881797504 executor.py:115] TRAIN Batch 133/200 loss 5.803550 loss_att 4.116322 loss_ctc 9.740416 lr 0.00063208 rank 0
I0626 08:26:11.774799 139724881797504 executor.py:115] TRAIN Batch 133/300 loss 3.548954 loss_att 2.204923 loss_ctc 6.685027 lr 0.00063192 rank 0
I0626 08:26:29.844913 139724881797504 executor.py:115] TRAIN Batch 133/400 loss 3.391267 loss_att 2.825120 loss_ctc 4.712277 lr 0.00063175 rank 0
I0626 08:26:47.712728 139724881797504 executor.py:115] TRAIN Batch 133/500 loss 1.953873 loss_att 1.261783 loss_ctc 3.568749 lr 0.00063160 rank 0
I0626 08:27:04.196303 139724881797504 executor.py:115] TRAIN Batch 133/600 loss 8.074980 loss_att 6.469602 loss_ctc 11.820860 lr 0.00063145 rank 0
I0626 08:27:21.780934 139724881797504 executor.py:115] TRAIN Batch 133/700 loss 5.163973 loss_att 3.570862 loss_ctc 8.881231 lr 0.00063130 rank 0
I0626 08:27:39.512479 139724881797504 executor.py:115] TRAIN Batch 133/800 loss 4.678624 loss_att 3.299738 loss_ctc 7.896025 lr 0.00063112 rank 0
I0626 08:27:56.878181 139724881797504 executor.py:115] TRAIN Batch 133/900 loss 5.965395 loss_att 3.518191 loss_ctc 11.675537 lr 0.00063097 rank 0
I0626 08:28:14.090137 139724881797504 executor.py:115] TRAIN Batch 133/1000 loss 4.522030 loss_att 2.576490 loss_ctc 9.061623 lr 0.00063082 rank 0
I0626 08:28:31.235225 139724881797504 executor.py:115] TRAIN Batch 133/1100 loss 5.521818 loss_att 4.389034 loss_ctc 8.164981 lr 0.00063067 rank 0
I0626 08:28:48.992789 139724881797504 executor.py:115] TRAIN Batch 133/1200 loss 4.771791 loss_att 3.280755 loss_ctc 8.250878 lr 0.00063049 rank 0
I0626 08:29:06.696581 139724881797504 executor.py:115] TRAIN Batch 133/1300 loss 6.811783 loss_att 4.832004 loss_ctc 11.431267 lr 0.00063034 rank 0
I0626 08:29:24.386961 139724881797504 executor.py:115] TRAIN Batch 133/1400 loss 4.240751 loss_att 3.076574 loss_ctc 6.957162 lr 0.00063019 rank 0
I0626 08:29:42.019047 139724881797504 executor.py:115] TRAIN Batch 133/1500 loss 3.294323 loss_att 2.208305 loss_ctc 5.828366 lr 0.00063004 rank 0
I0626 08:29:47.644233 139724881797504 executor.py:152] CV Batch 133/0 loss 15.887344 loss_att 13.681616 loss_ctc 21.034042 history loss 14.122084 rank 0
I0626 08:29:56.835423 139724881797504 executor.py:152] CV Batch 133/100 loss 39.158211 loss_att 39.749832 loss_ctc 37.777760 history loss 26.930633 rank 0
I0626 08:30:03.761390 139724881797504 train.py:288] Epoch 133 CV info cv_loss 31.44095278817794
I0626 08:30:03.761553 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/133.pt
I0626 08:30:04.027871 139724881797504 train.py:274] Epoch 134 TRAIN info lr 0.0006300408027636233
I0626 08:30:04.030803 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:30:04.030862 139724881797504 executor.py:46] total epoch is 134.
I0626 08:30:11.993735 139724881797504 executor.py:115] TRAIN Batch 134/0 loss 4.497978 loss_att 3.012011 loss_ctc 7.965234 lr 0.00063002 rank 0
I0626 08:30:29.285976 139724881797504 executor.py:115] TRAIN Batch 134/100 loss 5.895437 loss_att 4.715163 loss_ctc 8.649408 lr 0.00062987 rank 0
I0626 08:30:46.686082 139724881797504 executor.py:115] TRAIN Batch 134/200 loss 3.619000 loss_att 2.862285 loss_ctc 5.384669 lr 0.00062972 rank 0
I0626 08:31:04.751297 139724881797504 executor.py:115] TRAIN Batch 134/300 loss 4.067191 loss_att 2.724959 loss_ctc 7.199064 lr 0.00062957 rank 0
I0626 08:31:23.054346 139724881797504 executor.py:115] TRAIN Batch 134/400 loss 4.631732 loss_att 2.976497 loss_ctc 8.493947 lr 0.00062939 rank 0
I0626 08:31:41.178095 139724881797504 executor.py:115] TRAIN Batch 134/500 loss 5.120448 loss_att 2.902220 loss_ctc 10.296312 lr 0.00062924 rank 0
I0626 08:31:57.576351 139724881797504 executor.py:115] TRAIN Batch 134/600 loss 4.001943 loss_att 3.760365 loss_ctc 4.565623 lr 0.00062909 rank 0
I0626 08:32:14.847207 139724881797504 executor.py:115] TRAIN Batch 134/700 loss 5.174549 loss_att 3.707098 loss_ctc 8.598602 lr 0.00062894 rank 0
I0626 08:32:32.556234 139724881797504 executor.py:115] TRAIN Batch 134/800 loss 3.880739 loss_att 2.552899 loss_ctc 6.979033 lr 0.00062877 rank 0
I0626 08:32:50.138862 139724881797504 executor.py:115] TRAIN Batch 134/900 loss 3.611980 loss_att 2.178075 loss_ctc 6.957759 lr 0.00062862 rank 0
I0626 08:33:07.397690 139724881797504 executor.py:115] TRAIN Batch 134/1000 loss 2.577985 loss_att 1.644538 loss_ctc 4.756027 lr 0.00062847 rank 0
I0626 08:33:24.248920 139724881797504 executor.py:115] TRAIN Batch 134/1100 loss 5.212261 loss_att 4.142869 loss_ctc 7.707508 lr 0.00062832 rank 0
I0626 08:33:41.908654 139724881797504 executor.py:115] TRAIN Batch 134/1200 loss 4.775867 loss_att 3.620342 loss_ctc 7.472091 lr 0.00062815 rank 0
I0626 08:33:59.211815 139724881797504 executor.py:115] TRAIN Batch 134/1300 loss 4.405259 loss_att 3.044101 loss_ctc 7.581294 lr 0.00062800 rank 0
I0626 08:34:16.405191 139724881797504 executor.py:115] TRAIN Batch 134/1400 loss 4.856031 loss_att 3.152131 loss_ctc 8.831800 lr 0.00062785 rank 0
I0626 08:34:33.835353 139724881797504 executor.py:115] TRAIN Batch 134/1500 loss 4.745225 loss_att 3.562550 loss_ctc 7.504798 lr 0.00062770 rank 0
I0626 08:34:39.436570 139724881797504 executor.py:152] CV Batch 134/0 loss 15.658887 loss_att 13.337526 loss_ctc 21.075394 history loss 13.919011 rank 0
I0626 08:34:48.747803 139724881797504 executor.py:152] CV Batch 134/100 loss 38.940605 loss_att 39.430840 loss_ctc 37.796730 history loss 26.978477 rank 0
I0626 08:34:55.659220 139724881797504 train.py:288] Epoch 134 CV info cv_loss 31.235075727677064
I0626 08:34:55.659413 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/134.pt
I0626 08:34:55.926262 139724881797504 train.py:274] Epoch 135 TRAIN info lr 0.0006277029809622578
I0626 08:34:55.929141 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:34:55.929211 139724881797504 executor.py:46] total epoch is 135.
I0626 08:35:03.882090 139724881797504 executor.py:115] TRAIN Batch 135/0 loss 2.076962 loss_att 1.382071 loss_ctc 3.698375 lr 0.00062768 rank 0
I0626 08:35:20.882423 139724881797504 executor.py:115] TRAIN Batch 135/100 loss 5.533573 loss_att 3.872411 loss_ctc 9.409618 lr 0.00062753 rank 0
I0626 08:35:37.967692 139724881797504 executor.py:115] TRAIN Batch 135/200 loss 3.457627 loss_att 2.812921 loss_ctc 4.961944 lr 0.00062738 rank 0
I0626 08:35:55.360383 139724881797504 executor.py:115] TRAIN Batch 135/300 loss 5.028301 loss_att 3.674288 loss_ctc 8.187664 lr 0.00062723 rank 0
I0626 08:36:12.910755 139724881797504 executor.py:115] TRAIN Batch 135/400 loss 4.782864 loss_att 3.027304 loss_ctc 8.879169 lr 0.00062706 rank 0
I0626 08:36:30.745300 139724881797504 executor.py:115] TRAIN Batch 135/500 loss 3.464366 loss_att 2.100053 loss_ctc 6.647763 lr 0.00062691 rank 0
I0626 08:36:47.376655 139724881797504 executor.py:115] TRAIN Batch 135/600 loss 7.612960 loss_att 5.738877 loss_ctc 11.985820 lr 0.00062677 rank 0
I0626 08:37:05.057352 139724881797504 executor.py:115] TRAIN Batch 135/700 loss 5.914916 loss_att 4.635160 loss_ctc 8.901012 lr 0.00062662 rank 0
I0626 08:37:22.623718 139724881797504 executor.py:115] TRAIN Batch 135/800 loss 6.240935 loss_att 4.249145 loss_ctc 10.888445 lr 0.00062645 rank 0
I0626 08:37:40.718869 139724881797504 executor.py:115] TRAIN Batch 135/900 loss 3.494313 loss_att 3.390524 loss_ctc 3.736485 lr 0.00062630 rank 0
I0626 08:37:58.225938 139724881797504 executor.py:115] TRAIN Batch 135/1000 loss 1.965050 loss_att 1.680944 loss_ctc 2.627964 lr 0.00062615 rank 0
I0626 08:38:15.071951 139724881797504 executor.py:115] TRAIN Batch 135/1100 loss 4.071373 loss_att 3.502026 loss_ctc 5.399848 lr 0.00062600 rank 0
I0626 08:38:32.886213 139724881797504 executor.py:115] TRAIN Batch 135/1200 loss 5.514466 loss_att 3.550469 loss_ctc 10.097126 lr 0.00062583 rank 0
I0626 08:38:50.265295 139724881797504 executor.py:115] TRAIN Batch 135/1300 loss 4.158617 loss_att 2.476430 loss_ctc 8.083718 lr 0.00062568 rank 0
I0626 08:39:07.923576 139724881797504 executor.py:115] TRAIN Batch 135/1400 loss 5.093976 loss_att 3.513108 loss_ctc 8.782669 lr 0.00062554 rank 0
I0626 08:39:25.407808 139724881797504 executor.py:115] TRAIN Batch 135/1500 loss 2.963196 loss_att 2.825487 loss_ctc 3.284518 lr 0.00062539 rank 0
I0626 08:39:31.047272 139724881797504 executor.py:152] CV Batch 135/0 loss 15.288568 loss_att 13.623427 loss_ctc 19.173895 history loss 13.589838 rank 0
I0626 08:39:40.290361 139724881797504 executor.py:152] CV Batch 135/100 loss 38.974693 loss_att 38.979591 loss_ctc 38.963272 history loss 26.949723 rank 0
I0626 08:39:47.214197 139724881797504 train.py:288] Epoch 135 CV info cv_loss 31.426971701750453
I0626 08:39:47.214339 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/135.pt
I0626 08:39:47.481003 139724881797504 train.py:274] Epoch 136 TRAIN info lr 0.0006253909915928249
I0626 08:39:47.483992 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:39:47.484065 139724881797504 executor.py:46] total epoch is 136.
I0626 08:39:56.067816 139724881797504 executor.py:115] TRAIN Batch 136/0 loss 2.872660 loss_att 2.198160 loss_ctc 4.446493 lr 0.00062537 rank 0
I0626 08:40:13.344055 139724881797504 executor.py:115] TRAIN Batch 136/100 loss 5.104214 loss_att 3.430676 loss_ctc 9.009136 lr 0.00062522 rank 0
I0626 08:40:30.273708 139724881797504 executor.py:115] TRAIN Batch 136/200 loss 7.764585 loss_att 5.516809 loss_ctc 13.009396 lr 0.00062507 rank 0
I0626 08:40:48.077513 139724881797504 executor.py:115] TRAIN Batch 136/300 loss 3.993219 loss_att 2.597001 loss_ctc 7.251061 lr 0.00062493 rank 0
I0626 08:41:06.020495 139724881797504 executor.py:115] TRAIN Batch 136/400 loss 7.737319 loss_att 3.720632 loss_ctc 17.109587 lr 0.00062476 rank 0
I0626 08:41:24.726294 139724881797504 executor.py:115] TRAIN Batch 136/500 loss 3.354557 loss_att 2.002531 loss_ctc 6.509283 lr 0.00062461 rank 0
I0626 08:41:41.143392 139724881797504 executor.py:115] TRAIN Batch 136/600 loss 7.382394 loss_att 4.434826 loss_ctc 14.260052 lr 0.00062446 rank 0
I0626 08:41:58.602770 139724881797504 executor.py:115] TRAIN Batch 136/700 loss 6.184503 loss_att 5.230010 loss_ctc 8.411652 lr 0.00062432 rank 0
I0626 08:42:16.006024 139724881797504 executor.py:115] TRAIN Batch 136/800 loss 4.073306 loss_att 2.234852 loss_ctc 8.363030 lr 0.00062415 rank 0
I0626 08:42:33.541515 139724881797504 executor.py:115] TRAIN Batch 136/900 loss 4.274339 loss_att 2.231780 loss_ctc 9.040309 lr 0.00062400 rank 0
I0626 08:42:51.382722 139724881797504 executor.py:115] TRAIN Batch 136/1000 loss 3.670377 loss_att 2.296958 loss_ctc 6.875022 lr 0.00062386 rank 0
I0626 08:43:08.445528 139724881797504 executor.py:115] TRAIN Batch 136/1100 loss 6.022919 loss_att 4.717328 loss_ctc 9.069298 lr 0.00062371 rank 0
I0626 08:43:25.907627 139724881797504 executor.py:115] TRAIN Batch 136/1200 loss 4.534516 loss_att 3.536806 loss_ctc 6.862509 lr 0.00062354 rank 0
I0626 08:43:43.248025 139724881797504 executor.py:115] TRAIN Batch 136/1300 loss 8.368939 loss_att 4.814705 loss_ctc 16.662151 lr 0.00062339 rank 0
I0626 08:44:00.866183 139724881797504 executor.py:115] TRAIN Batch 136/1400 loss 4.209453 loss_att 3.157229 loss_ctc 6.664639 lr 0.00062325 rank 0
I0626 08:44:18.214976 139724881797504 executor.py:115] TRAIN Batch 136/1500 loss 4.537954 loss_att 3.800850 loss_ctc 6.257865 lr 0.00062310 rank 0
I0626 08:44:23.854448 139724881797504 executor.py:152] CV Batch 136/0 loss 16.893721 loss_att 14.440720 loss_ctc 22.617390 history loss 15.016641 rank 0
I0626 08:44:33.095805 139724881797504 executor.py:152] CV Batch 136/100 loss 38.368492 loss_att 39.145905 loss_ctc 36.554535 history loss 27.370629 rank 0
I0626 08:44:40.006440 139724881797504 train.py:288] Epoch 136 CV info cv_loss 31.930272487685635
I0626 08:44:40.006582 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/136.pt
I0626 08:44:40.272729 139724881797504 train.py:274] Epoch 137 TRAIN info lr 0.0006231043623950822
I0626 08:44:40.275854 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:44:40.275923 139724881797504 executor.py:46] total epoch is 137.
I0626 08:44:48.196831 139724881797504 executor.py:115] TRAIN Batch 137/0 loss 2.594322 loss_att 1.822094 loss_ctc 4.396186 lr 0.00062308 rank 0
I0626 08:45:05.064497 139724881797504 executor.py:115] TRAIN Batch 137/100 loss 4.110565 loss_att 3.561109 loss_ctc 5.392628 lr 0.00062294 rank 0
I0626 08:45:22.276301 139724881797504 executor.py:115] TRAIN Batch 137/200 loss 6.946483 loss_att 3.673321 loss_ctc 14.583862 lr 0.00062279 rank 0
I0626 08:45:39.806587 139724881797504 executor.py:115] TRAIN Batch 137/300 loss 4.228502 loss_att 3.252584 loss_ctc 6.505643 lr 0.00062265 rank 0
I0626 08:45:57.632957 139724881797504 executor.py:115] TRAIN Batch 137/400 loss 4.960061 loss_att 2.523727 loss_ctc 10.644841 lr 0.00062248 rank 0
I0626 08:46:15.709071 139724881797504 executor.py:115] TRAIN Batch 137/500 loss 3.966432 loss_att 3.203638 loss_ctc 5.746283 lr 0.00062233 rank 0
I0626 08:46:32.028846 139724881797504 executor.py:115] TRAIN Batch 137/600 loss 3.980341 loss_att 3.379914 loss_ctc 5.381337 lr 0.00062219 rank 0
I0626 08:46:49.644154 139724881797504 executor.py:115] TRAIN Batch 137/700 loss 4.245329 loss_att 3.816167 loss_ctc 5.246708 lr 0.00062204 rank 0
I0626 08:47:06.980122 139724881797504 executor.py:115] TRAIN Batch 137/800 loss 3.781420 loss_att 2.783005 loss_ctc 6.111055 lr 0.00062187 rank 0
I0626 08:47:24.781079 139724881797504 executor.py:115] TRAIN Batch 137/900 loss 5.805203 loss_att 2.910743 loss_ctc 12.558943 lr 0.00062173 rank 0
I0626 08:47:42.422851 139724881797504 executor.py:115] TRAIN Batch 137/1000 loss 2.808567 loss_att 1.814954 loss_ctc 5.126997 lr 0.00062159 rank 0
I0626 08:47:59.420112 139724881797504 executor.py:115] TRAIN Batch 137/1100 loss 4.518824 loss_att 3.381173 loss_ctc 7.173341 lr 0.00062144 rank 0
I0626 08:48:16.820720 139724881797504 executor.py:115] TRAIN Batch 137/1200 loss 5.223288 loss_att 4.024010 loss_ctc 8.021603 lr 0.00062127 rank 0
I0626 08:48:33.960488 139724881797504 executor.py:115] TRAIN Batch 137/1300 loss 5.302695 loss_att 3.465713 loss_ctc 9.588987 lr 0.00062113 rank 0
I0626 08:48:51.149937 139724881797504 executor.py:115] TRAIN Batch 137/1400 loss 2.394294 loss_att 1.820508 loss_ctc 3.733129 lr 0.00062099 rank 0
I0626 08:49:08.409099 139724881797504 executor.py:115] TRAIN Batch 137/1500 loss 3.699531 loss_att 3.040982 loss_ctc 5.236145 lr 0.00062084 rank 0
I0626 08:49:14.032788 139724881797504 executor.py:152] CV Batch 137/0 loss 17.334478 loss_att 14.847700 loss_ctc 23.136961 history loss 15.408425 rank 0
I0626 08:49:23.230990 139724881797504 executor.py:152] CV Batch 137/100 loss 38.847343 loss_att 38.938889 loss_ctc 38.633743 history loss 27.885120 rank 0
I0626 08:49:30.130803 139724881797504 train.py:288] Epoch 137 CV info cv_loss 32.18890090165068
I0626 08:49:30.131058 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/137.pt
I0626 08:49:30.432067 139724881797504 train.py:274] Epoch 138 TRAIN info lr 0.0006208426331082604
I0626 08:49:30.434993 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:49:30.435053 139724881797504 executor.py:46] total epoch is 138.
I0626 08:49:38.295008 139724881797504 executor.py:115] TRAIN Batch 138/0 loss 2.750463 loss_att 1.903654 loss_ctc 4.726352 lr 0.00062082 rank 0
I0626 08:49:55.564136 139724881797504 executor.py:115] TRAIN Batch 138/100 loss 5.503798 loss_att 4.302878 loss_ctc 8.305946 lr 0.00062068 rank 0
I0626 08:50:12.478116 139724881797504 executor.py:115] TRAIN Batch 138/200 loss 7.825518 loss_att 5.298028 loss_ctc 13.722993 lr 0.00062053 rank 0
I0626 08:50:29.970960 139724881797504 executor.py:115] TRAIN Batch 138/300 loss 3.662520 loss_att 3.031948 loss_ctc 5.133854 lr 0.00062039 rank 0
I0626 08:50:47.469800 139724881797504 executor.py:115] TRAIN Batch 138/400 loss 3.866227 loss_att 2.724114 loss_ctc 6.531158 lr 0.00062022 rank 0
I0626 08:51:05.285093 139724881797504 executor.py:115] TRAIN Batch 138/500 loss 2.801082 loss_att 1.654360 loss_ctc 5.476765 lr 0.00062008 rank 0
I0626 08:51:21.751867 139724881797504 executor.py:115] TRAIN Batch 138/600 loss 5.018800 loss_att 4.624042 loss_ctc 5.939904 lr 0.00061994 rank 0
I0626 08:51:39.458858 139724881797504 executor.py:115] TRAIN Batch 138/700 loss 6.647171 loss_att 4.425403 loss_ctc 11.831297 lr 0.00061979 rank 0
I0626 08:51:57.141459 139724881797504 executor.py:115] TRAIN Batch 138/800 loss 6.229874 loss_att 3.887527 loss_ctc 11.695349 lr 0.00061963 rank 0
I0626 08:52:14.900209 139724881797504 executor.py:115] TRAIN Batch 138/900 loss 3.035279 loss_att 1.877374 loss_ctc 5.737058 lr 0.00061948 rank 0
I0626 08:52:32.343909 139724881797504 executor.py:115] TRAIN Batch 138/1000 loss 3.598878 loss_att 1.868778 loss_ctc 7.635778 lr 0.00061934 rank 0
I0626 08:52:49.087842 139724881797504 executor.py:115] TRAIN Batch 138/1100 loss 7.231684 loss_att 4.999416 loss_ctc 12.440309 lr 0.00061920 rank 0
I0626 08:53:06.565138 139724881797504 executor.py:115] TRAIN Batch 138/1200 loss 5.225572 loss_att 3.507208 loss_ctc 9.235085 lr 0.00061903 rank 0
I0626 08:53:24.326265 139724881797504 executor.py:115] TRAIN Batch 138/1300 loss 5.121906 loss_att 3.515331 loss_ctc 8.870580 lr 0.00061889 rank 0
I0626 08:53:41.950650 139724881797504 executor.py:115] TRAIN Batch 138/1400 loss 4.083596 loss_att 3.695255 loss_ctc 4.989727 lr 0.00061875 rank 0
I0626 08:53:59.213616 139724881797504 executor.py:115] TRAIN Batch 138/1500 loss 3.913145 loss_att 2.596458 loss_ctc 6.985415 lr 0.00061861 rank 0
I0626 08:54:04.792940 139724881797504 executor.py:152] CV Batch 138/0 loss 16.861023 loss_att 14.221623 loss_ctc 23.019621 history loss 14.987576 rank 0
I0626 08:54:14.104437 139724881797504 executor.py:152] CV Batch 138/100 loss 38.852760 loss_att 39.243217 loss_ctc 37.941700 history loss 27.503469 rank 0
I0626 08:54:21.034607 139724881797504 train.py:288] Epoch 138 CV info cv_loss 32.12329678159449
I0626 08:54:21.034762 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/138.pt
I0626 08:54:21.310482 139724881797504 train.py:274] Epoch 139 TRAIN info lr 0.0006186053550818861
I0626 08:54:21.313508 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:54:21.313570 139724881797504 executor.py:46] total epoch is 139.
I0626 08:54:29.182083 139724881797504 executor.py:115] TRAIN Batch 139/0 loss 2.114256 loss_att 1.152781 loss_ctc 4.357697 lr 0.00061858 rank 0
I0626 08:54:46.684805 139724881797504 executor.py:115] TRAIN Batch 139/100 loss 6.766207 loss_att 5.381999 loss_ctc 9.996025 lr 0.00061844 rank 0
I0626 08:55:04.177213 139724881797504 executor.py:115] TRAIN Batch 139/200 loss 4.036064 loss_att 3.249063 loss_ctc 5.872398 lr 0.00061830 rank 0
I0626 08:55:22.124973 139724881797504 executor.py:115] TRAIN Batch 139/300 loss 5.871832 loss_att 3.614358 loss_ctc 11.139271 lr 0.00061816 rank 0
I0626 08:55:40.035650 139724881797504 executor.py:115] TRAIN Batch 139/400 loss 2.620843 loss_att 1.999777 loss_ctc 4.069998 lr 0.00061799 rank 0
I0626 08:55:58.426308 139724881797504 executor.py:115] TRAIN Batch 139/500 loss 3.307527 loss_att 1.462281 loss_ctc 7.613101 lr 0.00061785 rank 0
I0626 08:56:14.700222 139724881797504 executor.py:115] TRAIN Batch 139/600 loss 6.752367 loss_att 5.452248 loss_ctc 9.785980 lr 0.00061771 rank 0
I0626 08:56:32.334591 139724881797504 executor.py:115] TRAIN Batch 139/700 loss 6.295958 loss_att 3.419777 loss_ctc 13.007046 lr 0.00061757 rank 0
I0626 08:56:50.040569 139724881797504 executor.py:115] TRAIN Batch 139/800 loss 5.186724 loss_att 3.125024 loss_ctc 9.997357 lr 0.00061740 rank 0
I0626 08:57:07.597637 139724881797504 executor.py:115] TRAIN Batch 139/900 loss 4.582205 loss_att 2.987486 loss_ctc 8.303215 lr 0.00061726 rank 0
I0626 08:57:24.827267 139724881797504 executor.py:115] TRAIN Batch 139/1000 loss 5.158186 loss_att 3.333490 loss_ctc 9.415810 lr 0.00061712 rank 0
I0626 08:57:41.912147 139724881797504 executor.py:115] TRAIN Batch 139/1100 loss 6.433187 loss_att 5.237418 loss_ctc 9.223318 lr 0.00061698 rank 0
I0626 08:57:59.426948 139724881797504 executor.py:115] TRAIN Batch 139/1200 loss 4.538728 loss_att 3.640857 loss_ctc 6.633760 lr 0.00061681 rank 0
I0626 08:58:16.732797 139724881797504 executor.py:115] TRAIN Batch 139/1300 loss 5.082503 loss_att 2.638666 loss_ctc 10.784791 lr 0.00061667 rank 0
I0626 08:58:34.256453 139724881797504 executor.py:115] TRAIN Batch 139/1400 loss 3.282721 loss_att 1.987907 loss_ctc 6.303952 lr 0.00061653 rank 0
I0626 08:58:51.833628 139724881797504 executor.py:115] TRAIN Batch 139/1500 loss 5.538148 loss_att 2.372249 loss_ctc 12.925246 lr 0.00061639 rank 0
I0626 08:58:57.462771 139724881797504 executor.py:152] CV Batch 139/0 loss 16.425444 loss_att 14.283720 loss_ctc 21.422800 history loss 14.600394 rank 0
I0626 08:59:06.682624 139724881797504 executor.py:152] CV Batch 139/100 loss 38.833031 loss_att 39.257187 loss_ctc 37.843330 history loss 27.434737 rank 0
I0626 08:59:13.591939 139724881797504 train.py:288] Epoch 139 CV info cv_loss 31.950496447310943
I0626 08:59:13.592061 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/139.pt
I0626 08:59:13.862591 139724881797504 train.py:274] Epoch 140 TRAIN info lr 0.0006163920909019224
I0626 08:59:13.865465 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 08:59:13.865538 139724881797504 executor.py:46] total epoch is 140.
I0626 08:59:21.835433 139724881797504 executor.py:115] TRAIN Batch 140/0 loss 3.591058 loss_att 1.931108 loss_ctc 7.464274 lr 0.00061637 rank 0
I0626 08:59:39.136075 139724881797504 executor.py:115] TRAIN Batch 140/100 loss 5.051476 loss_att 3.602049 loss_ctc 8.433469 lr 0.00061623 rank 0
I0626 08:59:56.705064 139724881797504 executor.py:115] TRAIN Batch 140/200 loss 4.669163 loss_att 3.449387 loss_ctc 7.515306 lr 0.00061609 rank 0
I0626 09:00:14.417459 139724881797504 executor.py:115] TRAIN Batch 140/300 loss 5.517344 loss_att 3.440184 loss_ctc 10.364053 lr 0.00061595 rank 0
I0626 09:00:32.262459 139724881797504 executor.py:115] TRAIN Batch 140/400 loss 3.833643 loss_att 3.048909 loss_ctc 5.664688 lr 0.00061578 rank 0
I0626 09:00:50.534364 139724881797504 executor.py:115] TRAIN Batch 140/500 loss 2.442760 loss_att 1.524838 loss_ctc 4.584579 lr 0.00061564 rank 0
I0626 09:01:07.166139 139724881797504 executor.py:115] TRAIN Batch 140/600 loss 5.616685 loss_att 3.800555 loss_ctc 9.854321 lr 0.00061550 rank 0
I0626 09:01:24.909192 139724881797504 executor.py:115] TRAIN Batch 140/700 loss 7.691801 loss_att 4.843905 loss_ctc 14.336891 lr 0.00061536 rank 0
I0626 09:01:42.863127 139724881797504 executor.py:115] TRAIN Batch 140/800 loss 5.742640 loss_att 3.849271 loss_ctc 10.160502 lr 0.00061520 rank 0
I0626 09:02:00.455212 139724881797504 executor.py:115] TRAIN Batch 140/900 loss 4.584827 loss_att 3.056176 loss_ctc 8.151680 lr 0.00061506 rank 0
I0626 09:02:18.390554 139724881797504 executor.py:115] TRAIN Batch 140/1000 loss 3.494138 loss_att 2.361687 loss_ctc 6.136525 lr 0.00061492 rank 0
I0626 09:02:35.425374 139724881797504 executor.py:115] TRAIN Batch 140/1100 loss 5.070268 loss_att 3.503096 loss_ctc 8.727003 lr 0.00061478 rank 0
I0626 09:02:53.249137 139724881797504 executor.py:115] TRAIN Batch 140/1200 loss 4.889320 loss_att 3.248258 loss_ctc 8.718465 lr 0.00061462 rank 0
I0626 09:03:10.663416 139724881797504 executor.py:115] TRAIN Batch 140/1300 loss 4.661161 loss_att 3.167476 loss_ctc 8.146427 lr 0.00061448 rank 0
I0626 09:03:28.122915 139724881797504 executor.py:115] TRAIN Batch 140/1400 loss 5.743252 loss_att 3.717718 loss_ctc 10.469498 lr 0.00061434 rank 0
I0626 09:03:45.271658 139724881797504 executor.py:115] TRAIN Batch 140/1500 loss 4.407620 loss_att 3.114764 loss_ctc 7.424284 lr 0.00061420 rank 0
I0626 09:03:50.856594 139724881797504 executor.py:152] CV Batch 140/0 loss 15.635429 loss_att 13.833565 loss_ctc 19.839779 history loss 13.898159 rank 0
I0626 09:04:00.338321 139724881797504 executor.py:152] CV Batch 140/100 loss 38.530228 loss_att 39.350716 loss_ctc 36.615761 history loss 27.021814 rank 0
I0626 09:04:07.408491 139724881797504 train.py:288] Epoch 140 CV info cv_loss 31.514022544972644
I0626 09:04:07.408741 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/140.pt
I0626 09:04:07.712563 139724881797504 train.py:274] Epoch 141 TRAIN info lr 0.0006142024140315169
I0626 09:04:07.715615 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:04:07.715827 139724881797504 executor.py:46] total epoch is 141.
I0626 09:04:16.744934 139724881797504 executor.py:115] TRAIN Batch 141/0 loss 2.432444 loss_att 1.555620 loss_ctc 4.478368 lr 0.00061418 rank 0
I0626 09:04:36.035179 139724881797504 executor.py:115] TRAIN Batch 141/100 loss 5.746378 loss_att 4.041726 loss_ctc 9.723899 lr 0.00061404 rank 0
I0626 09:04:53.541555 139724881797504 executor.py:115] TRAIN Batch 141/200 loss 4.179709 loss_att 3.014407 loss_ctc 6.898747 lr 0.00061390 rank 0
I0626 09:05:11.555380 139724881797504 executor.py:115] TRAIN Batch 141/300 loss 4.271043 loss_att 2.758223 loss_ctc 7.800958 lr 0.00061376 rank 0
I0626 09:05:29.848868 139724881797504 executor.py:115] TRAIN Batch 141/400 loss 4.373893 loss_att 2.122967 loss_ctc 9.626051 lr 0.00061360 rank 0
I0626 09:05:48.036284 139724881797504 executor.py:115] TRAIN Batch 141/500 loss 4.005356 loss_att 2.717931 loss_ctc 7.009347 lr 0.00061346 rank 0
I0626 09:06:05.298062 139724881797504 executor.py:115] TRAIN Batch 141/600 loss 5.035254 loss_att 3.635837 loss_ctc 8.300560 lr 0.00061332 rank 0
I0626 09:06:22.968645 139724881797504 executor.py:115] TRAIN Batch 141/700 loss 4.612710 loss_att 2.945937 loss_ctc 8.501849 lr 0.00061319 rank 0
I0626 09:06:40.969834 139724881797504 executor.py:115] TRAIN Batch 141/800 loss 5.254752 loss_att 3.653404 loss_ctc 8.991231 lr 0.00061302 rank 0
I0626 09:06:58.572640 139724881797504 executor.py:115] TRAIN Batch 141/900 loss 6.721414 loss_att 3.857916 loss_ctc 13.402908 lr 0.00061289 rank 0
I0626 09:07:15.943795 139724881797504 executor.py:115] TRAIN Batch 141/1000 loss 1.810372 loss_att 1.388526 loss_ctc 2.794681 lr 0.00061275 rank 0
I0626 09:07:33.103160 139724881797504 executor.py:115] TRAIN Batch 141/1100 loss 5.177196 loss_att 3.657983 loss_ctc 8.722026 lr 0.00061261 rank 0
I0626 09:07:51.027767 139724881797504 executor.py:115] TRAIN Batch 141/1200 loss 5.211099 loss_att 3.434633 loss_ctc 9.356186 lr 0.00061245 rank 0
I0626 09:08:08.827733 139724881797504 executor.py:115] TRAIN Batch 141/1300 loss 6.405284 loss_att 3.886314 loss_ctc 12.282879 lr 0.00061231 rank 0
I0626 09:08:26.798909 139724881797504 executor.py:115] TRAIN Batch 141/1400 loss 5.668321 loss_att 2.837307 loss_ctc 12.274019 lr 0.00061217 rank 0
I0626 09:08:44.110225 139724881797504 executor.py:115] TRAIN Batch 141/1500 loss 3.496174 loss_att 3.150262 loss_ctc 4.303301 lr 0.00061204 rank 0
I0626 09:08:49.813488 139724881797504 executor.py:152] CV Batch 141/0 loss 17.022919 loss_att 14.584670 loss_ctc 22.712164 history loss 15.131483 rank 0
I0626 09:08:59.668099 139724881797504 executor.py:152] CV Batch 141/100 loss 38.103615 loss_att 38.184479 loss_ctc 37.914936 history loss 27.239713 rank 0
I0626 09:09:06.599296 139724881797504 train.py:288] Epoch 141 CV info cv_loss 31.721117508136885
I0626 09:09:06.599536 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/141.pt
I0626 09:09:06.872681 139724881797504 train.py:274] Epoch 142 TRAIN info lr 0.0006120359084656907
I0626 09:09:06.875705 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:09:06.875765 139724881797504 executor.py:46] total epoch is 142.
I0626 09:09:14.798345 139724881797504 executor.py:115] TRAIN Batch 142/0 loss 3.251553 loss_att 1.863792 loss_ctc 6.489662 lr 0.00061201 rank 0
I0626 09:09:31.938182 139724881797504 executor.py:115] TRAIN Batch 142/100 loss 6.739761 loss_att 4.783589 loss_ctc 11.304162 lr 0.00061188 rank 0
I0626 09:09:49.211024 139724881797504 executor.py:115] TRAIN Batch 142/200 loss 6.714663 loss_att 4.979579 loss_ctc 10.763191 lr 0.00061174 rank 0
I0626 09:10:06.892909 139724881797504 executor.py:115] TRAIN Batch 142/300 loss 5.443707 loss_att 2.985322 loss_ctc 11.179935 lr 0.00061160 rank 0
I0626 09:10:24.854535 139724881797504 executor.py:115] TRAIN Batch 142/400 loss 4.305279 loss_att 3.257114 loss_ctc 6.750997 lr 0.00061144 rank 0
I0626 09:10:42.938072 139724881797504 executor.py:115] TRAIN Batch 142/500 loss 3.863039 loss_att 2.117688 loss_ctc 7.935525 lr 0.00061130 rank 0
I0626 09:10:59.418210 139724881797504 executor.py:115] TRAIN Batch 142/600 loss 5.128063 loss_att 3.611797 loss_ctc 8.666018 lr 0.00061117 rank 0
I0626 09:11:16.925902 139724881797504 executor.py:115] TRAIN Batch 142/700 loss 6.946679 loss_att 4.129926 loss_ctc 13.519102 lr 0.00061103 rank 0
I0626 09:11:34.831430 139724881797504 executor.py:115] TRAIN Batch 142/800 loss 3.598575 loss_att 2.534142 loss_ctc 6.082253 lr 0.00061087 rank 0
I0626 09:11:52.968304 139724881797504 executor.py:115] TRAIN Batch 142/900 loss 5.260824 loss_att 3.210020 loss_ctc 10.046032 lr 0.00061073 rank 0
I0626 09:12:10.615242 139724881797504 executor.py:115] TRAIN Batch 142/1000 loss 3.565980 loss_att 2.341337 loss_ctc 6.423481 lr 0.00061060 rank 0
I0626 09:12:27.519767 139724881797504 executor.py:115] TRAIN Batch 142/1100 loss 4.814651 loss_att 3.532738 loss_ctc 7.805784 lr 0.00061046 rank 0
I0626 09:12:44.991555 139724881797504 executor.py:115] TRAIN Batch 142/1200 loss 4.323266 loss_att 3.233894 loss_ctc 6.865133 lr 0.00061030 rank 0
I0626 09:13:02.126899 139724881797504 executor.py:115] TRAIN Batch 142/1300 loss 2.898626 loss_att 2.417893 loss_ctc 4.020337 lr 0.00061016 rank 0
I0626 09:13:19.613481 139724881797504 executor.py:115] TRAIN Batch 142/1400 loss 2.581726 loss_att 2.078851 loss_ctc 3.755100 lr 0.00061003 rank 0
I0626 09:13:37.055689 139724881797504 executor.py:115] TRAIN Batch 142/1500 loss 5.369082 loss_att 3.617297 loss_ctc 9.456583 lr 0.00060989 rank 0
I0626 09:13:42.733172 139724881797504 executor.py:152] CV Batch 142/0 loss 16.249861 loss_att 14.997055 loss_ctc 19.173073 history loss 14.444321 rank 0
I0626 09:13:51.994051 139724881797504 executor.py:152] CV Batch 142/100 loss 39.626255 loss_att 39.666267 loss_ctc 39.532898 history loss 27.609032 rank 0
I0626 09:13:58.940241 139724881797504 train.py:288] Epoch 142 CV info cv_loss 32.17643636635609
I0626 09:13:58.940452 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/142.pt
I0626 09:13:59.210567 139724881797504 train.py:274] Epoch 143 TRAIN info lr 0.0006098921683993328
I0626 09:13:59.213686 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:13:59.213746 139724881797504 executor.py:46] total epoch is 143.
I0626 09:14:07.306873 139724881797504 executor.py:115] TRAIN Batch 143/0 loss 2.689331 loss_att 1.781863 loss_ctc 4.806756 lr 0.00060987 rank 0
I0626 09:14:24.503746 139724881797504 executor.py:115] TRAIN Batch 143/100 loss 6.316164 loss_att 4.874007 loss_ctc 9.681195 lr 0.00060973 rank 0
I0626 09:14:41.886362 139724881797504 executor.py:115] TRAIN Batch 143/200 loss 4.748082 loss_att 3.029614 loss_ctc 8.757841 lr 0.00060960 rank 0
I0626 09:14:59.314106 139724881797504 executor.py:115] TRAIN Batch 143/300 loss 4.369832 loss_att 2.587857 loss_ctc 8.527772 lr 0.00060946 rank 0
I0626 09:15:17.489259 139724881797504 executor.py:115] TRAIN Batch 143/400 loss 4.445524 loss_att 2.621820 loss_ctc 8.700832 lr 0.00060930 rank 0
I0626 09:15:36.036824 139724881797504 executor.py:115] TRAIN Batch 143/500 loss 3.355642 loss_att 2.196642 loss_ctc 6.059976 lr 0.00060917 rank 0
I0626 09:15:52.519624 139724881797504 executor.py:115] TRAIN Batch 143/600 loss 4.641317 loss_att 3.775520 loss_ctc 6.661509 lr 0.00060903 rank 0
I0626 09:16:10.322558 139724881797504 executor.py:115] TRAIN Batch 143/700 loss 3.033879 loss_att 2.562639 loss_ctc 4.133438 lr 0.00060890 rank 0
I0626 09:16:28.494108 139724881797504 executor.py:115] TRAIN Batch 143/800 loss 3.364447 loss_att 2.570423 loss_ctc 5.217169 lr 0.00060874 rank 0
I0626 09:16:47.067406 139724881797504 executor.py:115] TRAIN Batch 143/900 loss 5.386243 loss_att 3.516950 loss_ctc 9.747925 lr 0.00060860 rank 0
I0626 09:17:05.288518 139724881797504 executor.py:115] TRAIN Batch 143/1000 loss 2.410312 loss_att 1.668912 loss_ctc 4.140245 lr 0.00060847 rank 0
I0626 09:17:23.381129 139724881797504 executor.py:115] TRAIN Batch 143/1100 loss 4.891819 loss_att 3.304471 loss_ctc 8.595631 lr 0.00060833 rank 0
I0626 09:17:41.864151 139724881797504 executor.py:115] TRAIN Batch 143/1200 loss 4.879982 loss_att 3.390407 loss_ctc 8.355659 lr 0.00060818 rank 0
I0626 09:17:59.896903 139724881797504 executor.py:115] TRAIN Batch 143/1300 loss 5.376993 loss_att 3.214102 loss_ctc 10.423738 lr 0.00060804 rank 0
I0626 09:18:17.614444 139724881797504 executor.py:115] TRAIN Batch 143/1400 loss 4.354726 loss_att 2.345062 loss_ctc 9.043943 lr 0.00060791 rank 0
I0626 09:18:35.155748 139724881797504 executor.py:115] TRAIN Batch 143/1500 loss 2.911209 loss_att 1.970659 loss_ctc 5.105824 lr 0.00060777 rank 0
I0626 09:18:40.792110 139724881797504 executor.py:152] CV Batch 143/0 loss 15.722410 loss_att 13.805311 loss_ctc 20.195642 history loss 13.975476 rank 0
I0626 09:18:50.071058 139724881797504 executor.py:152] CV Batch 143/100 loss 39.605839 loss_att 39.427528 loss_ctc 40.021893 history loss 27.210153 rank 0
I0626 09:18:57.167962 139724881797504 train.py:288] Epoch 143 CV info cv_loss 31.5829001704214
I0626 09:18:57.168141 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/143.pt
I0626 09:18:57.442007 139724881797504 train.py:274] Epoch 144 TRAIN info lr 0.0006077707979078939
I0626 09:18:57.444959 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:18:57.445028 139724881797504 executor.py:46] total epoch is 144.
I0626 09:19:05.473466 139724881797504 executor.py:115] TRAIN Batch 144/0 loss 2.309571 loss_att 1.500344 loss_ctc 4.197769 lr 0.00060775 rank 0
I0626 09:19:22.311720 139724881797504 executor.py:115] TRAIN Batch 144/100 loss 5.879186 loss_att 4.215903 loss_ctc 9.760178 lr 0.00060761 rank 0
I0626 09:19:40.583920 139724881797504 executor.py:115] TRAIN Batch 144/200 loss 3.609672 loss_att 2.407412 loss_ctc 6.414945 lr 0.00060748 rank 0
I0626 09:19:59.373620 139724881797504 executor.py:115] TRAIN Batch 144/300 loss 6.040677 loss_att 3.559753 loss_ctc 11.829498 lr 0.00060734 rank 0
I0626 09:20:17.920120 139724881797504 executor.py:115] TRAIN Batch 144/400 loss 2.080698 loss_att 1.860026 loss_ctc 2.595599 lr 0.00060719 rank 0
I0626 09:20:37.114396 139724881797504 executor.py:115] TRAIN Batch 144/500 loss 1.917691 loss_att 1.318551 loss_ctc 3.315685 lr 0.00060705 rank 0
I0626 09:20:54.393905 139724881797504 executor.py:115] TRAIN Batch 144/600 loss 9.387674 loss_att 6.350608 loss_ctc 16.474163 lr 0.00060692 rank 0
I0626 09:21:12.502309 139724881797504 executor.py:115] TRAIN Batch 144/700 loss 3.582160 loss_att 2.471114 loss_ctc 6.174602 lr 0.00060679 rank 0
I0626 09:21:30.683805 139724881797504 executor.py:115] TRAIN Batch 144/800 loss 5.904496 loss_att 3.962034 loss_ctc 10.436906 lr 0.00060663 rank 0
I0626 09:21:49.539730 139724881797504 executor.py:115] TRAIN Batch 144/900 loss 5.163164 loss_att 3.142385 loss_ctc 9.878315 lr 0.00060650 rank 0
I0626 09:22:07.446540 139724881797504 executor.py:115] TRAIN Batch 144/1000 loss 2.308183 loss_att 1.546050 loss_ctc 4.086493 lr 0.00060636 rank 0
I0626 09:22:24.918714 139724881797504 executor.py:115] TRAIN Batch 144/1100 loss 4.663960 loss_att 3.639564 loss_ctc 7.054215 lr 0.00060623 rank 0
I0626 09:22:42.815840 139724881797504 executor.py:115] TRAIN Batch 144/1200 loss 4.201841 loss_att 2.579995 loss_ctc 7.986148 lr 0.00060607 rank 0
I0626 09:23:00.831656 139724881797504 executor.py:115] TRAIN Batch 144/1300 loss 3.354430 loss_att 2.415528 loss_ctc 5.545199 lr 0.00060594 rank 0
I0626 09:23:18.657829 139724881797504 executor.py:115] TRAIN Batch 144/1400 loss 3.467658 loss_att 2.549428 loss_ctc 5.610193 lr 0.00060580 rank 0
I0626 09:23:36.225836 139724881797504 executor.py:115] TRAIN Batch 144/1500 loss 3.770706 loss_att 3.109750 loss_ctc 5.312936 lr 0.00060567 rank 0
I0626 09:23:42.011808 139724881797504 executor.py:152] CV Batch 144/0 loss 16.663906 loss_att 15.210746 loss_ctc 20.054615 history loss 14.812361 rank 0
I0626 09:23:51.456865 139724881797504 executor.py:152] CV Batch 144/100 loss 39.483028 loss_att 39.741333 loss_ctc 38.880318 history loss 27.472463 rank 0
I0626 09:23:58.550447 139724881797504 train.py:288] Epoch 144 CV info cv_loss 31.757772979416885
I0626 09:23:58.550598 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/144.pt
I0626 09:23:58.823151 139724881797504 train.py:274] Epoch 145 TRAIN info lr 0.0006056714106402177
I0626 09:23:58.826071 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:23:58.826130 139724881797504 executor.py:46] total epoch is 145.
I0626 09:24:07.069400 139724881797504 executor.py:115] TRAIN Batch 145/0 loss 2.048630 loss_att 1.465723 loss_ctc 3.408745 lr 0.00060565 rank 0
I0626 09:24:24.859943 139724881797504 executor.py:115] TRAIN Batch 145/100 loss 4.577990 loss_att 3.236024 loss_ctc 7.709242 lr 0.00060552 rank 0
I0626 09:24:42.639661 139724881797504 executor.py:115] TRAIN Batch 145/200 loss 4.235662 loss_att 2.954311 loss_ctc 7.225480 lr 0.00060538 rank 0
I0626 09:25:00.494097 139724881797504 executor.py:115] TRAIN Batch 145/300 loss 3.623675 loss_att 2.667316 loss_ctc 5.855178 lr 0.00060525 rank 0
I0626 09:25:18.384141 139724881797504 executor.py:115] TRAIN Batch 145/400 loss 3.724841 loss_att 2.190883 loss_ctc 7.304075 lr 0.00060509 rank 0
I0626 09:25:36.774837 139724881797504 executor.py:115] TRAIN Batch 145/500 loss 3.960307 loss_att 2.764820 loss_ctc 6.749778 lr 0.00060496 rank 0
I0626 09:25:53.680655 139724881797504 executor.py:115] TRAIN Batch 145/600 loss 5.021841 loss_att 4.199997 loss_ctc 6.939476 lr 0.00060483 rank 0
I0626 09:26:12.265080 139724881797504 executor.py:115] TRAIN Batch 145/700 loss 3.301554 loss_att 2.855022 loss_ctc 4.343462 lr 0.00060470 rank 0
I0626 09:26:30.971377 139724881797504 executor.py:115] TRAIN Batch 145/800 loss 5.784410 loss_att 3.848352 loss_ctc 10.301880 lr 0.00060454 rank 0
I0626 09:26:49.171038 139724881797504 executor.py:115] TRAIN Batch 145/900 loss 4.771411 loss_att 3.280846 loss_ctc 8.249395 lr 0.00060441 rank 0
I0626 09:27:07.471233 139724881797504 executor.py:115] TRAIN Batch 145/1000 loss 2.515627 loss_att 1.921678 loss_ctc 3.901507 lr 0.00060428 rank 0
I0626 09:27:25.531277 139724881797504 executor.py:115] TRAIN Batch 145/1100 loss 4.557573 loss_att 3.422605 loss_ctc 7.205833 lr 0.00060414 rank 0
I0626 09:27:44.014550 139724881797504 executor.py:115] TRAIN Batch 145/1200 loss 8.542411 loss_att 4.936945 loss_ctc 16.955166 lr 0.00060399 rank 0
I0626 09:28:01.521389 139724881797504 executor.py:115] TRAIN Batch 145/1300 loss 5.071940 loss_att 3.779663 loss_ctc 8.087254 lr 0.00060386 rank 0
I0626 09:28:19.406341 139724881797504 executor.py:115] TRAIN Batch 145/1400 loss 4.408769 loss_att 3.095856 loss_ctc 7.472230 lr 0.00060373 rank 0
I0626 09:28:37.804697 139724881797504 executor.py:115] TRAIN Batch 145/1500 loss 3.625226 loss_att 2.651429 loss_ctc 5.897418 lr 0.00060359 rank 0
I0626 09:28:43.491395 139724881797504 executor.py:152] CV Batch 145/0 loss 17.248981 loss_att 15.357484 loss_ctc 21.662474 history loss 15.332428 rank 0
I0626 09:28:53.317831 139724881797504 executor.py:152] CV Batch 145/100 loss 38.033306 loss_att 38.341423 loss_ctc 37.314369 history loss 27.466776 rank 0
I0626 09:29:00.312359 139724881797504 train.py:288] Epoch 145 CV info cv_loss 31.91173018397606
I0626 09:29:00.312592 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/145.pt
I0626 09:29:00.622385 139724881797504 train.py:274] Epoch 146 TRAIN info lr 0.0006035936295229593
I0626 09:29:00.625315 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:29:00.625388 139724881797504 executor.py:46] total epoch is 146.
I0626 09:29:09.144021 139724881797504 executor.py:115] TRAIN Batch 146/0 loss 1.733853 loss_att 1.321429 loss_ctc 2.696177 lr 0.00060357 rank 0
I0626 09:29:27.221806 139724881797504 executor.py:115] TRAIN Batch 146/100 loss 5.830175 loss_att 4.100800 loss_ctc 9.865384 lr 0.00060344 rank 0
I0626 09:29:44.786737 139724881797504 executor.py:115] TRAIN Batch 146/200 loss 5.503227 loss_att 3.927024 loss_ctc 9.181032 lr 0.00060331 rank 0
I0626 09:30:03.001890 139724881797504 executor.py:115] TRAIN Batch 146/300 loss 5.464685 loss_att 3.208281 loss_ctc 10.729629 lr 0.00060318 rank 0
I0626 09:30:21.565932 139724881797504 executor.py:115] TRAIN Batch 146/400 loss 5.753912 loss_att 3.597914 loss_ctc 10.784575 lr 0.00060302 rank 0
I0626 09:30:40.526965 139724881797504 executor.py:115] TRAIN Batch 146/500 loss 3.939208 loss_att 2.390967 loss_ctc 7.551770 lr 0.00060289 rank 0
I0626 09:30:57.841770 139724881797504 executor.py:115] TRAIN Batch 146/600 loss 6.363631 loss_att 4.543430 loss_ctc 10.610767 lr 0.00060276 rank 0
I0626 09:31:16.334599 139724881797504 executor.py:115] TRAIN Batch 146/700 loss 6.307273 loss_att 4.236893 loss_ctc 11.138159 lr 0.00060263 rank 0
I0626 09:31:35.621082 139724881797504 executor.py:115] TRAIN Batch 146/800 loss 2.883133 loss_att 2.396545 loss_ctc 4.018504 lr 0.00060248 rank 0
I0626 09:31:54.503548 139724881797504 executor.py:115] TRAIN Batch 146/900 loss 2.764702 loss_att 2.085192 loss_ctc 4.350224 lr 0.00060234 rank 0
I0626 09:32:12.950661 139724881797504 executor.py:115] TRAIN Batch 146/1000 loss 3.499763 loss_att 2.271092 loss_ctc 6.366663 lr 0.00060221 rank 0
I0626 09:32:30.840864 139724881797504 executor.py:115] TRAIN Batch 146/1100 loss 5.877087 loss_att 4.278198 loss_ctc 9.607826 lr 0.00060208 rank 0
I0626 09:32:49.228134 139724881797504 executor.py:115] TRAIN Batch 146/1200 loss 6.696401 loss_att 3.943368 loss_ctc 13.120142 lr 0.00060193 rank 0
I0626 09:33:07.483059 139724881797504 executor.py:115] TRAIN Batch 146/1300 loss 4.423223 loss_att 2.767479 loss_ctc 8.286624 lr 0.00060180 rank 0
I0626 09:33:25.743293 139724881797504 executor.py:115] TRAIN Batch 146/1400 loss 2.731418 loss_att 1.915619 loss_ctc 4.634948 lr 0.00060167 rank 0
I0626 09:33:43.250223 139724881797504 executor.py:115] TRAIN Batch 146/1500 loss 6.453900 loss_att 4.589216 loss_ctc 10.804829 lr 0.00060154 rank 0
I0626 09:33:48.971780 139724881797504 executor.py:152] CV Batch 146/0 loss 17.510941 loss_att 15.203377 loss_ctc 22.895252 history loss 15.565280 rank 0
I0626 09:33:58.483815 139724881797504 executor.py:152] CV Batch 146/100 loss 38.460052 loss_att 38.300545 loss_ctc 38.832230 history loss 27.470570 rank 0
I0626 09:34:05.490423 139724881797504 train.py:288] Epoch 146 CV info cv_loss 31.649536180729143
I0626 09:34:05.490568 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/146.pt
I0626 09:34:05.770685 139724881797504 train.py:274] Epoch 147 TRAIN info lr 0.0006015370864760849
I0626 09:34:05.773570 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:34:05.773629 139724881797504 executor.py:46] total epoch is 147.
I0626 09:34:13.951651 139724881797504 executor.py:115] TRAIN Batch 147/0 loss 3.223588 loss_att 2.614189 loss_ctc 4.645518 lr 0.00060152 rank 0
I0626 09:34:31.271787 139724881797504 executor.py:115] TRAIN Batch 147/100 loss 4.882392 loss_att 3.268347 loss_ctc 8.648497 lr 0.00060138 rank 0
I0626 09:34:48.777047 139724881797504 executor.py:115] TRAIN Batch 147/200 loss 3.287635 loss_att 2.095520 loss_ctc 6.069238 lr 0.00060125 rank 0
I0626 09:35:06.993486 139724881797504 executor.py:115] TRAIN Batch 147/300 loss 4.307896 loss_att 2.859623 loss_ctc 7.687200 lr 0.00060112 rank 0
I0626 09:35:25.462925 139724881797504 executor.py:115] TRAIN Batch 147/400 loss 5.179517 loss_att 3.205490 loss_ctc 9.785582 lr 0.00060097 rank 0
I0626 09:35:44.138113 139724881797504 executor.py:115] TRAIN Batch 147/500 loss 2.125061 loss_att 1.617168 loss_ctc 3.310146 lr 0.00060084 rank 0
I0626 09:36:01.652835 139724881797504 executor.py:115] TRAIN Batch 147/600 loss 5.878737 loss_att 4.448030 loss_ctc 9.217055 lr 0.00060071 rank 0
I0626 09:36:19.641824 139724881797504 executor.py:115] TRAIN Batch 147/700 loss 4.379548 loss_att 2.794245 loss_ctc 8.078588 lr 0.00060058 rank 0
I0626 09:36:38.394720 139724881797504 executor.py:115] TRAIN Batch 147/800 loss 5.619323 loss_att 3.882393 loss_ctc 9.672157 lr 0.00060043 rank 0
I0626 09:36:56.273602 139724881797504 executor.py:115] TRAIN Batch 147/900 loss 3.977955 loss_att 2.332085 loss_ctc 7.818319 lr 0.00060030 rank 0
I0626 09:37:14.028352 139724881797504 executor.py:115] TRAIN Batch 147/1000 loss 1.498804 loss_att 1.208035 loss_ctc 2.177267 lr 0.00060017 rank 0
I0626 09:37:31.844541 139724881797504 executor.py:115] TRAIN Batch 147/1100 loss 9.502815 loss_att 6.634033 loss_ctc 16.196642 lr 0.00060004 rank 0
I0626 09:37:51.182780 139724881797504 executor.py:115] TRAIN Batch 147/1200 loss 4.521737 loss_att 3.764302 loss_ctc 6.289086 lr 0.00059989 rank 0
I0626 09:38:10.655240 139724881797504 executor.py:115] TRAIN Batch 147/1300 loss 2.801348 loss_att 2.028389 loss_ctc 4.604917 lr 0.00059976 rank 0
I0626 09:38:29.527727 139724881797504 executor.py:115] TRAIN Batch 147/1400 loss 3.248374 loss_att 2.374537 loss_ctc 5.287325 lr 0.00059963 rank 0
I0626 09:38:48.403352 139724881797504 executor.py:115] TRAIN Batch 147/1500 loss 4.502363 loss_att 3.191797 loss_ctc 7.560351 lr 0.00059950 rank 0
I0626 09:38:54.532049 139724881797504 executor.py:152] CV Batch 147/0 loss 17.016243 loss_att 15.584311 loss_ctc 20.357414 history loss 15.125549 rank 0
I0626 09:39:04.462739 139724881797504 executor.py:152] CV Batch 147/100 loss 38.366337 loss_att 39.083942 loss_ctc 36.691925 history loss 27.198525 rank 0
I0626 09:39:11.979063 139724881797504 train.py:288] Epoch 147 CV info cv_loss 31.446667023663412
I0626 09:39:11.979218 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/147.pt
I0626 09:39:12.258481 139724881797504 train.py:274] Epoch 148 TRAIN info lr 0.0005995014221389604
I0626 09:39:12.261697 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:39:12.261833 139724881797504 executor.py:46] total epoch is 148.
I0626 09:39:20.652210 139724881797504 executor.py:115] TRAIN Batch 148/0 loss 4.840710 loss_att 2.807055 loss_ctc 9.585903 lr 0.00059948 rank 0
I0626 09:39:39.387980 139724881797504 executor.py:115] TRAIN Batch 148/100 loss 8.421618 loss_att 4.930374 loss_ctc 16.567856 lr 0.00059935 rank 0
I0626 09:39:58.101092 139724881797504 executor.py:115] TRAIN Batch 148/200 loss 5.947764 loss_att 4.408566 loss_ctc 9.539225 lr 0.00059922 rank 0
I0626 09:40:17.425622 139724881797504 executor.py:115] TRAIN Batch 148/300 loss 2.582864 loss_att 2.243572 loss_ctc 3.374544 lr 0.00059909 rank 0
I0626 09:40:36.305858 139724881797504 executor.py:115] TRAIN Batch 148/400 loss 5.467555 loss_att 3.001203 loss_ctc 11.222378 lr 0.00059894 rank 0
I0626 09:40:55.625289 139724881797504 executor.py:115] TRAIN Batch 148/500 loss 2.841593 loss_att 1.836343 loss_ctc 5.187177 lr 0.00059881 rank 0
I0626 09:41:13.529996 139724881797504 executor.py:115] TRAIN Batch 148/600 loss 7.248894 loss_att 5.253157 loss_ctc 11.905613 lr 0.00059868 rank 0
I0626 09:41:32.357219 139724881797504 executor.py:115] TRAIN Batch 148/700 loss 3.457240 loss_att 2.954320 loss_ctc 4.630721 lr 0.00059856 rank 0
I0626 09:41:51.506016 139724881797504 executor.py:115] TRAIN Batch 148/800 loss 4.859104 loss_att 3.187288 loss_ctc 8.760006 lr 0.00059841 rank 0
I0626 09:42:10.382577 139724881797504 executor.py:115] TRAIN Batch 148/900 loss 4.959170 loss_att 3.304825 loss_ctc 8.819308 lr 0.00059828 rank 0
I0626 09:42:28.686276 139724881797504 executor.py:115] TRAIN Batch 148/1000 loss 2.473827 loss_att 1.290769 loss_ctc 5.234294 lr 0.00059815 rank 0
I0626 09:42:46.211958 139724881797504 executor.py:115] TRAIN Batch 148/1100 loss 5.357510 loss_att 3.411995 loss_ctc 9.897042 lr 0.00059802 rank 0
I0626 09:43:04.895992 139724881797504 executor.py:115] TRAIN Batch 148/1200 loss 5.433399 loss_att 3.332484 loss_ctc 10.335535 lr 0.00059787 rank 0
I0626 09:43:23.017862 139724881797504 executor.py:115] TRAIN Batch 148/1300 loss 4.475016 loss_att 3.127589 loss_ctc 7.619011 lr 0.00059774 rank 0
I0626 09:43:41.275729 139724881797504 executor.py:115] TRAIN Batch 148/1400 loss 3.773953 loss_att 1.938795 loss_ctc 8.055990 lr 0.00059761 rank 0
I0626 09:43:59.203158 139724881797504 executor.py:115] TRAIN Batch 148/1500 loss 3.715904 loss_att 3.477892 loss_ctc 4.271266 lr 0.00059749 rank 0
I0626 09:44:05.081245 139724881797504 executor.py:152] CV Batch 148/0 loss 17.240030 loss_att 14.731129 loss_ctc 23.094137 history loss 15.324471 rank 0
I0626 09:44:14.810691 139724881797504 executor.py:152] CV Batch 148/100 loss 37.874580 loss_att 37.810852 loss_ctc 38.023285 history loss 26.874149 rank 0
I0626 09:44:22.146688 139724881797504 train.py:288] Epoch 148 CV info cv_loss 31.04066885224773
I0626 09:44:22.146927 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/148.pt
I0626 09:44:22.437136 139724881797504 train.py:274] Epoch 149 TRAIN info lr 0.0005974862856065684
I0626 09:44:22.440259 139724881797504 executor.py:34] using accumulate grad, new batch size is 16 times larger than before
I0626 09:44:22.440328 139724881797504 executor.py:46] total epoch is 149.
I0626 09:44:30.750735 139724881797504 executor.py:115] TRAIN Batch 149/0 loss 2.968589 loss_att 1.677775 loss_ctc 5.980488 lr 0.00059746 rank 0
I0626 09:44:48.715390 139724881797504 executor.py:115] TRAIN Batch 149/100 loss 4.399176 loss_att 3.650324 loss_ctc 6.146497 lr 0.00059734 rank 0
I0626 09:45:07.121657 139724881797504 executor.py:115] TRAIN Batch 149/200 loss 8.134101 loss_att 3.635022 loss_ctc 18.631952 lr 0.00059721 rank 0
I0626 09:45:25.588660 139724881797504 executor.py:115] TRAIN Batch 149/300 loss 4.318893 loss_att 3.178518 loss_ctc 6.979770 lr 0.00059708 rank 0
I0626 09:45:43.837782 139724881797504 executor.py:115] TRAIN Batch 149/400 loss 4.792440 loss_att 2.766610 loss_ctc 9.519377 lr 0.00059693 rank 0
I0626 09:46:03.057742 139724881797504 executor.py:115] TRAIN Batch 149/500 loss 2.248580 loss_att 1.225253 loss_ctc 4.636345 lr 0.00059680 rank 0
I0626 09:46:20.980263 139724881797504 executor.py:115] TRAIN Batch 149/600 loss 5.287897 loss_att 4.529794 loss_ctc 7.056805 lr 0.00059668 rank 0
I0626 09:46:39.897769 139724881797504 executor.py:115] TRAIN Batch 149/700 loss 4.624355 loss_att 2.853074 loss_ctc 8.757343 lr 0.00059655 rank 0
I0626 09:46:58.489038 139724881797504 executor.py:115] TRAIN Batch 149/800 loss 7.038855 loss_att 3.996092 loss_ctc 14.138635 lr 0.00059640 rank 0
I0626 09:47:17.192554 139724881797504 executor.py:115] TRAIN Batch 149/900 loss 5.467155 loss_att 2.952005 loss_ctc 11.335840 lr 0.00059627 rank 0
I0626 09:47:35.953687 139724881797504 executor.py:115] TRAIN Batch 149/1000 loss 3.123257 loss_att 2.405004 loss_ctc 4.799181 lr 0.00059615 rank 0
I0626 09:47:53.997716 139724881797504 executor.py:115] TRAIN Batch 149/1100 loss 3.974336 loss_att 3.530732 loss_ctc 5.009412 lr 0.00059602 rank 0
I0626 09:48:12.356526 139724881797504 executor.py:115] TRAIN Batch 149/1200 loss 3.963891 loss_att 3.186028 loss_ctc 5.778903 lr 0.00059587 rank 0
I0626 09:48:30.360997 139724881797504 executor.py:115] TRAIN Batch 149/1300 loss 4.903199 loss_att 3.407324 loss_ctc 8.393574 lr 0.00059574 rank 0
I0626 09:48:48.623506 139724881797504 executor.py:115] TRAIN Batch 149/1400 loss 5.527780 loss_att 3.765040 loss_ctc 9.640840 lr 0.00059562 rank 0
I0626 09:49:06.881530 139724881797504 executor.py:115] TRAIN Batch 149/1500 loss 3.971694 loss_att 1.997431 loss_ctc 8.578310 lr 0.00059549 rank 0
I0626 09:49:12.660047 139724881797504 executor.py:152] CV Batch 149/0 loss 18.381626 loss_att 16.021664 loss_ctc 23.888206 history loss 16.339223 rank 0
I0626 09:49:22.474807 139724881797504 executor.py:152] CV Batch 149/100 loss 39.277946 loss_att 38.830795 loss_ctc 40.321304 history loss 27.367579 rank 0
I0626 09:49:30.032795 139724881797504 train.py:288] Epoch 149 CV info cv_loss 31.75881217961867
I0626 09:49:30.033020 139724881797504 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/149.pt

