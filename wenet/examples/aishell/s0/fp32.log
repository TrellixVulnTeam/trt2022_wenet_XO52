I0623 22:06:09.441769 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/init.pt
I0623 22:06:09.756594 139680259150208 train.py:268] Epoch 0 TRAIN info lr 2e-07
I0623 22:06:09.758314 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:06:09.758437 139680259150208 executor.py:45] total epoch is 0.
I0623 22:06:19.062442 139680259150208 executor.py:114] TRAIN Batch 0/0 loss 154.547089 loss_att 53.018173 loss_ctc 391.447876 lr 0.00000020 rank 0
I0623 22:06:29.409864 139680259150208 executor.py:114] TRAIN Batch 0/100 loss 333.560608 loss_att 117.518654 loss_ctc 837.658508 lr 0.00000140 rank 0
I0623 22:06:37.794809 139680259150208 executor.py:114] TRAIN Batch 0/200 loss 238.053680 loss_att 85.700470 loss_ctc 593.544495 lr 0.00000260 rank 0
I0623 22:06:49.541392 139680259150208 executor.py:114] TRAIN Batch 0/300 loss 398.299255 loss_att 133.586243 loss_ctc 1015.962891 lr 0.00000380 rank 0
I0623 22:06:57.827887 139680259150208 executor.py:114] TRAIN Batch 0/400 loss 245.026428 loss_att 97.915955 loss_ctc 588.284180 lr 0.00000520 rank 0
I0623 22:07:09.725226 139680259150208 executor.py:114] TRAIN Batch 0/500 loss 111.805313 loss_att 51.593941 loss_ctc 252.298508 lr 0.00000640 rank 0
I0623 22:07:17.477963 139680259150208 executor.py:114] TRAIN Batch 0/600 loss 215.827408 loss_att 124.707687 loss_ctc 428.440063 lr 0.00000760 rank 0
I0623 22:07:29.482797 139680259150208 executor.py:114] TRAIN Batch 0/700 loss 124.532478 loss_att 88.514145 loss_ctc 208.575256 lr 0.00000880 rank 0
I0623 22:07:40.327460 139680259150208 executor.py:151] CV Batch 0/0 loss 67.995300 loss_att 53.014568 loss_ctc 102.950348 history loss 63.995577 rank 0
I0623 22:07:43.703352 139680259150208 train.py:275] Epoch 0 CV info cv_loss 138.58200433149423
I0623 22:07:43.703565 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/0.pt
I0623 22:07:44.025310 139680259150208 train.py:268] Epoch 1 TRAIN info lr 9.4e-06
I0623 22:07:44.027388 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:07:44.027543 139680259150208 executor.py:45] total epoch is 1.
I0623 22:07:52.201303 139680259150208 executor.py:114] TRAIN Batch 1/0 loss 69.562126 loss_att 53.824177 loss_ctc 106.284019 lr 0.00000960 rank 0
I0623 22:08:03.874166 139680259150208 executor.py:114] TRAIN Batch 1/100 loss 135.380127 loss_att 114.922249 loss_ctc 183.115189 lr 0.00001080 rank 0
I0623 22:08:12.627501 139680259150208 executor.py:114] TRAIN Batch 1/200 loss 91.174240 loss_att 82.763550 loss_ctc 110.799187 lr 0.00001200 rank 0
I0623 22:08:23.872677 139680259150208 executor.py:114] TRAIN Batch 1/300 loss 148.752014 loss_att 136.699677 loss_ctc 176.874115 lr 0.00001320 rank 0
I0623 22:08:32.174505 139680259150208 executor.py:114] TRAIN Batch 1/400 loss 108.272194 loss_att 101.477455 loss_ctc 124.126587 lr 0.00001460 rank 0
I0623 22:08:43.893617 139680259150208 executor.py:114] TRAIN Batch 1/500 loss 57.262943 loss_att 54.877998 loss_ctc 62.827820 lr 0.00001580 rank 0
I0623 22:08:51.621897 139680259150208 executor.py:114] TRAIN Batch 1/600 loss 128.898865 loss_att 123.007217 loss_ctc 142.646072 lr 0.00001700 rank 0
I0623 22:09:03.567770 139680259150208 executor.py:114] TRAIN Batch 1/700 loss 82.485062 loss_att 79.259735 loss_ctc 90.010818 lr 0.00001820 rank 0
I0623 22:09:13.690430 139680259150208 executor.py:151] CV Batch 1/0 loss 53.839451 loss_att 52.262161 loss_ctc 57.519794 history loss 50.672424 rank 0
I0623 22:09:17.027560 139680259150208 train.py:275] Epoch 1 CV info cv_loss 111.54840036779538
I0623 22:09:17.027726 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/1.pt
I0623 22:09:17.326424 139680259150208 train.py:268] Epoch 2 TRAIN info lr 1.88e-05
I0623 22:09:17.328363 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:09:17.328494 139680259150208 executor.py:45] total epoch is 2.
I0623 22:09:25.702761 139680259150208 executor.py:114] TRAIN Batch 2/0 loss 59.838825 loss_att 57.746315 loss_ctc 64.721344 lr 0.00001900 rank 0
I0623 22:09:37.484721 139680259150208 executor.py:114] TRAIN Batch 2/100 loss 114.408844 loss_att 109.503265 loss_ctc 125.855194 lr 0.00002020 rank 0
I0623 22:09:45.951464 139680259150208 executor.py:114] TRAIN Batch 2/200 loss 82.749046 loss_att 80.155655 loss_ctc 88.800278 lr 0.00002140 rank 0
I0623 22:09:57.628747 139680259150208 executor.py:114] TRAIN Batch 2/300 loss 145.478195 loss_att 139.643219 loss_ctc 159.093140 lr 0.00002260 rank 0
I0623 22:10:05.852544 139680259150208 executor.py:114] TRAIN Batch 2/400 loss 96.892738 loss_att 93.864143 loss_ctc 103.959457 lr 0.00002400 rank 0
I0623 22:10:18.393569 139680259150208 executor.py:114] TRAIN Batch 2/500 loss 60.161835 loss_att 59.121090 loss_ctc 62.590248 lr 0.00002520 rank 0
I0623 22:10:26.462018 139680259150208 executor.py:114] TRAIN Batch 2/600 loss 116.608078 loss_att 113.248444 loss_ctc 124.447212 lr 0.00002640 rank 0
I0623 22:10:38.734276 139680259150208 executor.py:114] TRAIN Batch 2/700 loss 83.194145 loss_att 81.134796 loss_ctc 87.999283 lr 0.00002760 rank 0
I0623 22:10:49.571577 139680259150208 executor.py:151] CV Batch 2/0 loss 50.401848 loss_att 49.720623 loss_ctc 51.991375 history loss 47.437033 rank 0
I0623 22:10:52.968758 139680259150208 train.py:275] Epoch 2 CV info cv_loss 105.67694266426422
I0623 22:10:52.968888 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/2.pt
I0623 22:10:53.256582 139680259150208 train.py:268] Epoch 3 TRAIN info lr 2.8199999999999998e-05
I0623 22:10:53.258708 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:10:53.258844 139680259150208 executor.py:45] total epoch is 3.
I0623 22:11:01.824395 139680259150208 executor.py:114] TRAIN Batch 3/0 loss 50.198181 loss_att 49.314682 loss_ctc 52.259682 lr 0.00002840 rank 0
I0623 22:11:14.032656 139680259150208 executor.py:114] TRAIN Batch 3/100 loss 112.673325 loss_att 109.670517 loss_ctc 119.679855 lr 0.00002960 rank 0
I0623 22:11:23.006597 139680259150208 executor.py:114] TRAIN Batch 3/200 loss 82.259109 loss_att 80.025787 loss_ctc 87.470192 lr 0.00003080 rank 0
I0623 22:11:34.980191 139680259150208 executor.py:114] TRAIN Batch 3/300 loss 128.506378 loss_att 124.630562 loss_ctc 137.549973 lr 0.00003200 rank 0
I0623 22:11:43.807106 139680259150208 executor.py:114] TRAIN Batch 3/400 loss 88.317993 loss_att 85.316193 loss_ctc 95.322189 lr 0.00003340 rank 0
I0623 22:11:56.217972 139680259150208 executor.py:114] TRAIN Batch 3/500 loss 48.752277 loss_att 47.239399 loss_ctc 52.282333 lr 0.00003460 rank 0
I0623 22:12:04.467262 139680259150208 executor.py:114] TRAIN Batch 3/600 loss 101.928101 loss_att 98.239761 loss_ctc 110.534225 lr 0.00003580 rank 0
I0623 22:12:16.555352 139680259150208 executor.py:114] TRAIN Batch 3/700 loss 72.144737 loss_att 69.208328 loss_ctc 78.996368 lr 0.00003700 rank 0
I0623 22:12:27.386949 139680259150208 executor.py:151] CV Batch 3/0 loss 44.504253 loss_att 43.022041 loss_ctc 47.962742 history loss 41.886356 rank 0
I0623 22:12:30.845857 139680259150208 train.py:275] Epoch 3 CV info cv_loss 95.59635274691425
I0623 22:12:30.845993 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/3.pt
I0623 22:12:31.160861 139680259150208 train.py:268] Epoch 4 TRAIN info lr 3.76e-05
I0623 22:12:31.162902 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:12:31.163035 139680259150208 executor.py:45] total epoch is 4.
I0623 22:12:39.666051 139680259150208 executor.py:114] TRAIN Batch 4/0 loss 48.201050 loss_att 46.388073 loss_ctc 52.431328 lr 0.00003780 rank 0
I0623 22:12:51.795503 139680259150208 executor.py:114] TRAIN Batch 4/100 loss 102.640129 loss_att 98.553406 loss_ctc 112.175819 lr 0.00003900 rank 0
I0623 22:13:00.933574 139680259150208 executor.py:114] TRAIN Batch 4/200 loss 69.754349 loss_att 66.820892 loss_ctc 76.599083 lr 0.00004020 rank 0
I0623 22:13:12.380095 139680259150208 executor.py:114] TRAIN Batch 4/300 loss 115.040634 loss_att 110.245972 loss_ctc 126.228157 lr 0.00004140 rank 0
I0623 22:13:21.192786 139680259150208 executor.py:114] TRAIN Batch 4/400 loss 82.912109 loss_att 79.424622 loss_ctc 91.049591 lr 0.00004280 rank 0
I0623 22:13:33.520766 139680259150208 executor.py:114] TRAIN Batch 4/500 loss 41.943882 loss_att 40.294655 loss_ctc 45.792076 lr 0.00004400 rank 0
I0623 22:13:41.757222 139680259150208 executor.py:114] TRAIN Batch 4/600 loss 91.331398 loss_att 87.396408 loss_ctc 100.513039 lr 0.00004520 rank 0
I0623 22:13:53.219148 139680259150208 executor.py:114] TRAIN Batch 4/700 loss 67.310913 loss_att 64.307480 loss_ctc 74.318909 lr 0.00004640 rank 0
I0623 22:14:03.956530 139680259150208 executor.py:151] CV Batch 4/0 loss 42.417789 loss_att 40.785847 loss_ctc 46.225651 history loss 39.922625 rank 0
I0623 22:14:07.472594 139680259150208 train.py:275] Epoch 4 CV info cv_loss 91.23921641251962
I0623 22:14:07.472726 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/4.pt
I0623 22:14:07.767217 139680259150208 train.py:268] Epoch 5 TRAIN info lr 4.7e-05
I0623 22:14:07.770127 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:14:07.770375 139680259150208 executor.py:45] total epoch is 5.
I0623 22:14:16.337766 139680259150208 executor.py:114] TRAIN Batch 5/0 loss 44.098919 loss_att 42.285851 loss_ctc 48.329411 lr 0.00004720 rank 0
I0623 22:14:28.377157 139680259150208 executor.py:114] TRAIN Batch 5/100 loss 105.647469 loss_att 100.955353 loss_ctc 116.595734 lr 0.00004840 rank 0
I0623 22:14:37.379824 139680259150208 executor.py:114] TRAIN Batch 5/200 loss 67.498055 loss_att 64.612663 loss_ctc 74.230637 lr 0.00004960 rank 0
I0623 22:14:48.750905 139680259150208 executor.py:114] TRAIN Batch 5/300 loss 121.151436 loss_att 115.684982 loss_ctc 133.906494 lr 0.00005080 rank 0
I0623 22:14:57.359339 139680259150208 executor.py:114] TRAIN Batch 5/400 loss 76.199532 loss_att 72.966484 loss_ctc 83.743309 lr 0.00005220 rank 0
I0623 22:15:09.293951 139680259150208 executor.py:114] TRAIN Batch 5/500 loss 45.846870 loss_att 43.973454 loss_ctc 50.218174 lr 0.00005340 rank 0
I0623 22:15:17.354058 139680259150208 executor.py:114] TRAIN Batch 5/600 loss 100.067139 loss_att 95.437637 loss_ctc 110.869308 lr 0.00005460 rank 0
I0623 22:15:28.908480 139680259150208 executor.py:114] TRAIN Batch 5/700 loss 68.070625 loss_att 65.115479 loss_ctc 74.965973 lr 0.00005580 rank 0
I0623 22:15:39.315519 139680259150208 executor.py:151] CV Batch 5/0 loss 42.033577 loss_att 40.374535 loss_ctc 45.904678 history loss 39.561014 rank 0
I0623 22:15:42.661979 139680259150208 train.py:275] Epoch 5 CV info cv_loss 90.34049533117259
I0623 22:15:42.662132 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/5.pt
I0623 22:15:42.964023 139680259150208 train.py:268] Epoch 6 TRAIN info lr 5.6399999999999995e-05
I0623 22:15:42.966073 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:15:42.966210 139680259150208 executor.py:45] total epoch is 6.
I0623 22:15:51.317719 139680259150208 executor.py:114] TRAIN Batch 6/0 loss 37.460350 loss_att 35.844177 loss_ctc 41.231422 lr 0.00005660 rank 0
I0623 22:16:02.980149 139680259150208 executor.py:114] TRAIN Batch 6/100 loss 89.653114 loss_att 85.536873 loss_ctc 99.257675 lr 0.00005780 rank 0
I0623 22:16:11.389512 139680259150208 executor.py:114] TRAIN Batch 6/200 loss 66.507591 loss_att 63.545792 loss_ctc 73.418465 lr 0.00005900 rank 0
I0623 22:16:22.753240 139680259150208 executor.py:114] TRAIN Batch 6/300 loss 114.218719 loss_att 108.959656 loss_ctc 126.489845 lr 0.00006020 rank 0
I0623 22:16:31.192615 139680259150208 executor.py:114] TRAIN Batch 6/400 loss 81.442123 loss_att 77.679733 loss_ctc 90.221039 lr 0.00006160 rank 0
I0623 22:16:42.579593 139680259150208 executor.py:114] TRAIN Batch 6/500 loss 45.879639 loss_att 44.024162 loss_ctc 50.209076 lr 0.00006280 rank 0
I0623 22:16:50.499021 139680259150208 executor.py:114] TRAIN Batch 6/600 loss 94.823669 loss_att 90.521378 loss_ctc 104.862335 lr 0.00006400 rank 0
I0623 22:17:01.632470 139680259150208 executor.py:114] TRAIN Batch 6/700 loss 66.800995 loss_att 63.721382 loss_ctc 73.986755 lr 0.00006520 rank 0
I0623 22:17:11.999662 139680259150208 executor.py:151] CV Batch 6/0 loss 41.793438 loss_att 40.082428 loss_ctc 45.785789 history loss 39.335000 rank 0
I0623 22:17:15.368326 139680259150208 train.py:275] Epoch 6 CV info cv_loss 89.63870509720647
I0623 22:17:15.368555 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/6.pt
I0623 22:17:15.674401 139680259150208 train.py:268] Epoch 7 TRAIN info lr 6.58e-05
I0623 22:17:15.676423 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:17:15.676560 139680259150208 executor.py:45] total epoch is 7.
I0623 22:17:23.956302 139680259150208 executor.py:114] TRAIN Batch 7/0 loss 40.034550 loss_att 38.376240 loss_ctc 43.903942 lr 0.00006600 rank 0
I0623 22:17:35.663986 139680259150208 executor.py:114] TRAIN Batch 7/100 loss 92.989647 loss_att 88.466431 loss_ctc 103.543816 lr 0.00006720 rank 0
I0623 22:17:44.590283 139680259150208 executor.py:114] TRAIN Batch 7/200 loss 65.917046 loss_att 62.804436 loss_ctc 73.179810 lr 0.00006840 rank 0
I0623 22:17:55.532816 139680259150208 executor.py:114] TRAIN Batch 7/300 loss 109.748779 loss_att 104.313248 loss_ctc 122.431702 lr 0.00006960 rank 0
I0623 22:18:04.277597 139680259150208 executor.py:114] TRAIN Batch 7/400 loss 81.730560 loss_att 77.698624 loss_ctc 91.138412 lr 0.00007100 rank 0
I0623 22:18:16.359946 139680259150208 executor.py:114] TRAIN Batch 7/500 loss 45.470924 loss_att 43.402382 loss_ctc 50.297531 lr 0.00007220 rank 0
I0623 22:18:24.617096 139680259150208 executor.py:114] TRAIN Batch 7/600 loss 89.000473 loss_att 84.294594 loss_ctc 99.980865 lr 0.00007340 rank 0
I0623 22:18:35.767532 139680259150208 executor.py:114] TRAIN Batch 7/700 loss 66.380753 loss_att 62.899044 loss_ctc 74.504745 lr 0.00007460 rank 0
I0623 22:18:45.704175 139680259150208 executor.py:151] CV Batch 7/0 loss 41.512352 loss_att 39.709259 loss_ctc 45.719566 history loss 39.070449 rank 0
I0623 22:18:48.957447 139680259150208 train.py:275] Epoch 7 CV info cv_loss 88.76165975928723
I0623 22:18:48.957597 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/7.pt
I0623 22:18:49.239137 139680259150208 train.py:268] Epoch 8 TRAIN info lr 7.52e-05
I0623 22:18:49.241173 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:18:49.241348 139680259150208 executor.py:45] total epoch is 8.
I0623 22:18:57.211477 139680259150208 executor.py:114] TRAIN Batch 8/0 loss 47.403671 loss_att 45.312908 loss_ctc 52.282112 lr 0.00007540 rank 0
I0623 22:19:08.481039 139680259150208 executor.py:114] TRAIN Batch 8/100 loss 105.421745 loss_att 99.432816 loss_ctc 119.395920 lr 0.00007660 rank 0
I0623 22:19:17.221132 139680259150208 executor.py:114] TRAIN Batch 8/200 loss 65.763092 loss_att 62.035297 loss_ctc 74.461273 lr 0.00007780 rank 0
I0623 22:19:28.503262 139680259150208 executor.py:114] TRAIN Batch 8/300 loss 110.524406 loss_att 104.773750 loss_ctc 123.942612 lr 0.00007900 rank 0
I0623 22:19:36.919195 139680259150208 executor.py:114] TRAIN Batch 8/400 loss 77.062454 loss_att 73.070190 loss_ctc 86.377731 lr 0.00008040 rank 0
I0623 22:19:48.920557 139680259150208 executor.py:114] TRAIN Batch 8/500 loss 40.184479 loss_att 38.221786 loss_ctc 44.764091 lr 0.00008160 rank 0
I0623 22:19:57.371742 139680259150208 executor.py:114] TRAIN Batch 8/600 loss 99.200821 loss_att 93.592850 loss_ctc 112.286079 lr 0.00008280 rank 0
I0623 22:20:08.791518 139680259150208 executor.py:114] TRAIN Batch 8/700 loss 65.449051 loss_att 61.547539 loss_ctc 74.552597 lr 0.00008400 rank 0
I0623 22:20:19.277041 139680259150208 executor.py:151] CV Batch 8/0 loss 41.181465 loss_att 39.333893 loss_ctc 45.492462 history loss 38.759026 rank 0
I0623 22:20:22.674584 139680259150208 train.py:275] Epoch 8 CV info cv_loss 87.6592091615601
I0623 22:20:22.674844 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/8.pt
I0623 22:20:22.969433 139680259150208 train.py:268] Epoch 9 TRAIN info lr 8.459999999999998e-05
I0623 22:20:22.971351 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:20:22.971489 139680259150208 executor.py:45] total epoch is 9.
I0623 22:20:31.135495 139680259150208 executor.py:114] TRAIN Batch 9/0 loss 41.580151 loss_att 39.467655 loss_ctc 46.509308 lr 0.00008480 rank 0
I0623 22:20:42.912482 139680259150208 executor.py:114] TRAIN Batch 9/100 loss 94.958305 loss_att 89.525421 loss_ctc 107.635033 lr 0.00008600 rank 0
I0623 22:20:51.774727 139680259150208 executor.py:114] TRAIN Batch 9/200 loss 67.183655 loss_att 63.104370 loss_ctc 76.701973 lr 0.00008720 rank 0
I0623 22:21:03.080298 139680259150208 executor.py:114] TRAIN Batch 9/300 loss 104.276550 loss_att 97.694641 loss_ctc 119.634346 lr 0.00008840 rank 0
I0623 22:21:11.686836 139680259150208 executor.py:114] TRAIN Batch 9/400 loss 75.821136 loss_att 71.825539 loss_ctc 85.144188 lr 0.00008980 rank 0
I0623 22:21:23.988866 139680259150208 executor.py:114] TRAIN Batch 9/500 loss 41.662842 loss_att 39.734375 loss_ctc 46.162594 lr 0.00009100 rank 0
I0623 22:21:32.025950 139680259150208 executor.py:114] TRAIN Batch 9/600 loss 100.568344 loss_att 94.458832 loss_ctc 114.823868 lr 0.00009220 rank 0
I0623 22:21:43.859141 139680259150208 executor.py:114] TRAIN Batch 9/700 loss 68.341225 loss_att 64.107704 loss_ctc 78.219437 lr 0.00009340 rank 0
I0623 22:21:54.230634 139680259150208 executor.py:151] CV Batch 9/0 loss 40.947266 loss_att 39.132645 loss_ctc 45.181374 history loss 38.538603 rank 0
I0623 22:21:57.591474 139680259150208 train.py:275] Epoch 9 CV info cv_loss 86.56460816196353
I0623 22:21:57.591725 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/9.pt
I0623 22:21:57.887526 139680259150208 train.py:268] Epoch 10 TRAIN info lr 9.4e-05
I0623 22:21:57.889311 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:21:57.889425 139680259150208 executor.py:45] total epoch is 10.
I0623 22:22:06.227171 139680259150208 executor.py:114] TRAIN Batch 10/0 loss 45.227455 loss_att 42.721581 loss_ctc 51.074501 lr 0.00009420 rank 0
I0623 22:22:18.179189 139680259150208 executor.py:114] TRAIN Batch 10/100 loss 90.755325 loss_att 84.861328 loss_ctc 104.507980 lr 0.00009540 rank 0
I0623 22:22:27.152294 139680259150208 executor.py:114] TRAIN Batch 10/200 loss 65.634552 loss_att 61.448265 loss_ctc 75.402557 lr 0.00009660 rank 0
I0623 22:22:38.909712 139680259150208 executor.py:114] TRAIN Batch 10/300 loss 105.073059 loss_att 96.743584 loss_ctc 124.508522 lr 0.00009780 rank 0
I0623 22:22:47.523680 139680259150208 executor.py:114] TRAIN Batch 10/400 loss 76.769577 loss_att 71.667175 loss_ctc 88.675186 lr 0.00009920 rank 0
I0623 22:22:59.840599 139680259150208 executor.py:114] TRAIN Batch 10/500 loss 45.333660 loss_att 42.772507 loss_ctc 51.309681 lr 0.00010040 rank 0
I0623 22:23:07.973782 139680259150208 executor.py:114] TRAIN Batch 10/600 loss 84.610672 loss_att 78.290390 loss_ctc 99.357986 lr 0.00010160 rank 0
I0623 22:23:18.813174 139680259150208 executor.py:114] TRAIN Batch 10/700 loss 66.411148 loss_att 61.762779 loss_ctc 77.257339 lr 0.00010280 rank 0
I0623 22:23:29.244020 139680259150208 executor.py:151] CV Batch 10/0 loss 40.797890 loss_att 38.973976 loss_ctc 45.053684 history loss 38.398014 rank 0
I0623 22:23:32.788610 139680259150208 train.py:275] Epoch 10 CV info cv_loss 85.44299678443116
I0623 22:23:32.788763 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/10.pt
I0623 22:23:33.096543 139680259150208 train.py:268] Epoch 11 TRAIN info lr 0.00010339999999999999
I0623 22:23:33.097683 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:23:33.097756 139680259150208 executor.py:45] total epoch is 11.
I0623 22:23:41.346638 139680259150208 executor.py:114] TRAIN Batch 11/0 loss 42.067413 loss_att 39.713390 loss_ctc 47.560135 lr 0.00010360 rank 0
I0623 22:23:53.119923 139680259150208 executor.py:114] TRAIN Batch 11/100 loss 84.005905 loss_att 77.524948 loss_ctc 99.128143 lr 0.00010480 rank 0
I0623 22:24:01.941142 139680259150208 executor.py:114] TRAIN Batch 11/200 loss 61.807449 loss_att 57.509060 loss_ctc 71.837036 lr 0.00010600 rank 0
I0623 22:24:13.849284 139680259150208 executor.py:114] TRAIN Batch 11/300 loss 107.570160 loss_att 99.240631 loss_ctc 127.005737 lr 0.00010720 rank 0
I0623 22:24:22.471360 139680259150208 executor.py:114] TRAIN Batch 11/400 loss 74.963516 loss_att 68.705490 loss_ctc 89.565567 lr 0.00010860 rank 0
I0623 22:24:34.924710 139680259150208 executor.py:114] TRAIN Batch 11/500 loss 41.226746 loss_att 38.804581 loss_ctc 46.878464 lr 0.00010980 rank 0
I0623 22:24:42.896253 139680259150208 executor.py:114] TRAIN Batch 11/600 loss 84.658340 loss_att 77.886101 loss_ctc 100.460228 lr 0.00011100 rank 0
I0623 22:24:54.416238 139680259150208 executor.py:114] TRAIN Batch 11/700 loss 60.688202 loss_att 55.579056 loss_ctc 72.609535 lr 0.00011220 rank 0
I0623 22:25:04.456469 139680259150208 executor.py:151] CV Batch 11/0 loss 40.374306 loss_att 38.561569 loss_ctc 44.604031 history loss 37.999347 rank 0
I0623 22:25:07.695812 139680259150208 train.py:275] Epoch 11 CV info cv_loss 84.45833326234572
I0623 22:25:07.695958 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/11.pt
I0623 22:25:07.969812 139680259150208 train.py:268] Epoch 12 TRAIN info lr 0.00011279999999999999
I0623 22:25:07.971053 139680259150208 executor.py:33] using accumulate grad, new batch size is 16 times larger than before
I0623 22:25:07.971135 139680259150208 executor.py:45] total epoch is 12.
I0623 22:25:15.883597 139680259150208 executor.py:114] TRAIN Batch 12/0 loss 42.186653 loss_att 39.646736 loss_ctc 48.113129 lr 0.00011300 rank 0
I0623 22:25:27.347719 139680259150208 executor.py:114] TRAIN Batch 12/100 loss 87.904587 loss_att 81.257790 loss_ctc 103.413773 lr 0.00011420 rank 0
I0623 22:25:35.921679 139680259150208 executor.py:114] TRAIN Batch 12/200 loss 63.545948 loss_att 58.914352 loss_ctc 74.352997 lr 0.00011540 rank 0
I0623 22:25:47.059508 139680259150208 executor.py:114] TRAIN Batch 12/300 loss 109.304886 loss_att 100.470665 loss_ctc 129.918060 lr 0.00011660 rank 0
I0623 22:25:55.267954 139680259150208 executor.py:114] TRAIN Batch 12/400 loss 70.699265 loss_att 64.616043 loss_ctc 84.893456 lr 0.00011800 rank 0
I0623 22:26:06.785297 139680259150208 executor.py:114] TRAIN Batch 12/500 loss 42.262188 loss_att 39.360214 loss_ctc 49.033463 lr 0.00011920 rank 0
I0623 22:26:14.786288 139680259150208 executor.py:114] TRAIN Batch 12/600 loss 89.322037 loss_att 81.946365 loss_ctc 106.531944 lr 0.00012040 rank 0
I0623 22:26:26.054600 139680259150208 executor.py:114] TRAIN Batch 12/700 loss 57.017426 loss_att 52.046906 loss_ctc 68.615295 lr 0.00012160 rank 0
I0623 22:26:36.353332 139680259150208 executor.py:151] CV Batch 12/0 loss 39.695694 loss_att 37.807373 loss_ctc 44.101784 history loss 37.360653 rank 0
I0623 22:26:39.605561 139680259150208 train.py:275] Epoch 12 CV info cv_loss 83.43608324326424
I0623 22:26:39.605874 139680259150208 checkpoint.py:34] Checkpoint: save to checkpoint exp/conformer/12.pt

